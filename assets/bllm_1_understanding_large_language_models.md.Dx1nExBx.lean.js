import{_ as a,c as r,a2 as t,o as p}from"./chunks/framework.CpXGQlwB.js";const L="/repo/assets/image.CScjf83B.png",n="/repo/assets/image-1.qHfM4xjx.png",o="/repo/assets/image-2.CeLePzGr.png",s="/repo/assets/image-3.rq4p0AMa.png",i="/repo/assets/image-4.Dlc0I-tJ.png",l="/repo/assets/image-5.ChZ-NkuD.png",d="/repo/assets/image-6.B9Q8KDRN.png",m="/repo/assets/image-7.DnZQGmmK.png",G=JSON.parse('{"title":"认识大型语言模型","description":"","frontmatter":{},"headers":[],"relativePath":"bllm/1_understanding_large_language_models.md","filePath":"bllm/1_understanding_large_language_models.md"}'),h={name:"bllm/1_understanding_large_language_models.md"};function T(M,e,P,g,c,_){return p(),r("div",null,e[0]||(e[0]=[t('<h1 id="认识大型语言模型" tabindex="-1">认识大型语言模型 <a class="header-anchor" href="#认识大型语言模型" aria-label="Permalink to &quot;认识大型语言模型&quot;">​</a></h1><h2 id="本章内容" tabindex="-1">本章内容 <a class="header-anchor" href="#本章内容" aria-label="Permalink to &quot;本章内容&quot;">​</a></h2><ul><li>大型语言模型（LLM）的基本概念高级解释</li><li>对支撑 ChatGPT 等 LLM 的 Transformer 架构的深入分析</li><li>从零构建 LLM 的步骤规划</li></ul><p>大型语言模型（LLM），如 OpenAI 的 ChatGPT，是近年来开发的深度神经网络模型，为自然语言处理（NLP）领域带来了革命性的进展。在大型语言模型出现之前，传统方法主要在分类任务上表现优异，比如邮件垃圾分类或一些可以通过手工规则或简单模型捕捉的模式识别。然而，这些方法在需要复杂理解和生成能力的语言任务上往往表现不佳，例如解析复杂指令、进行上下文分析或创作连贯的原创文本。举个例子，早期的语言模型无法从关键词列表中生成一封完整的邮件，而这是当代 LLM 轻松完成的任务。</p><p>LLM 具备惊人的语言理解、生成和解释能力。然而，需要澄清的是，所谓的“理解”并非指 LLM 具有类似人类的意识，而是指它能够生成符合逻辑、上下文相关的文本，而不是具备真正的理解力或意识。</p><p>得益于深度学习的进步（深度学习是机器学习和人工智能的一个分支，主要研究神经网络），LLM 可以在庞大的文本数据上进行训练，捕捉比以往方法更丰富的上下文信息和语言微妙之处。因此，LLM 在翻译、情感分析、问答等各种 NLP 任务中都展现出了显著的性能提升。</p><p>当代 LLM 与早期 NLP 模型的一个显著区别在于，早期模型通常为特定任务设计，表现优异但应用面较窄，而 LLM 则在广泛的 NLP 任务中都展示了较强的通用能力。</p><p>LLM 成功的背后有两个关键因素：一是 Transformer 架构的支持，这是许多 LLM 的基础；二是庞大的训练数据，使得模型能够捕捉各种语言的细微差别、上下文和模式，这些都是人工编码难以实现的。</p><p>基于 Transformer 架构并借助大规模数据训练的模型，彻底改变了 NLP 的发展方向，提供了更强大的工具来理解和处理人类语言。</p><p>从本章开始，我们将逐步奠定基础，实现本书的主要目标：通过编码一步步构建一个基于 Transformer 架构的 ChatGPT 类 LLM。</p><h2 id="_1-1-什么是大型语言模型-llm" tabindex="-1">1.1 什么是大型语言模型（LLM）？ <a class="header-anchor" href="#_1-1-什么是大型语言模型-llm" aria-label="Permalink to &quot;1.1 什么是大型语言模型（LLM）？&quot;">​</a></h2><p>大型语言模型（LLM）是一种旨在理解、生成并回应类似人类文本的神经网络。这些模型是通过对海量文本数据进行训练而成，有时几乎涵盖了互联网上所有公开的文本内容。</p><p>“Large”（大型）不仅指模型参数的规模，还指模型所训练的数据量。这类模型通常包含数百亿甚至上千亿的参数，这些参数是神经网络中的可调权重，在训练过程中会被优化以预测下一个词。通过预测下一个词，这类模型能够借助语言的顺序特性来学习文本的上下文、结构以及各词之间的关系。尽管这个任务听起来简单，但出色的模型能力往往令研究人员感到意外。在接下来的章节中，我们将逐步讨论和实现这种“下一个词”训练过程。</p><p>LLM 采用了一种称为 Transformer 的架构（将在 1.4 节详细讲解），使其在预测时能够选择性地关注输入的不同部分，从而更加擅长处理人类语言的细微差别和复杂性。</p><p>因为 LLM 能够生成文本，它们也被视为一种生成式人工智能（简称生成式 AI 或 GenAI）。正如图 1.1 所示，AI 是一个广义的领域，旨在创造能够执行需要类人智能的任务的机器，包括理解语言、识别模式和做出决策，其中包含机器学习和深度学习等子领域。</p><p>图 1.1 展示了这些不同领域的层级关系，其中 LLM 是深度学习技术的一个具体应用，利用了其处理和生成类似人类文本的能力。深度学习是机器学习的一个分支，专注于使用多层神经网络。而机器学习和深度学习则致力于开发算法，使计算机能够从数据中学习并执行通常需要人类智能的任务。</p><p>在 AI 的实现中，机器学习是核心领域，专注于开发能够从数据中学习并基于数据进行预测或决策的算法，而无需显式编程。举例来说，垃圾邮件过滤器是机器学习的一个应用，不再通过手动编写识别垃圾邮件的规则，而是使用已标注为垃圾邮件和正常邮件的示例来训练算法。通过在训练数据集上最小化预测误差，模型学会识别代表垃圾邮件的模式和特征，从而能够将新邮件分类为垃圾或正常。</p><p>如图 1.1 所示，深度学习是机器学习的一个子集，专注于使用三层或更多层的神经网络（即深度神经网络）来建模数据中的复杂模式和抽象特征。与深度学习相比，传统机器学习需要人工提取特征，这意味着需要人类专家识别并选择最相关的特征供模型使用。</p><p>尽管目前 AI 领域以机器学习和深度学习为主导，但它还包括其他方法，例如基于规则的系统、遗传算法、专家系统、模糊逻辑或符号推理。</p><p>回到垃圾邮件分类的例子，在传统机器学习中，人类专家可能需要手动从邮件文本中提取特征，例如某些触发词的频率（如“prize”、“win”、“free”），感叹号的数量、大写单词的使用或是否包含可疑链接。基于这些人工定义的特征生成的数据集将用于训练模型。而在深度学习中则无需手动提取特征，这意味着不需要专家去识别和选择最相关的特征供深度学习模型使用。（但在垃圾邮件分类中，无论是传统机器学习还是深度学习，仍然需要收集标签数据，即垃圾邮件和正常邮件，由专家或用户标注。）</p><p>接下来的章节将介绍 LLM 当前可以解决的一些问题、LLM 解决的挑战以及 LLM 的总体架构，本书将逐步实现这一架构。</p><h2 id="_1-2-大型语言模型的应用" tabindex="-1">1.2 大型语言模型的应用 <a class="header-anchor" href="#_1-2-大型语言模型的应用" aria-label="Permalink to &quot;1.2 大型语言模型的应用&quot;">​</a></h2><p>由于 LLM 在解析和理解非结构化文本数据方面的强大能力，现今它们被广泛应用于各个领域。LLM 可以用于机器翻译、新文本生成（见图 1.2）、情感分析、文本摘要等众多任务。近年来，LLM 还被用于内容创作，比如写小说、文章，甚至编写计算机代码。</p><p>图 1.2 展示了通过 LLM 接口实现用户与 AI 系统之间的自然语言交流。截图显示 ChatGPT 根据用户的要求创作诗歌。 <img src="'+L+'" alt="alt text"></p><p>LLM 还能为智能聊天机器人和虚拟助手提供支持，比如 OpenAI 的 ChatGPT 或谷歌的 Gemini（前称 Bard），它们不仅可以解答用户问题，还能强化 Google Search 或 Microsoft Bing 等传统搜索引擎的功能。</p><p>此外，在医学、法律等专业领域，LLM 可以用于从大量文本中高效检索知识，包括浏览文档、总结长篇内容、回答技术性问题等。</p><p>简而言之，LLM 在几乎所有涉及解析和生成文本的自动化任务中都展现出极高的价值。它们的应用前景几乎无穷无尽，随着我们不断创新并探索这些模型的新用法，LLM 有望重新定义我们与技术的关系，使其更加对话化、直观和易于使用。</p><p>在本书中，我们将从头开始理解 LLM 的工作原理，并逐步构建一个能够生成文本的 LLM。同时，我们还将学习如何让 LLM 执行查询任务，包括回答问题、摘要文本、文本翻译等。换句话说，本书将通过逐步构建一个 LLM，帮助我们理解像 ChatGPT 这样复杂的智能助手的工作机制。</p><h2 id="_1-3-构建和使用大型语言模型的阶段" tabindex="-1">1.3 构建和使用大型语言模型的阶段 <a class="header-anchor" href="#_1-3-构建和使用大型语言模型的阶段" aria-label="Permalink to &quot;1.3 构建和使用大型语言模型的阶段&quot;">​</a></h2><p>为什么要构建自己的 LLM？从头开始编写一个 LLM，是理解其工作机制和局限性的绝佳方式。这也为我们提供了所需的知识，使我们能够在现有的开源 LLM 架构上进行预训练或微调，以适应特定领域的数据集或任务。研究表明，定制化的 LLM（针对特定任务或领域量身打造）在性能上往往优于通用 LLM，例如 ChatGPT，这类模型的设计初衷是适用于各种应用场景。比如，专为金融领域打造的 BloombergGPT，以及用于医疗问答的 LLM 就是这种定制化模型的典型例子（更多详情可参考附录 B 的延伸阅读和参考资料部分）。</p><p>构建 LLM 的基本流程包括预训练和微调。“预训练”中的“预”指的是模型的初始训练阶段，模型会在一个大规模、广泛的数据集上训练，以获得语言的基本理解。这一预训练模型成为后续微调的基础，在微调阶段，模型会在更具体、更小规模的数据集上进一步训练，以适应特定任务或领域。这种由预训练和微调组成的双阶段训练流程如图 1.3 所示。</p><p>图 1.3：LLM 的预训练涉及在大规模文本数据集上进行下一词预测。预训练后的 LLM 可以在一个较小的标注数据集上进行微调。 <img src="'+n+'" alt="alt text"> 如图 1.3 所示，创建 LLM 的第一步是将其训练在一个庞大的文本语料库上，这些数据通常被称为“原始文本”。这里的“原始”指的是未经标注的常规文本（[1] 过滤处理可能会被应用，比如去除格式化字符或不明语言的文档）。</p><p>LLM 的第一个训练阶段也称为预训练，生成一个初始的预训练模型，通常称为基础或底层模型。典型的例子如 GPT-3（ChatGPT 初版模型的前身），该模型能够完成用户提供的未完句子的续写。此外，它还具备一定的少样本学习能力，即可以通过少量示例学习新任务，而无需大量训练数据。这一点将在下一节“使用 Transformer 完成不同任务”中进一步说明。</p><p>在对大型文本数据集进行下一词预测训练以获得预训练的 LLM 后，我们可以在标注数据上对该 LLM 进行进一步训练，即微调。</p><p>LLM 微调的两大常用类型包括“指令微调”和“分类微调”。在指令微调中，标注数据集由指令和答案对组成，比如翻译请求及对应的正确翻译文本。在分类微调中，标注数据集由文本和关联的类别标签组成，例如带有垃圾邮件和非垃圾邮件标签的邮件。</p><p>在本书中，我们将涵盖 LLM 预训练和微调的代码实现，并在预训练出基础 LLM 后深入探讨指令微调和分类微调的具体内容。</p><h2 id="_1-4-使用大型语言模型完成不同任务" tabindex="-1">1.4 使用大型语言模型完成不同任务 <a class="header-anchor" href="#_1-4-使用大型语言模型完成不同任务" aria-label="Permalink to &quot;1.4 使用大型语言模型完成不同任务&quot;">​</a></h2><p>当前大多数大型语言模型（LLM）基于 Transformer 架构，这是2017年论文《Attention Is All You Need》提出的深度神经网络架构。要理解 LLM，我们需要简要回顾最初为机器翻译设计的 Transformer，它用于将英文翻译为德语或法语。图 1.4 展示了一个简化版的 Transformer 架构。</p><p>图 1.4：原始 Transformer 架构的简化示意图，这是用于语言翻译的深度学习模型。Transformer 包含两部分：编码器和解码器。编码器处理输入文本并生成文本的嵌入表示（通过不同维度捕捉多种因素的数值表示），解码器则使用这些表示逐词生成翻译文本。图中显示了翻译过程的最后一步：解码器只需生成最后一个词（“Beispiel”），完成对输入文本（“This is an example”）和部分已翻译句子（“Das ist ein”）的翻译。 <img src="'+o+'" alt="alt text"> 如图 1.4 所示，Transformer 架构由两个子模块组成：编码器和解码器。编码器模块处理输入文本并将其编码为一系列数值表示（向量），这些表示捕捉了输入的上下文信息。解码器模块则使用这些编码向量生成输出文本。在翻译任务中，编码器会将源语言文本编码为向量，解码器再将这些向量解码为目标语言文本。编码器和解码器由所谓的“自注意力”机制连接，可能会让您有很多关于输入如何预处理和编码的疑问，这些将在接下来的章节中逐步实现和讲解。</p><p>Transformer 和 LLM 的关键组成部分是自注意力机制（此处未展示），该机制允许模型权衡序列中不同词或标记的重要性。这种机制使模型能够捕捉长距离依赖和上下文关系，增强其生成连贯且上下文相关输出的能力。不过由于复杂性，我们将在第三章详细讲解和实现该机制。此外，第二章《处理文本数据》还将讨论并实现数据预处理步骤，以生成模型输入。</p><p>Transformer 架构的后续变体包括 BERT（双向编码器表示 Transformer 的缩写）和各种 GPT 模型（生成式预训练 Transformer 的缩写），它们基于该架构进行了不同任务的适应（详见附录 B）。</p><p>BERT 构建在原始 Transformer 的编码器模块之上，与 GPT 的训练方式不同。GPT 专注于生成任务，而 BERT 及其变体专长于“遮掩词预测”，即预测给定句子中被遮掩的词，正如图 1.5 所示。这种独特的训练方式使 BERT 在文本分类任务（如情感预测、文档分类）上表现出色。例如，截至本书撰写时，Twitter 使用 BERT 来检测有害内容。</p><p>图 1.5：Transformer 编码器和解码器子模块的示意图。左侧编码器段展示了类似 BERT 的 LLM，专注于遮掩词预测，主要用于文本分类任务。右侧解码器段展示了类似 GPT 的 LLM，专为生成任务设计，能够生成连贯的文本序列。 <img src="'+s+'" alt="alt text"> 而 GPT 侧重于原始 Transformer 架构的解码器部分，专注于生成文本的任务，例如机器翻译、文本摘要、小说创作、编写代码等。我们将在本章的剩余部分更详细地讨论 GPT 架构，并在本书中从零实现它。</p><p>GPT 模型主要被设计和训练用于文本补全任务，同时展现了非凡的多功能性。这些模型擅长执行零样本学习和少样本学习任务。零样本学习指的是模型能够在没有具体示例的情况下处理全新任务，而少样本学习则是从用户提供的少量示例中学习，如图 1.6 所示。</p><p>图 1.6：除文本补全外，类似 GPT 的 LLM 可以根据输入解决多种任务，而无需重新训练、微调或改变任务特定的模型架构。提供目标示例的方式被称为少样本设置，而不提供具体示例的任务则称为零样本设置。 <img src="'+i+'" alt="alt text"> Transformer 与 LLM 的区别 现今的 LLM 基于之前提到的 Transformer 架构，因此 Transformer 和 LLM 两个术语在文献中常被互换使用。然而，需要注意的是，并非所有 Transformer 都是 LLM，因为 Transformer 也可以用于计算机视觉。同样，也并非所有 LLM 都基于 Transformer，因为仍然存在基于循环网络和卷积网络的 LLM。这些替代架构的主要动机是提高 LLM 的计算效率。不过，基于这些架构的 LLM 能否在能力上与 Transformer 型 LLM 竞争，以及是否会在实际应用中被广泛采用，仍有待观察。（有兴趣的读者可以在本章结尾的延伸阅读部分找到相关文献参考。）</p><h2 id="_1-5-利用大型数据集" tabindex="-1">1.5 利用大型数据集 <a class="header-anchor" href="#_1-5-利用大型数据集" aria-label="Permalink to &quot;1.5 利用大型数据集&quot;">​</a></h2><p>用于预训练 GPT 和 BERT 等模型的大型数据集包含了数十亿词汇的多样化文本语料库，涵盖了广泛的主题和语言（包括自然语言和计算机语言）。为更直观地了解，表 1.1 列出了 GPT-3 预训练数据集的摘要，该模型是 ChatGPT 第一版的基础模型。</p><p>表 1.1：GPT-3 的预训练数据集</p><table tabindex="0"><thead><tr><th>数据集名称</th><th>数据集描述</th><th>词元数量</th><th>在训练数据中所占比例</th></tr></thead><tbody><tr><td>CommonCrawl（过滤）</td><td>网站爬取数据</td><td>4100 亿</td><td>60%</td></tr><tr><td>WebText2</td><td>网站爬取数据</td><td>190 亿</td><td>22%</td></tr><tr><td>Books1</td><td>网络图书语料库</td><td>120 亿</td><td>8%</td></tr><tr><td>Books2</td><td>网络图书语料库</td><td>550 亿</td><td>8%</td></tr><tr><td>Wikipedia</td><td>高质量文本</td><td>30 亿</td><td>3%</td></tr></tbody></table><p>在表 1.1 中，“词元”指模型处理的文本单元，其数量大致相当于文本中的单词和标点符号数量。下一章我们将详细讲解“词元化”这一将文本转化为词元的过程。</p><p>最核心的启示是，这种规模和多样化的数据集使这些模型在多种任务上表现优异，包括语言的语法、语义、上下文分析，甚至一些需要常识性知识的任务。</p><h4 id="gpt-3-数据集详解" tabindex="-1">GPT-3 数据集详解 <a class="header-anchor" href="#gpt-3-数据集详解" aria-label="Permalink to &quot;GPT-3 数据集详解&quot;">​</a></h4><p>表 1.1 中的数据集中，仅使用了数据的一部分，总计约 3000 亿词元。这种抽样方法意味着模型并未训练所有数据，而是从每个数据集中抽取一个总量为 3000 亿的词元子集，有些数据集可能被多次纳入以达到总词元数。表中显示的比例总和约为 100%，已考虑到可能的四舍五入误差。</p><p>例如，CommonCrawl 数据集包含 4100 亿词元，存储容量约为 570 GB。而 GPT-3 的后续版本，如 Meta 的 LLaMA，进一步扩展了训练数据，加入了更多数据源，比如 Arxiv 研究论文（92 GB）和 StackExchange 的代码问答（78 GB）。</p><p>Wikipedia 语料库则由英语维基百科内容构成。尽管 GPT-3 论文作者未进一步说明细节，但 Books1 很可能源自 Project Gutenberg（<a href="./.html">https://www.gutenberg.org/），而</a> Books2 则可能来自 Libgen（<a href="https://en.wikipedia.org/wiki/Library_Genesis%EF%BC%89%E3%80%82CommonCrawl" target="_blank" rel="noreferrer">https://en.wikipedia.org/wiki/Library_Genesis）。CommonCrawl</a> 是 CommonCrawl 数据库（<a href="./.html">https://commoncrawl.org/）的过滤子集，WebText2</a> 则包含来自 Reddit 帖子中 3 个以上点赞的外链页面内容。</p><p>GPT-3 论文未公开具体训练数据集，但类似的数据集“ The Pile”（<a href="./.html">https://pile.eleuther.ai/）是公开可用的。不过，该集合可能包含受版权保护的作品，使用条款可能因使用目的和国家而异。更多信息可见</a> HackerNews 讨论：<a href="./.html">https://news.ycombinator.com/item?id=25607809。</a></p><p>这些预训练模型之所以被称为基础或底层模型，是因为它们在进一步微调时表现出极大的灵活性。预训练 LLM 需要大量资源，成本非常高昂，例如 GPT-3 的预训练成本估计约为 460 万美元的云计算费用[2]。</p><p>好消息是，许多预训练 LLM 已作为开源模型发布，供人们用作通用工具来编写、提取和编辑不在训练数据中的文本。此外，LLM 可以在相对较小的数据集上进行微调，从而降低计算资源需求，并在特定任务上提高性能。</p><p>本书将实现 LLM 的预训练代码，并用于教育目的的预训练过程。所有计算都将在普通硬件上运行。在实现预训练代码后，我们将学习如何加载开源的模型权重到我们实现的架构中，使我们能够在后续的 LLM 微调过程中跳过昂贵的预训练阶段。</p><h2 id="_1-6-gpt-架构的深入分析" tabindex="-1">1.6 GPT 架构的深入分析 <a class="header-anchor" href="#_1-6-gpt-架构的深入分析" aria-label="Permalink to &quot;1.6 GPT 架构的深入分析&quot;">​</a></h2><p>在本章前面，我们提到过 GPT 类模型、GPT-3 和 ChatGPT，现在让我们更详细地了解 GPT 架构的基本结构。GPT 的全称是“生成式预训练 Transformer”（Generative Pretrained Transformer），最早在以下论文中提出： <em>Improving Language Understanding by Generative Pre-Training</em>（2018），由 OpenAI 的 Radford 等人撰写，链接：<a href="http://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf" target="_blank" rel="noreferrer">http://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf</a></p><p>GPT-3 是此模型的扩展版，具有更多的参数和更大的训练数据集。而 ChatGPT 的初版则是在 GPT-3 的基础上，使用 OpenAI 的 InstructGPT 论文中提到的“指令数据集”进行微调得出的模型（我们将在第七章详细讲解如何通过人类反馈微调模型以遵循指令）。如图 1.6 所示，这些模型不仅在文本补全上表现优异，还能完成拼写纠正、分类、翻译等任务。令人惊讶的是，GPT 模型主要通过相对简单的“下一词预测”任务进行预训练，如图 1.7 所示。</p><p>图 1.7：在 GPT 模型的下一词预测预训练任务中，系统通过查看前面的词来学习预测句子中的下一个词。这一方法帮助模型理解词汇和短语如何在语言中自然地组合在一起，为多种任务奠定了基础。 <img src="'+l+'" alt="alt text"> 下一词预测任务属于自监督学习的一种形式，即一种自我标注的方式。也就是说，我们不需要为训练数据收集显式标签，而是利用数据本身的结构：可以将句子或文档中的下一个词视为模型要预测的标签。由于下一词预测任务可以“即刻”生成标签，我们可以利用庞大的未标注文本数据集来训练 LLM，正如 1.5 节所讨论的那样。</p><p>与 1.4 节介绍的原始 Transformer 架构相比，GPT 架构相对简单。它仅包含解码器部分，没有编码器，如图 1.8 所示。像 GPT 这样的解码器模型通过逐词预测生成文本，因此被视为自回归模型。自回归模型将先前的输出作为未来预测的输入。因此，在 GPT 中，每个新词的选择都基于前面的序列，从而提升生成文本的连贯性。</p><p>GPT-3 这类架构在规模上远超原始 Transformer 模型。比如，原始 Transformer 重复了六次编码器和解码器块，而 GPT-3 包含 96 层 Transformer 和总计 1750 亿个参数。</p><p>图 1.8：GPT 架构仅使用原始 Transformer 的解码器部分。其单向、从左至右的处理方式非常适合文本生成和下一词预测任务，通过逐词迭代生成文本。 <img src="'+d+'" alt="alt text"> GPT-3 于 2020 年推出，虽然从深度学习和 LLM 发展的角度来看时间较早，但像 Meta 的 LLaMA 等更新的架构仍然基于相同的核心概念，仅做了少量修改。因此，理解 GPT 仍然非常重要，本书将专注于实现 GPT 背后的核心架构，同时提供不同 LLM 的特定调整参考。</p><p>最后值得注意的是，尽管最初的 Transformer 模型专为翻译而设计，GPT 模型——即使在其更大但更简单的下一词预测架构下——也能完成翻译任务。研究人员起初没有预料到这一点，因为它的训练目标主要是下一词预测任务，并未特别针对翻译。</p><p>模型执行未明确训练过的任务的能力被称为“突现行为”（emergent behavior）。这种能力并非在训练中被显式教授，而是模型在大量多语言和多样化上下文数据的接触中自然产生的。这表明 GPT 模型能够“学习”语言间的翻译模式并执行翻译任务，即使没有专门为此训练。这种大型生成式语言模型的优势在于，我们可以用一个模型执行多种任务，而不需要为每种任务开发特定模型。</p><h2 id="_1-7-构建大型语言模型" tabindex="-1">1.7 构建大型语言模型 <a class="header-anchor" href="#_1-7-构建大型语言模型" aria-label="Permalink to &quot;1.7 构建大型语言模型&quot;">​</a></h2><p>本章为理解 LLM 奠定了基础。在接下来的内容中，我们将从头开始编写一个 LLM。我们将以 GPT 的基本思路为蓝图，分三步进行，具体如图 1.9 所示。</p><p>图 1.9：本书构建 LLM 的三个阶段，包括实现 LLM 架构和数据预处理过程，预训练 LLM 以创建基础模型，以及微调基础模型，使其成为个人助手或文本分类器。 <img src="'+m+'" alt="alt text"> 首先，我们将学习数据预处理的基本步骤，并编写每个 LLM 核心的注意力机制代码。</p><p>接着，在第 2 阶段中，我们将学习如何编写并预训练一个 GPT 类 LLM，使其能够生成新文本。此外，我们还将学习 LLM 评估的基本原理，这是开发高效 NLP 系统的关键。需要注意的是，从零开始预训练一个大型 LLM 是一项巨大的工程，GPT 类模型的预训练成本可能高达数千甚至数百万美元。因此，第 2 阶段的重点在于通过小型数据集实现教学用途的训练，书中还会提供加载开源模型权重的代码示例。</p><p>最后，在第 3 阶段，我们将采用预训练的 LLM 并微调其功能，使其能够执行回答问题或文本分类等指令任务——这是许多实际应用和研究中最常见的任务。</p><p>希望您期待这段激动人心的学习之旅！</p><h2 id="_1-8-总结" tabindex="-1">1.8 总结 <a class="header-anchor" href="#_1-8-总结" aria-label="Permalink to &quot;1.8 总结&quot;">​</a></h2><p>LLM 改变了自然语言处理领域，该领域以前主要依赖于基于规则的系统和简单的统计方法。LLM 的出现引入了新的深度学习驱动方法，推动了对人类语言的理解、生成和翻译方面的进步。</p><p>现代 LLM 的训练分为两个主要步骤：</p><ol><li>首先，在大量未标注文本上预训练，通过预测句子中的下一个词作为“标签”。</li><li>接着，在较小的标注目标数据集上进行微调，以执行指令或分类任务。</li></ol><p>LLM 基于 Transformer 架构，其核心思想是注意力机制，使 LLM 在逐词生成输出时能选择性访问整个输入序列。</p><p>最初的 Transformer 架构包含用于解析文本的编码器和用于生成文本的解码器。 GPT-3 和 ChatGPT 等生成文本和执行指令的 LLM 仅使用解码器模块，简化了架构。</p><p>包含数十亿词汇的大型数据集对于预训练 LLM 至关重要。本书中，我们将实现并在小型数据集上训练 LLM，用于教学目的，同时了解如何加载开源模型权重。</p><p>虽然 GPT 类模型的预训练任务通常是预测句子中的下一个词，但这些 LLM 展现了“突现”属性，如分类、翻译或摘要的能力。</p><p>一旦 LLM 经过预训练，生成的基础模型可以更高效地进行多种下游任务的微调。</p><p>在定制数据集上微调的 LLM 可以在特定任务上优于通用 LLM。</p><p><em>[1]</em> 具有机器学习背景的读者可能会注意到，传统机器学习模型和深度神经网络的监督学习通常需要标注信息。然而，这在 LLM 预训练阶段并非如此。在此阶段，LLM 利用自监督学习，即模型从输入数据中生成自身标签。本章稍后将详细讲解该概念。</p><p><em>[2]</em> GPT-3：460 万美元的语言模型 来源：<a href="https://www.reddit.com/r/MachineLearning/comments/h0jwoz/d_gpt3_the_4600000_lan" target="_blank" rel="noreferrer">https://www.reddit.com/r/MachineLearning/comments/h0jwoz/d_gpt3_the_4600000_lan</a></p>',86)]))}const u=a(h,[["render",T]]);export{G as __pageData,u as default};
