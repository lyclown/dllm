const t='{"documentCount":197,"nextId":197,"documentIds":{"0":"api-examples.html#runtime-api-examples","1":"api-examples.html#results","2":"api-examples.html#theme-data","3":"api-examples.html#page-data","4":"api-examples.html#page-frontmatter","5":"api-examples.html#more","6":"bllm/1_understanding_large_language_models.html#认识大型语言模型","7":"bllm/1_understanding_large_language_models.html#本章内容","8":"bllm/1_understanding_large_language_models.html#_1-1-什么是大型语言模型-llm","9":"bllm/1_understanding_large_language_models.html#_1-2-大型语言模型的应用","10":"bllm/1_understanding_large_language_models.html#_1-3-构建和使用大型语言模型的阶段","11":"bllm/1_understanding_large_language_models.html#_1-4-使用大型语言模型完成不同任务","12":"bllm/1_understanding_large_language_models.html#_1-5-利用大型数据集","13":"bllm/1_understanding_large_language_models.html#gpt-3-数据集详解","14":"bllm/1_understanding_large_language_models.html#_1-6-gpt-架构的深入分析","15":"bllm/1_understanding_large_language_models.html#_1-7-构建大型语言模型","16":"bllm/1_understanding_large_language_models.html#_1-8-总结","17":"bllm/2_working_with_text_data.html#处理文本数据","18":"bllm/2_working_with_text_data.html#本章内容","19":"bllm/2_working_with_text_data.html#_2-1-理解词嵌入","20":"bllm/2_working_with_text_data.html#_2-2-文本词元化","21":"bllm/2_working_with_text_data.html#代码示例-2-1-将短篇小说作为文本示例导入-python","22":"bllm/2_working_with_text_data.html#文本样本大小","23":"bllm/2_working_with_text_data.html#如何有效地拆分文本","24":"bllm/2_working_with_text_data.html#是否移除空白字符","25":"bllm/2_working_with_text_data.html#将词元化应用于整个短篇小说","26":"bllm/2_working_with_text_data.html#_2-3-将词元转换为词元-id","27":"bllm/2_working_with_text_data.html#代码示例-2-2-创建词汇表","28":"bllm/2_working_with_text_data.html#代码示例-2-3-实现简单的文本词元化器","29":"bllm/2_working_with_text_data.html#实例化并使用词元化器","30":"bllm/2_working_with_text_data.html#处理训练集中未包含的词","31":"bllm/2_working_with_text_data.html#_2-4-添加特殊上下文词元","32":"bllm/2_working_with_text_data.html#修改词汇表以包含特殊词元","33":"bllm/2_working_with_text_data.html#代码示例-2-4-处理未知单词的简单文本词元化器","34":"bllm/2_working_with_text_data.html#实践中试用新词元化器","35":"bllm/2_working_with_text_data.html#对词元化文本进行去词元化检查","36":"bllm/2_working_with_text_data.html#其他特殊词元","37":"bllm/2_working_with_text_data.html#_2-5-字节对编码","38":"bllm/2_working_with_text_data.html#关键观察","39":"bllm/2_working_with_text_data.html#bpe-如何处理未知单词","40":"bllm/2_working_with_text_data.html#练习-2-1-字节对编码未知单词","41":"bllm/2_working_with_text_data.html#bpe-的基本原理","42":"bllm/2_working_with_text_data.html#_2-6-使用滑动窗口进行数据采样","43":"bllm/2_working_with_text_data.html#代码示例-2-5-用于批量输入和目标的数据集","44":"bllm/2_working_with_text_data.html#代码示例-2-6-用于生成输入-目标对批次的数据加载器","45":"bllm/2_working_with_text_data.html#练习-2-2-不同-stride-和-context-size-的数据加载器","46":"bllm/2_working_with_text_data.html#_2-7-创建词元嵌入","47":"bllm/2_working_with_text_data.html#嵌入层与矩阵乘法的关系","48":"bllm/2_working_with_text_data.html#_2-8-编码词元位置","49":"bllm/2_working_with_text_data.html#_2-9-小结","50":"bllm/4_implementing_a_gpt_model_from_scratch_to_generate_text.html#从零开始实现一个-gpt-模型来生成文本","51":"bllm/4_implementing_a_gpt_model_from_scratch_to_generate_text.html#本章内容","52":"bllm/4_implementing_a_gpt_model_from_scratch_to_generate_text.html#_4-1-编写-llm-架构","53":"bllm/4_implementing_a_gpt_model_from_scratch_to_generate_text.html#gpt-2-与-gpt-3-的对比","54":"bllm/4_implementing_a_gpt_model_from_scratch_to_generate_text.html#_4-2-使用层归一化来调整激活值","55":"bllm/4_implementing_a_gpt_model_from_scratch_to_generate_text.html#有偏方差","56":"bllm/4_implementing_a_gpt_model_from_scratch_to_generate_text.html#层归一化与批量归一化的比较","57":"bllm/4_implementing_a_gpt_model_from_scratch_to_generate_text.html#_4-3-使用-gelu-激活函数实现前馈神经网络","58":"bllm/4_implementing_a_gpt_model_from_scratch_to_generate_text.html#_4-4-添加捷径连接","59":"bllm/4_implementing_a_gpt_model_from_scratch_to_generate_text.html#_4-5-在-transformer-块中连接注意力和线性层","60":"bllm/4_implementing_a_gpt_model_from_scratch_to_generate_text.html#_4-6-编写-gpt-模型代码","61":"bllm/4_implementing_a_gpt_model_from_scratch_to_generate_text.html#练习-4-1-前馈模块和注意力模块中的参数数量","62":"bllm/4_implementing_a_gpt_model_from_scratch_to_generate_text.html#练习-4-2-初始化更大规模的-gpt-模型","63":"bllm/4_implementing_a_gpt_model_from_scratch_to_generate_text.html#_4-7-生成文本","64":"bllm/4_implementing_a_gpt_model_from_scratch_to_generate_text.html#练习-4-3-使用单独的-dropout-参数","65":"bllm/4_implementing_a_gpt_model_from_scratch_to_generate_text.html#_4-8-总结","66":"bllm/5_pretraining_on_unlabeled_data.html#在无标签数据上进行预训练","67":"bllm/5_pretraining_on_unlabeled_data.html#本章内容","68":"bllm/5_pretraining_on_unlabeled_data.html#权重参数","69":"bllm/5_pretraining_on_unlabeled_data.html#_5-1-评估生成文本模型","70":"bllm/5_pretraining_on_unlabeled_data.html#_5-1-1-使用-gpt-生成文本","71":"bllm/5_pretraining_on_unlabeled_data.html#_5-1-2-计算文本生成损失","72":"bllm/5_pretraining_on_unlabeled_data.html#反向传播","73":"bllm/5_pretraining_on_unlabeled_data.html#交叉熵损失","74":"bllm/5_pretraining_on_unlabeled_data.html#困惑度","75":"bllm/5_pretraining_on_unlabeled_data.html#_5-1-3-计算训练集和验证集的损失","76":"bllm/5_pretraining_on_unlabeled_data.html#预训练-llm-的成本","77":"bllm/5_pretraining_on_unlabeled_data.html#使用可变长度进行训练","78":"bllm/5_pretraining_on_unlabeled_data.html#_5-2-训练大语言模型","79":"bllm/5_pretraining_on_unlabeled_data.html#_5-3-解码策略以控制随机性","80":"bllm/5_pretraining_on_unlabeled_data.html#_5-3-1-温度缩放","81":"bllm/5_pretraining_on_unlabeled_data.html#练习5-1","82":"bllm/5_pretraining_on_unlabeled_data.html#_5-3-2-top-k采样","83":"bllm/5_pretraining_on_unlabeled_data.html#_5-3-3-修改文本生成函数","84":"bllm/5_pretraining_on_unlabeled_data.html#练习5-2","85":"bllm/5_pretraining_on_unlabeled_data.html#练习5-3","86":"bllm/5_pretraining_on_unlabeled_data.html#_5-4-在-pytorch-中加载和保存模型权重","87":"bllm/5_pretraining_on_unlabeled_data.html#练习-5-4","88":"bllm/5_pretraining_on_unlabeled_data.html#_5-5-从-openai-加载预训练权重","89":"bllm/5_pretraining_on_unlabeled_data.html#更新的下载说明","90":"bllm/5_pretraining_on_unlabeled_data.html#练习-5-5","91":"bllm/5_pretraining_on_unlabeled_data.html#练习-5-6","92":"bllm/5_pretraining_on_unlabeled_data.html#_5-6-总结","93":"bllm/3_coding_attention_mechanisms.html#编码注意力机制","94":"bllm/3_coding_attention_mechanisms.html#本章内容","95":"bllm/3_coding_attention_mechanisms.html#_3-1-处理长序列的问题","96":"bllm/3_coding_attention_mechanisms.html#_3-2-使用注意力机制捕获数据依赖关系","97":"bllm/3_coding_attention_mechanisms.html#_3-3-使用自注意力关注输入的不同部分","98":"bllm/3_coding_attention_mechanisms.html#自注意力中的-自我","99":"bllm/3_coding_attention_mechanisms.html#_3-3-1-一个简单的无可训练权重的自注意力机制","100":"bllm/3_coding_attention_mechanisms.html#_3-3-2-为所有输入标记计算注意力权重","101":"bllm/3_coding_attention_mechanisms.html#步骤-1-计算所有输入对之间的点积","102":"bllm/3_coding_attention_mechanisms.html#步骤-2-归一化每行使得总和为-1","103":"bllm/3_coding_attention_mechanisms.html#步骤-3-计算所有上下文向量","104":"bllm/3_coding_attention_mechanisms.html#_3-4-实现带可训练权重的自注意力机制","105":"bllm/3_coding_attention_mechanisms.html#变量定义","106":"bllm/3_coding_attention_mechanisms.html#计算查询、键和值向量","107":"bllm/3_coding_attention_mechanisms.html#第二步-计算注意力得分","108":"bllm/3_coding_attention_mechanisms.html#第三步-从注意力得分到注意力权重","109":"bllm/3_coding_attention_mechanisms.html#最后一步-计算上下文向量","110":"bllm/3_coding_attention_mechanisms.html#代码清单-3-1-一个紧凑的自注意力类","111":"bllm/3_coding_attention_mechanisms.html#使用-pytorch-的-linear-层改进-selfattention-v1-实现","112":"bllm/3_coding_attention_mechanisms.html#代码清单-3-2-使用-pytorch-linear-层的自注意力类","113":"bllm/3_coding_attention_mechanisms.html#练习-3-1-比较-selfattention-v1-和-selfattention-v2","114":"bllm/3_coding_attention_mechanisms.html#下一节预告","115":"bllm/3_coding_attention_mechanisms.html#_3-5-使用因果注意力隐藏未来词","116":"bllm/3_coding_attention_mechanisms.html#第一步-计算注意力权重","117":"bllm/3_coding_attention_mechanisms.html#第二步-生成遮罩矩阵","118":"bllm/3_coding_attention_mechanisms.html#第三步-应用遮罩","119":"bllm/3_coding_attention_mechanisms.html#第四步-重新归一化遮罩的注意力权重","120":"bllm/3_coding_attention_mechanisms.html#避免信息泄露的说明","121":"bllm/3_coding_attention_mechanisms.html#更高效的遮罩方法","122":"bllm/3_coding_attention_mechanisms.html#将-dropout-应用于注意力权重矩阵","123":"bllm/3_coding_attention_mechanisms.html#代码清单-3-3-一个紧凑的因果注意力类","124":"bllm/3_coding_attention_mechanisms.html#代码解释","125":"bllm/3_coding_attention_mechanisms.html#_3-6-从单头注意力扩展到多头注意力","126":"bllm/3_coding_attention_mechanisms.html#_3-6-1-堆叠多个单头注意力层","127":"bllm/3_coding_attention_mechanisms.html#代码清单-3-4-实现多头注意力的包装类","128":"bllm/3_coding_attention_mechanisms.html#具体代码示例","129":"bllm/3_coding_attention_mechanisms.html#_3-6-2-实现带有权重分割的多头注意力机制","130":"bllm/3_coding_attention_mechanisms.html#示例-多头矩阵乘法的实现","131":"bllm/3_coding_attention_mechanisms.html#使用-multiheadattention-类","132":"bllm/3_coding_attention_mechanisms.html#练习-3-3-初始化与-gpt-2-相同大小的注意力模块","133":"bllm/3_coding_attention_mechanisms.html#_3-7-总结","134":"bllm/appendix_a_introduction_to_pytorch.html#附录-a-pytorch-简介","135":"bllm/appendix_a_introduction_to_pytorch.html#本章内容","136":"bllm/appendix_a_introduction_to_pytorch.html#a-1-什么是-pytorch","137":"bllm/appendix_a_introduction_to_pytorch.html#a-1-1-pytorch-的三个核心组件","138":"bllm/appendix_a_introduction_to_pytorch.html#a-1-2-定义深度学习","139":"bllm/appendix_a_introduction_to_pytorch.html#a-1-3-安装-pytorch","140":"bllm/appendix_a_introduction_to_pytorch.html#python-版本","141":"bllm/appendix_a_introduction_to_pytorch.html#使用-amd-gpu-进行深度学习","142":"bllm/appendix_a_introduction_to_pytorch.html#pytorch-和-torch","143":"bllm/appendix_a_introduction_to_pytorch.html#检查-gpu-是否被识别","144":"bllm/appendix_a_introduction_to_pytorch.html#在-apple-silicon-上使用-pytorch","145":"bllm/appendix_a_introduction_to_pytorch.html#练习-a-1","146":"bllm/appendix_a_introduction_to_pytorch.html#练习-a-2","147":"bllm/appendix_a_introduction_to_pytorch.html#a-2-理解张量","148":"bllm/appendix_a_introduction_to_pytorch.html#pytorch-的-numpy-类-api","149":"bllm/appendix_a_introduction_to_pytorch.html#a-2-1-标量、向量、矩阵和张量","150":"bllm/appendix_a_introduction_to_pytorch.html#a-2-2-张量数据类型","151":"bllm/appendix_a_introduction_to_pytorch.html#a-2-3-常见的-pytorch-张量操作","152":"bllm/appendix_a_introduction_to_pytorch.html#a-3-将模型视为计算图","153":"bllm/appendix_a_introduction_to_pytorch.html#a-4-自动微分简化计算","154":"bllm/appendix_a_introduction_to_pytorch.html#偏导数和梯度","155":"bllm/appendix_a_introduction_to_pytorch.html#a-5-实现多层神经网络","156":"bllm/appendix_a_introduction_to_pytorch.html#a-6-设置高效的数据加载器","157":"bllm/appendix_a_introduction_to_pytorch.html#类标签编号","158":"bllm/appendix_a_introduction_to_pytorch.html#num-workers-0-设置","159":"bllm/appendix_a_introduction_to_pytorch.html#a-7-典型的训练循环","160":"bllm/appendix_a_introduction_to_pytorch.html#练习-a-3","161":"bllm/appendix_a_introduction_to_pytorch.html#防止不期望的梯度累积","162":"bllm/appendix_a_introduction_to_pytorch.html#a-8-模型的保存与加载","163":"bllm/appendix_a_introduction_to_pytorch.html#a-9-使用-gpu-优化训练性能","164":"bllm/appendix_a_introduction_to_pytorch.html#a-9-1-pytorch-中的-gpu-计算","165":"bllm/appendix_a_introduction_to_pytorch.html#a-9-2-单-gpu-训练","166":"bllm/appendix_a_introduction_to_pytorch.html#mac-上的-pytorch","167":"bllm/appendix_a_introduction_to_pytorch.html#练习-a-4","168":"bllm/appendix_a_introduction_to_pytorch.html#a-9-3-使用多gpu进行训练","169":"bllm/appendix_a_introduction_to_pytorch.html#多gpu计算是可选的","170":"bllm/appendix_a_introduction_to_pytorch.html#交互式环境中的多gpu计算","171":"bllm/appendix_a_introduction_to_pytorch.html#a-10-总结","172":"bllm/appendix_a_introduction_to_pytorch.html#a-11-延伸阅读","173":"bllm/appendix_a_introduction_to_pytorch.html#a-12-练习答案","174":"bllm/appendix_a_introduction_to_pytorch.html#练习-a-3-1","175":"bllm/appendix_a_introduction_to_pytorch.html#练习-a-4-1","176":"bllm/appendix_c_exercise_solutions.html#附录-c-练习解答","177":"bllm/appendix_c_exercise_solutions.html#c-1-第2章","178":"bllm/appendix_c_exercise_solutions.html#c-2-第3章","179":"bllm/appendix_c_exercise_solutions.html#c-3-第4章","180":"bllm/appendix_c_exercise_solutions.html#c-4-第5章","181":"bllm/appendix_b_references_and_further_reading.html#附录-b-参考文献及延伸阅读","182":"bllm/appendix_b_references_and_further_reading.html#b-1-第1章","183":"bllm/appendix_b_references_and_further_reading.html#b-2-第2章","184":"bllm/appendix_b_references_and_further_reading.html#b-3-第3章","185":"bllm/appendix_b_references_and_further_reading.html#b-4-第4章","186":"bllm/appendix_b_references_and_further_reading.html#b-5-第5章","187":"bllm/appendix_d_adding_bells_and_whistles_to_the_training_loop.html#附录-d-为训练循环添加增强功能","188":"bllm/appendix_d_adding_bells_and_whistles_to_the_training_loop.html#d-1-学习率预热","189":"bllm/appendix_d_adding_bells_and_whistles_to_the_training_loop.html#d-2-余弦衰减","190":"bllm/appendix_d_adding_bells_and_whistles_to_the_training_loop.html#d-3-梯度裁剪","191":"#书籍列表","192":"markdown-examples.html#markdown-extension-examples","193":"markdown-examples.html#syntax-highlighting","194":"markdown-examples.html#custom-containers","195":"markdown-examples.html#more","196":"bllm/#欢迎"},"fieldIds":{"title":0,"titles":1,"text":2},"fieldLength":{"0":[3,1,52],"1":[1,3,1],"2":[2,4,2],"3":[2,4,2],"4":[2,4,2],"5":[1,3,11],"6":[1,1,1],"7":[1,1,77],"8":[4,1,105],"9":[3,1,69],"10":[3,1,92],"11":[3,1,156],"12":[3,1,54],"13":[3,4,116],"14":[4,1,154],"15":[3,1,56],"16":[3,1,77],"17":[1,1,1],"18":[1,1,60],"19":[3,1,110],"20":[2,1,44],"21":[5,3,47],"22":[1,3,13],"23":[1,3,57],"24":[1,3,49],"25":[1,3,46],"26":[4,1,51],"27":[3,5,62],"28":[4,5,55],"29":[1,5,61],"30":[1,5,29],"31":[3,1,49],"32":[1,4,58],"33":[4,4,47],"34":[1,4,58],"35":[1,4,25],"36":[1,4,42],"37":[3,1,109],"38":[1,4,23],"39":[2,4,19],"40":[4,4,24],"41":[2,4,24],"42":[3,4,168],"43":[4,6,71],"44":[5,6,123],"45":[8,6,93],"46":[3,1,144],"47":[1,6,77],"48":[3,1,262],"49":[3,1,54],"50":[3,1,1],"51":[1,3,40],"52":[5,3,86],"53":[5,8,367],"54":[3,3,279],"55":[1,9,83],"56":[1,9,13],"57":[5,3,249],"58":[2,3,229],"59":[5,3,200],"60":[5,3,370],"61":[4,7,56],"62":[6,7,36],"63":[3,3,301],"64":[6,6,25],"65":[3,3,47],"66":[1,1,1],"67":[1,1,40],"68":[1,2,37],"69":[3,1,19],"70":[5,4,163],"71":[4,4,236],"72":[1,6,101],"73":[1,6,88],"74":[1,6,29],"75":[4,4,39],"76":[3,6,108],"77":[1,6,176],"78":[3,1,363],"79":[3,1,91],"80":[4,4,208],"81":[2,6,10],"82":[5,4,115],"83":[3,4,114],"84":[2,5,9],"85":[2,5,9],"86":[5,1,100],"87":[3,6,7],"88":[4,1,170],"89":[1,7,321],"90":[2,7,5],"91":[3,7,7],"92":[3,1,21],"93":[1,1,1],"94":[1,1,61],"95":[3,1,87],"96":[3,1,79],"97":[2,1,15],"98":[3,3,26],"99":[3,3,230],"100":[3,3,26],"101":[3,5,57],"102":[4,5,61],"103":[3,5,57],"104":[3,1,123],"105":[1,4,55],"106":[2,4,59],"107":[2,4,40],"108":[2,4,54],"109":[2,4,74],"110":[4,4,135],"111":[8,4,20],"112":[7,4,75],"113":[8,4,33],"114":[1,4,15],"115":[3,1,62],"116":[2,4,66],"117":[2,4,24],"118":[2,4,40],"119":[2,4,46],"120":[1,4,10],"121":[1,4,183],"122":[3,4,74],"123":[3,4,64],"124":[1,4,70],"125":[3,1,17],"126":[4,4,28],"127":[4,4,69],"128":[1,4,102],"129":[4,4,131],"130":[2,6,80],"131":[3,6,33],"132":[6,6,13],"133":[3,1,41],"134":[4,1,1],"135":[1,4,40],"136":[4,4,43],"137":[4,6,42],"138":[4,6,111],"139":[5,6,13],"140":[2,8,47],"141":[4,8,65],"142":[3,8,28],"143":[3,8,63],"144":[5,8,32],"145":[3,6,3],"146":[3,6,3],"147":[3,4,48],"148":[5,7,23],"149":[6,6,45],"150":[3,6,62],"151":[6,6,86],"152":[3,4,85],"153":[3,4,31],"154":[1,10,98],"155":[3,4,199],"156":[3,4,55],"157":[1,10,163],"158":[3,10,67],"159":[3,4,109],"160":[3,10,54],"161":[1,10,144],"162":[3,4,47],"163":[5,4,13],"164":[7,8,92],"165":[6,8,106],"166":[3,11,27],"167":[3,11,6],"168":[4,8,12],"169":[1,10,63],"170":[1,10,206],"171":[3,4,25],"172":[3,4,115],"173":[3,4,1],"174":[4,6,50],"175":[4,6,43],"176":[3,1,10],"177":[3,3,61],"178":[3,3,45],"179":[3,3,80],"180":[3,3,136],"181":[3,1,1],"182":[3,3,163],"183":[3,3,87],"184":[3,3,104],"185":[3,3,162],"186":[3,3,172],"187":[3,1,135],"188":[3,3,102],"189":[3,3,79],"190":[3,3,280],"191":[1,1,3],"192":[3,1,14],"193":[2,3,27],"194":[2,3,21],"195":[1,3,11],"196":[1,1,44]},"averageFieldLength":[2.8883248730964466,4.025380710659896,76.09137055837562],"storedFields":{"0":{"title":"Runtime API Examples","titles":[]},"1":{"title":"Results","titles":["Runtime API Examples"]},"2":{"title":"Theme Data","titles":["Runtime API Examples","Results"]},"3":{"title":"Page Data","titles":["Runtime API Examples","Results"]},"4":{"title":"Page Frontmatter","titles":["Runtime API Examples","Results"]},"5":{"title":"More","titles":["Runtime API Examples"]},"6":{"title":"认识大型语言模型","titles":[]},"7":{"title":"本章内容","titles":["认识大型语言模型"]},"8":{"title":"1.1 什么是大型语言模型（LLM）？","titles":["认识大型语言模型"]},"9":{"title":"1.2 大型语言模型的应用","titles":["认识大型语言模型"]},"10":{"title":"1.3 构建和使用大型语言模型的阶段","titles":["认识大型语言模型"]},"11":{"title":"1.4 使用大型语言模型完成不同任务","titles":["认识大型语言模型"]},"12":{"title":"1.5 利用大型数据集","titles":["认识大型语言模型"]},"13":{"title":"GPT-3 数据集详解","titles":["认识大型语言模型","1.5 利用大型数据集"]},"14":{"title":"1.6 GPT 架构的深入分析","titles":["认识大型语言模型"]},"15":{"title":"1.7 构建大型语言模型","titles":["认识大型语言模型"]},"16":{"title":"1.8 总结","titles":["认识大型语言模型"]},"17":{"title":"处理文本数据","titles":[]},"18":{"title":"本章内容","titles":["处理文本数据"]},"19":{"title":"2.1 理解词嵌入","titles":["处理文本数据"]},"20":{"title":"2.2 文本词元化","titles":["处理文本数据"]},"21":{"title":"代码示例 2.1：将短篇小说作为文本示例导入 Python","titles":["处理文本数据","2.2 文本词元化"]},"22":{"title":"文本样本大小","titles":["处理文本数据","2.2 文本词元化"]},"23":{"title":"如何有效地拆分文本","titles":["处理文本数据","2.2 文本词元化"]},"24":{"title":"是否移除空白字符","titles":["处理文本数据","2.2 文本词元化"]},"25":{"title":"将词元化应用于整个短篇小说","titles":["处理文本数据","2.2 文本词元化"]},"26":{"title":"2.3 将词元转换为词元 ID","titles":["处理文本数据"]},"27":{"title":"代码示例 2.2：创建词汇表","titles":["处理文本数据","2.3 将词元转换为词元 ID"]},"28":{"title":"代码示例 2.3：实现简单的文本词元化器","titles":["处理文本数据","2.3 将词元转换为词元 ID"]},"29":{"title":"实例化并使用词元化器","titles":["处理文本数据","2.3 将词元转换为词元 ID"]},"30":{"title":"处理训练集中未包含的词","titles":["处理文本数据","2.3 将词元转换为词元 ID"]},"31":{"title":"2.4 添加特殊上下文词元","titles":["处理文本数据"]},"32":{"title":"修改词汇表以包含特殊词元","titles":["处理文本数据","2.4 添加特殊上下文词元"]},"33":{"title":"代码示例 2.4：处理未知单词的简单文本词元化器","titles":["处理文本数据","2.4 添加特殊上下文词元"]},"34":{"title":"实践中试用新词元化器","titles":["处理文本数据","2.4 添加特殊上下文词元"]},"35":{"title":"对词元化文本进行去词元化检查","titles":["处理文本数据","2.4 添加特殊上下文词元"]},"36":{"title":"其他特殊词元","titles":["处理文本数据","2.4 添加特殊上下文词元"]},"37":{"title":"2.5 字节对编码","titles":["处理文本数据"]},"38":{"title":"关键观察","titles":["处理文本数据","2.5 字节对编码"]},"39":{"title":"BPE 如何处理未知单词","titles":["处理文本数据","2.5 字节对编码"]},"40":{"title":"练习 2.1：字节对编码未知单词","titles":["处理文本数据","2.5 字节对编码"]},"41":{"title":"BPE 的基本原理","titles":["处理文本数据","2.5 字节对编码"]},"42":{"title":"2.6 使用滑动窗口进行数据采样","titles":["处理文本数据","2.5 字节对编码"]},"43":{"title":"代码示例 2.5：用于批量输入和目标的数据集","titles":["处理文本数据","2.5 字节对编码","2.6 使用滑动窗口进行数据采样"]},"44":{"title":"代码示例 2.6：用于生成输入-目标对批次的数据加载器","titles":["处理文本数据","2.5 字节对编码","2.6 使用滑动窗口进行数据采样"]},"45":{"title":"练习 2.2：不同 stride 和 context_size 的数据加载器","titles":["处理文本数据","2.5 字节对编码","2.6 使用滑动窗口进行数据采样"]},"46":{"title":"2.7 创建词元嵌入","titles":["处理文本数据"]},"47":{"title":"嵌入层与矩阵乘法的关系","titles":["处理文本数据","2.7 创建词元嵌入","2.6 使用滑动窗口进行数据采样"]},"48":{"title":"2.8 编码词元位置","titles":["处理文本数据"]},"49":{"title":"2.9 小结","titles":["处理文本数据"]},"50":{"title":"从零开始实现一个 GPT 模型来生成文本","titles":[]},"51":{"title":"本章内容","titles":["从零开始实现一个 GPT 模型来生成文本"]},"52":{"title":"4.1 编写 LLM 架构","titles":["从零开始实现一个 GPT 模型来生成文本"]},"53":{"title":"GPT-2 与 GPT-3 的对比","titles":["从零开始实现一个 GPT 模型来生成文本","4.1 编写 LLM 架构"]},"54":{"title":"4.2 使用层归一化来调整激活值","titles":["从零开始实现一个 GPT 模型来生成文本"]},"55":{"title":"有偏方差","titles":["从零开始实现一个 GPT 模型来生成文本","4.2 使用层归一化来调整激活值","GPT-2 与 GPT-3 的对比"]},"56":{"title":"层归一化与批量归一化的比较","titles":["从零开始实现一个 GPT 模型来生成文本","4.2 使用层归一化来调整激活值","GPT-2 与 GPT-3 的对比"]},"57":{"title":"4.3 使用 GELU 激活函数实现前馈神经网络","titles":["从零开始实现一个 GPT 模型来生成文本"]},"58":{"title":"4.4 添加捷径连接","titles":["从零开始实现一个 GPT 模型来生成文本"]},"59":{"title":"4.5 在 Transformer 块中连接注意力和线性层","titles":["从零开始实现一个 GPT 模型来生成文本"]},"60":{"title":"4.6 编写 GPT 模型代码","titles":["从零开始实现一个 GPT 模型来生成文本"]},"61":{"title":"练习 4.1：前馈模块和注意力模块中的参数数量","titles":["从零开始实现一个 GPT 模型来生成文本","4.6 编写 GPT 模型代码"]},"62":{"title":"练习 4.2：初始化更大规模的 GPT 模型","titles":["从零开始实现一个 GPT 模型来生成文本","4.6 编写 GPT 模型代码"]},"63":{"title":"4.7 生成文本","titles":["从零开始实现一个 GPT 模型来生成文本"]},"64":{"title":"练习 4.3 使用单独的 dropout 参数","titles":["从零开始实现一个 GPT 模型来生成文本","4.7 生成文本"]},"65":{"title":"4.8 总结","titles":["从零开始实现一个 GPT 模型来生成文本"]},"66":{"title":"在无标签数据上进行预训练","titles":[]},"67":{"title":"本章内容","titles":["在无标签数据上进行预训练"]},"68":{"title":"权重参数","titles":["在无标签数据上进行预训练","本章内容"]},"69":{"title":"5.1 评估生成文本模型","titles":["在无标签数据上进行预训练"]},"70":{"title":"5.1.1 使用 GPT 生成文本","titles":["在无标签数据上进行预训练","5.1 评估生成文本模型"]},"71":{"title":"5.1.2 计算文本生成损失","titles":["在无标签数据上进行预训练","5.1 评估生成文本模型"]},"72":{"title":"反向传播","titles":["在无标签数据上进行预训练","5.1 评估生成文本模型","5.1.2 计算文本生成损失"]},"73":{"title":"交叉熵损失","titles":["在无标签数据上进行预训练","5.1 评估生成文本模型","5.1.2 计算文本生成损失"]},"74":{"title":"困惑度","titles":["在无标签数据上进行预训练","5.1 评估生成文本模型","5.1.2 计算文本生成损失"]},"75":{"title":"5.1.3 计算训练集和验证集的损失","titles":["在无标签数据上进行预训练","5.1 评估生成文本模型"]},"76":{"title":"预训练 LLM 的成本","titles":["在无标签数据上进行预训练","5.1 评估生成文本模型","5.1.3 计算训练集和验证集的损失"]},"77":{"title":"使用可变长度进行训练","titles":["在无标签数据上进行预训练","5.1 评估生成文本模型","5.1.3 计算训练集和验证集的损失"]},"78":{"title":"5.2 训练大语言模型","titles":["在无标签数据上进行预训练"]},"79":{"title":"5.3 解码策略以控制随机性","titles":["在无标签数据上进行预训练"]},"80":{"title":"5.3.1 温度缩放","titles":["在无标签数据上进行预训练","5.3 解码策略以控制随机性"]},"81":{"title":"练习5.1","titles":["在无标签数据上进行预训练","5.3 解码策略以控制随机性","5.3.1 温度缩放"]},"82":{"title":"5.3.2 Top-k采样","titles":["在无标签数据上进行预训练","5.3 解码策略以控制随机性"]},"83":{"title":"5.3.3 修改文本生成函数","titles":["在无标签数据上进行预训练","5.3 解码策略以控制随机性"]},"84":{"title":"练习5.2","titles":["在无标签数据上进行预训练","5.3 解码策略以控制随机性","5.3.3 修改文本生成函数"]},"85":{"title":"练习5.3","titles":["在无标签数据上进行预训练","5.3 解码策略以控制随机性","5.3.3 修改文本生成函数"]},"86":{"title":"5.4 在 PyTorch 中加载和保存模型权重","titles":["在无标签数据上进行预训练"]},"87":{"title":"练习 5.4","titles":["在无标签数据上进行预训练","5.4 在 PyTorch 中加载和保存模型权重"]},"88":{"title":"5.5 从 OpenAI 加载预训练权重","titles":["在无标签数据上进行预训练"]},"89":{"title":"更新的下载说明","titles":["在无标签数据上进行预训练","5.5 从 OpenAI 加载预训练权重","练习 5.4"]},"90":{"title":"练习 5.5","titles":["在无标签数据上进行预训练","5.5 从 OpenAI 加载预训练权重","练习 5.4"]},"91":{"title":"练习 5.6","titles":["在无标签数据上进行预训练","5.5 从 OpenAI 加载预训练权重","练习 5.4"]},"92":{"title":"5.6 总结","titles":["在无标签数据上进行预训练"]},"93":{"title":"编码注意力机制","titles":[]},"94":{"title":"本章内容","titles":["编码注意力机制"]},"95":{"title":"3.1 处理长序列的问题","titles":["编码注意力机制"]},"96":{"title":"3.2 使用注意力机制捕获数据依赖关系","titles":["编码注意力机制"]},"97":{"title":"3.3 使用自注意力关注输入的不同部分","titles":["编码注意力机制"]},"98":{"title":"自注意力中的“自我”","titles":["编码注意力机制","3.3 使用自注意力关注输入的不同部分"]},"99":{"title":"3.3.1 一个简单的无可训练权重的自注意力机制","titles":["编码注意力机制","3.3 使用自注意力关注输入的不同部分"]},"100":{"title":"3.3.2 为所有输入标记计算注意力权重","titles":["编码注意力机制","3.3 使用自注意力关注输入的不同部分"]},"101":{"title":"步骤 1：计算所有输入对之间的点积","titles":["编码注意力机制","3.3 使用自注意力关注输入的不同部分","3.3.2 为所有输入标记计算注意力权重"]},"102":{"title":"步骤 2：归一化每行使得总和为 1","titles":["编码注意力机制","3.3 使用自注意力关注输入的不同部分","3.3.2 为所有输入标记计算注意力权重"]},"103":{"title":"步骤 3：计算所有上下文向量","titles":["编码注意力机制","3.3 使用自注意力关注输入的不同部分","3.3.2 为所有输入标记计算注意力权重"]},"104":{"title":"3.4 实现带可训练权重的自注意力机制","titles":["编码注意力机制"]},"105":{"title":"变量定义","titles":["编码注意力机制","3.4 实现带可训练权重的自注意力机制"]},"106":{"title":"计算查询、键和值向量","titles":["编码注意力机制","3.4 实现带可训练权重的自注意力机制"]},"107":{"title":"第二步：计算注意力得分","titles":["编码注意力机制","3.4 实现带可训练权重的自注意力机制"]},"108":{"title":"第三步：从注意力得分到注意力权重","titles":["编码注意力机制","3.4 实现带可训练权重的自注意力机制"]},"109":{"title":"最后一步：计算上下文向量","titles":["编码注意力机制","3.4 实现带可训练权重的自注意力机制"]},"110":{"title":"代码清单 3.1：一个紧凑的自注意力类","titles":["编码注意力机制","3.4 实现带可训练权重的自注意力机制"]},"111":{"title":"使用 PyTorch 的 Linear 层改进 SelfAttention_v1 实现","titles":["编码注意力机制","3.4 实现带可训练权重的自注意力机制"]},"112":{"title":"代码清单 3.2：使用 PyTorch Linear 层的自注意力类","titles":["编码注意力机制","3.4 实现带可训练权重的自注意力机制"]},"113":{"title":"练习 3.1 比较 SelfAttention_v1 和 SelfAttention_v2","titles":["编码注意力机制","3.4 实现带可训练权重的自注意力机制"]},"114":{"title":"下一节预告","titles":["编码注意力机制","3.4 实现带可训练权重的自注意力机制"]},"115":{"title":"3.5 使用因果注意力隐藏未来词","titles":["编码注意力机制"]},"116":{"title":"第一步：计算注意力权重","titles":["编码注意力机制","3.5 使用因果注意力隐藏未来词"]},"117":{"title":"第二步：生成遮罩矩阵","titles":["编码注意力机制","3.5 使用因果注意力隐藏未来词"]},"118":{"title":"第三步：应用遮罩","titles":["编码注意力机制","3.5 使用因果注意力隐藏未来词"]},"119":{"title":"第四步：重新归一化遮罩的注意力权重","titles":["编码注意力机制","3.5 使用因果注意力隐藏未来词"]},"120":{"title":"避免信息泄露的说明","titles":["编码注意力机制","3.5 使用因果注意力隐藏未来词"]},"121":{"title":"更高效的遮罩方法","titles":["编码注意力机制","3.5 使用因果注意力隐藏未来词"]},"122":{"title":"将 Dropout 应用于注意力权重矩阵","titles":["编码注意力机制","3.5 使用因果注意力隐藏未来词"]},"123":{"title":"代码清单 3.3：一个紧凑的因果注意力类","titles":["编码注意力机制","3.5 使用因果注意力隐藏未来词"]},"124":{"title":"代码解释","titles":["编码注意力机制","3.5 使用因果注意力隐藏未来词"]},"125":{"title":"3.6 从单头注意力扩展到多头注意力","titles":["编码注意力机制"]},"126":{"title":"3.6.1 堆叠多个单头注意力层","titles":["编码注意力机制","3.6 从单头注意力扩展到多头注意力"]},"127":{"title":"代码清单 3.4：实现多头注意力的包装类","titles":["编码注意力机制","3.6 从单头注意力扩展到多头注意力"]},"128":{"title":"具体代码示例","titles":["编码注意力机制","3.6 从单头注意力扩展到多头注意力"]},"129":{"title":"3.6.2 实现带有权重分割的多头注意力机制","titles":["编码注意力机制","3.6 从单头注意力扩展到多头注意力"]},"130":{"title":"示例：多头矩阵乘法的实现","titles":["编码注意力机制","3.6 从单头注意力扩展到多头注意力","3.6.2 实现带有权重分割的多头注意力机制"]},"131":{"title":"使用 MultiHeadAttention 类","titles":["编码注意力机制","3.6 从单头注意力扩展到多头注意力","3.6.2 实现带有权重分割的多头注意力机制"]},"132":{"title":"练习 3.3 初始化与 GPT-2 相同大小的注意力模块","titles":["编码注意力机制","3.6 从单头注意力扩展到多头注意力","3.6.2 实现带有权重分割的多头注意力机制"]},"133":{"title":"3.7 总结","titles":["编码注意力机制"]},"134":{"title":"附录 A. PyTorch 简介","titles":[]},"135":{"title":"本章内容","titles":["附录 A. PyTorch 简介"]},"136":{"title":"A.1 什么是 PyTorch","titles":["附录 A. PyTorch 简介"]},"137":{"title":"A.1.1 PyTorch 的三个核心组件","titles":["附录 A. PyTorch 简介","A.1 什么是 PyTorch"]},"138":{"title":"A.1.2 定义深度学习","titles":["附录 A. PyTorch 简介","A.1 什么是 PyTorch"]},"139":{"title":"A.1.3 安装 PyTorch","titles":["附录 A. PyTorch 简介","A.1 什么是 PyTorch"]},"140":{"title":"Python 版本","titles":["附录 A. PyTorch 简介","A.1 什么是 PyTorch","A.1.3 安装 PyTorch"]},"141":{"title":"使用 AMD GPU 进行深度学习","titles":["附录 A. PyTorch 简介","A.1 什么是 PyTorch","A.1.3 安装 PyTorch"]},"142":{"title":"PyTorch 和 Torch","titles":["附录 A. PyTorch 简介","A.1 什么是 PyTorch","A.1.3 安装 PyTorch"]},"143":{"title":"检查 GPU 是否被识别","titles":["附录 A. PyTorch 简介","A.1 什么是 PyTorch","A.1.3 安装 PyTorch"]},"144":{"title":"在 Apple Silicon 上使用 PyTorch","titles":["附录 A. PyTorch 简介","A.1 什么是 PyTorch","A.1.3 安装 PyTorch"]},"145":{"title":"练习 A.1","titles":["附录 A. PyTorch 简介","A.1 什么是 PyTorch"]},"146":{"title":"练习 A.2","titles":["附录 A. PyTorch 简介","A.1 什么是 PyTorch"]},"147":{"title":"A.2 理解张量","titles":["附录 A. PyTorch 简介"]},"148":{"title":"PyTorch 的 NumPy 类 API","titles":["附录 A. PyTorch 简介","A.2 理解张量","练习 A.2"]},"149":{"title":"A.2.1 标量、向量、矩阵和张量","titles":["附录 A. PyTorch 简介","A.2 理解张量"]},"150":{"title":"A.2.2 张量数据类型","titles":["附录 A. PyTorch 简介","A.2 理解张量"]},"151":{"title":"A.2.3 常见的 PyTorch 张量操作","titles":["附录 A. PyTorch 简介","A.2 理解张量"]},"152":{"title":"A.3 将模型视为计算图","titles":["附录 A. PyTorch 简介"]},"153":{"title":"A.4 自动微分简化计算","titles":["附录 A. PyTorch 简介"]},"154":{"title":"偏导数和梯度","titles":["附录 A. PyTorch 简介","A.4 自动微分简化计算","A.2.3 常见的 PyTorch 张量操作"]},"155":{"title":"A.5 实现多层神经网络","titles":["附录 A. PyTorch 简介"]},"156":{"title":"A.6 设置高效的数据加载器","titles":["附录 A. PyTorch 简介"]},"157":{"title":"类标签编号","titles":["附录 A. PyTorch 简介","A.6 设置高效的数据加载器","A.2.3 常见的 PyTorch 张量操作"]},"158":{"title":"num_workers=0 设置","titles":["附录 A. PyTorch 简介","A.6 设置高效的数据加载器","A.2.3 常见的 PyTorch 张量操作"]},"159":{"title":"A.7 典型的训练循环","titles":["附录 A. PyTorch 简介"]},"160":{"title":"练习 A.3","titles":["附录 A. PyTorch 简介","A.7 典型的训练循环","A.2.3 常见的 PyTorch 张量操作"]},"161":{"title":"防止不期望的梯度累积","titles":["附录 A. PyTorch 简介","A.7 典型的训练循环","A.2.3 常见的 PyTorch 张量操作"]},"162":{"title":"A.8 模型的保存与加载","titles":["附录 A. PyTorch 简介"]},"163":{"title":"A.9 使用 GPU 优化训练性能","titles":["附录 A. PyTorch 简介"]},"164":{"title":"A.9.1 PyTorch 中的 GPU 计算","titles":["附录 A. PyTorch 简介","A.9 使用 GPU 优化训练性能"]},"165":{"title":"A.9.2 单 GPU 训练","titles":["附录 A. PyTorch 简介","A.9 使用 GPU 优化训练性能"]},"166":{"title":"Mac 上的 PyTorch","titles":["附录 A. PyTorch 简介","A.9 使用 GPU 优化训练性能","A.9.2 单 GPU 训练"]},"167":{"title":"练习 A.4","titles":["附录 A. PyTorch 简介","A.9 使用 GPU 优化训练性能","A.9.2 单 GPU 训练"]},"168":{"title":"A.9.3 使用多GPU进行训练","titles":["附录 A. PyTorch 简介","A.9 使用 GPU 优化训练性能"]},"169":{"title":"多GPU计算是可选的","titles":["附录 A. PyTorch 简介","A.9 使用 GPU 优化训练性能","A.9.3 使用多GPU进行训练"]},"170":{"title":"交互式环境中的多GPU计算","titles":["附录 A. PyTorch 简介","A.9 使用 GPU 优化训练性能","A.9.3 使用多GPU进行训练"]},"171":{"title":"A.10 总结","titles":["附录 A. PyTorch 简介"]},"172":{"title":"A.11 延伸阅读","titles":["附录 A. PyTorch 简介"]},"173":{"title":"A.12 练习答案","titles":["附录 A. PyTorch 简介"]},"174":{"title":"练习 A.3：","titles":["附录 A. PyTorch 简介","A.12 练习答案"]},"175":{"title":"练习 A.4：","titles":["附录 A. PyTorch 简介","A.12 练习答案"]},"176":{"title":"附录 C. 练习解答","titles":[]},"177":{"title":"C.1 第2章","titles":["附录 C. 练习解答"]},"178":{"title":"C.2 第3章","titles":["附录 C. 练习解答"]},"179":{"title":"C.3 第4章","titles":["附录 C. 练习解答"]},"180":{"title":"C.4 第5章","titles":["附录 C. 练习解答"]},"181":{"title":"附录 B. 参考文献及延伸阅读","titles":[]},"182":{"title":"B.1 第1章","titles":["附录 B. 参考文献及延伸阅读"]},"183":{"title":"B.2 第2章","titles":["附录 B. 参考文献及延伸阅读"]},"184":{"title":"B.3 第3章","titles":["附录 B. 参考文献及延伸阅读"]},"185":{"title":"B.4 第4章","titles":["附录 B. 参考文献及延伸阅读"]},"186":{"title":"B.5 第5章","titles":["附录 B. 参考文献及延伸阅读"]},"187":{"title":"附录 D. 为训练循环添加增强功能","titles":[]},"188":{"title":"D.1 学习率预热","titles":["附录 D. 为训练循环添加增强功能"]},"189":{"title":"D.2 余弦衰减","titles":["附录 D. 为训练循环添加增强功能"]},"190":{"title":"D.3 梯度裁剪","titles":["附录 D. 为训练循环添加增强功能"]},"191":{"title":"书籍列表","titles":[]},"192":{"title":"Markdown Extension Examples","titles":[]},"193":{"title":"Syntax Highlighting","titles":["Markdown Extension Examples"]},"194":{"title":"Custom Containers","titles":["Markdown Extension Examples"]},"195":{"title":"More","titles":["Markdown Extension Examples"]},"196":{"title":"欢迎","titles":[]}},"dirtCount":0,"index":[["拥有深度学习的经验以及对pytorch的基础了解或对其他深度学习框架",{"2":{"196":1}}],["拥有优化的权重初始化方案",{"2":{"111":1}}],["揭示其开发和个性化的思路",{"2":{"196":1}}],["局限性及其定制方法",{"2":{"196":1}}],["希望帮助您深入理解llm的工作原理",{"2":{"196":1}}],["希望您期待这段激动人心的学习之旅",{"2":{"15":1}}],["熟悉我作品的朋友",{"2":{"196":1}}],["欢迎",{"0":{"196":1}}],["```",{"2":{"193":1}}],["书籍列表",{"0":{"191":1}}],["书中采用了详尽的步骤介绍",{"2":{"196":1}}],["书中的附加代码还提供了更大规模的数据集准备方法",{"2":{"75":1}}],["书中还会提供加载开源模型权重的代码示例",{"2":{"15":1}}],["线性预热",{"2":{"190":1}}],["线性层将输入与权重矩阵相乘并加上偏置向量",{"2":{"155":1}}],["找到最大的梯度值",{"2":{"190":1}}],["找到最大值的索引",{"2":{"63":1}}],["​",{"2":{"190":1}}],["范数",{"2":{"190":1}}],["范围为",{"2":{"71":1}}],["逐渐降低学习率",{"2":{"189":1}}],["逐步计算注意力权重",{"2":{"104":1}}],["逐步计算这些权重和生成的上下文向量",{"2":{"99":1}}],["逐步构建完整句子",{"2":{"63":1}}],["逐步生成文本的过程",{"2":{"63":1}}],["缩短的上下文长度",{"2":{"187":1}}],["缩写",{"2":{"60":1}}],["余弦衰减和梯度裁剪",{"2":{"190":1}}],["余弦衰减和梯度裁剪等技术的实现",{"2":{"187":1}}],["余弦衰减的逐渐减速更新有助于在训练后期保持训练的稳定性",{"2":{"189":1}}],["余弦衰减将学习率逐渐减少至接近零",{"2":{"189":1}}],["余弦衰减",{"0":{"189":1}}],["余弦退火和梯度裁剪等",{"2":{"78":1}}],["兼顾了效率和质量",{"2":{"186":1}}],["兼容的",{"2":{"140":2,"141":1}}],["束搜索",{"2":{"186":1}}],["首次提出top",{"2":{"186":1}}],["首先计算这些梯度的l2范数",{"2":{"190":1}}],["首先让我们定义一下计算图的概念",{"2":{"152":1}}],["首先需要按照常规方式安装",{"2":{"144":1}}],["首先我们对上节的注意力得分和权重进行操作",{"2":{"115":1}}],["首先选择logits值最高的token",{"2":{"82":1}}],["首先从第",{"2":{"53":1}}],["首先将所有单个字符添加到词汇表中",{"2":{"41":1}}],["首先",{"2":{"15":1,"16":1,"42":1,"48":1,"57":2,"63":1,"70":1,"79":1,"89":1,"104":1,"107":1,"122":1,"137":1,"156":1,"159":1,"164":1,"169":1,"170":1,"187":1}}],["超参数及架构细节",{"2":{"186":1}}],["超出本书范围",{"2":{"19":1}}],["感谢您购买",{"2":{"196":1}}],["感兴趣的读者可以参考openai的gpt",{"2":{"185":1}}],["感叹号的数量",{"2":{"8":1}}],["潜在空间及向量表示的概念感兴趣的读者",{"2":{"183":1}}],["说明并非所有llm都基于transformer架构",{"2":{"182":1}}],["说明了即使是相对较小的",{"2":{"61":1}}],["描述解码器风格gpt",{"2":{"182":1}}],["描绘了本节实现的自注意力机制如何融入本书和本章的上下文",{"2":{"104":1}}],["谷歌研究和deepmind的团队在医疗领域展示了这一点",{"2":{"182":1}}],["另一方面",{"2":{"180":1}}],["另一个广泛采用的复杂深度神经网络和llm的训练技术是余弦衰减",{"2":{"189":1}}],["另一个支持",{"2":{"140":1}}],["另一个张量",{"2":{"42":1}}],["另一个目标张量包含",{"2":{"42":1}}],["问答和教育内容",{"2":{"180":1}}],["问答等各种",{"2":{"7":1}}],["连贯的文本的情况",{"2":{"180":1}}],["连贯性",{"2":{"70":1}}],["估计概率为32",{"2":{"180":1}}],["复用4",{"2":{"179":1}}],["±",{"2":{"175":2}}],["µs",{"2":{"175":3}}],["累积梯度可能是有利的",{"2":{"172":1}}],["累加损失在",{"2":{"77":1}}],["介绍了微积分的基础内容",{"2":{"172":1}}],["介绍一个基本的自注意力框架",{"2":{"94":1}}],["读者可以观看我录制的15分钟视频教程",{"2":{"172":1}}],["作者的一段视频讲座",{"2":{"186":1}}],["作者发现去除value权重矩阵和投影层仍能实现较好的性能",{"2":{"184":1}}],["作者",{"2":{"172":5,"182":12,"183":4,"184":6,"185":8,"186":10}}],["作为一种分词方法",{"2":{"183":1}}],["作为实现深度神经网络的库",{"2":{"155":1}}],["作为深度学习基础数据结构的张量",{"2":{"135":1}}],["作为查询",{"2":{"104":1}}],["作为查询输入",{"2":{"104":1}}],["作为在大型数据集上自行预训练模型的替代方案",{"2":{"92":1}}],["作为对比",{"2":{"77":1}}],["作为可选检查",{"2":{"77":1}}],["作为生成的下一个",{"2":{"71":1}}],["作为输入上下文",{"2":{"63":1}}],["作为输入",{"2":{"63":1,"78":1}}],["作为下一轮预测的输入",{"2":{"63":1}}],["作为附加内容",{"2":{"62":1}}],["延伸阅读",{"0":{"172":1}}],["功能允许我们通过反向传播便捷地训练神经网络",{"2":{"171":1}}],["释放资源",{"2":{"170":1}}],["销毁进程组",{"2":{"170":1}}],["固定内存以提高数据传输性能",{"2":{"170":1}}],["禁止随机打乱数据",{"2":{"170":1}}],["禁用梯度跟踪和dropout",{"2":{"78":1}}],["地址为",{"2":{"170":1,"176":1}}],["地选择最可能的token",{"2":{"80":1}}],["交互式环境中的多gpu计算",{"0":{"170":1}}],["交叉熵损失是机器学习和深度学习中的一种常用度量",{"2":{"73":1}}],["交叉熵损失",{"0":{"73":1}}],["策略",{"2":{"169":1}}],["统计正确预测的数量",{"2":{"161":1}}],["正确的权重分配如下",{"2":{"178":1}}],["正确",{"2":{"161":1}}],["正如你在接下来的章节中将看到的",{"2":{"148":1}}],["正如你将看到的",{"2":{"104":1}}],["正如第1章第1节",{"2":{"138":1}}],["正如前文所述",{"2":{"96":1}}],["正如我们在上一节基于",{"2":{"54":1}}],["正如我们在第一章中所了解到的",{"2":{"42":1}}],["正如",{"2":{"14":1,"52":1}}],["正如图a",{"2":{"153":1}}],["正如图",{"2":{"8":1,"11":1,"19":2,"31":1,"57":1,"69":1,"99":1}}],["发现模型100",{"2":{"161":1}}],["发展的角度来看时间较早",{"2":{"14":1}}],["属于类别1的概率为0",{"2":{"161":1}}],["属性查看其维度",{"2":{"155":1}}],["属性中第一个",{"2":{"155":1}}],["属性中",{"2":{"154":1}}],["属性中的参数",{"2":{"88":1}}],["属性设置为",{"2":{"153":1}}],["属性设为",{"2":{"58":1}}],["属性允许我们访问张量的形状",{"2":{"151":1}}],["属性访问张量的数据类型",{"2":{"150":1}}],["属性访问其权重",{"2":{"68":1}}],["属性",{"2":{"16":1}}],["否则计算会失败",{"2":{"164":1}}],["否则",{"2":{"161":1}}],["否则输出为零",{"2":{"57":1}}],["防止不期望的梯度累积",{"0":{"161":1}}],["防止过拟合",{"2":{"59":1}}],["背景计算图中的梯度",{"2":{"160":1}}],["背后的核心架构",{"2":{"14":1}}],["顾名思义",{"2":{"160":1}}],["验证集类似于测试集",{"2":{"160":1}}],["合理使用时",{"2":{"158":1}}],["合并为一个步骤",{"2":{"42":1}}],["合并为子词",{"2":{"41":1}}],["崩溃",{"2":{"158":1}}],["反而可能引发问题",{"2":{"158":1}}],["反向传播可以看作是对神经网络应用微积分中的链式法则",{"2":{"153":1}}],["反向传播需要一个损失函数",{"2":{"72":1}}],["反向传播",{"0":{"72":1}}],["反向传播计算梯度",{"2":{"58":1}}],["充分利用系统资源",{"2":{"158":1}}],["尤其是通过我的博客",{"2":{"196":1}}],["尤其是llm的开发",{"2":{"196":1}}],["尤其是训练较大网络时",{"2":{"158":1}}],["尤其是在网络非常深或结构复杂的情况下",{"2":{"57":1}}],["迭代训练加载器时可以看到最后一个批次被忽略",{"2":{"157":1}}],["洗牌顺序将会变化",{"2":{"157":1}}],["测试加载器的迭代方式类似",{"2":{"157":1}}],["父类来创建一个自定义数据集类",{"2":{"157":1}}],["神经网络的输出层应该包含",{"2":{"157":1}}],["两种实验性",{"2":{"182":1}}],["两条数据属于类别1",{"2":{"156":1}}],["两个术语在文献中常被互换使用",{"2":{"11":1}}],["查看模型结构可以帮助了解模型",{"2":{"155":1}}],["查询",{"2":{"109":2,"133":1,"187":1}}],["查询向量",{"2":{"104":1}}],["演示了",{"2":{"155":1}}],["手动",{"2":{"154":1}}],["引擎在后台构建计算图",{"2":{"154":1}}],["引号和双短横线等",{"2":{"24":1}}],["偏导数和梯度",{"0":{"154":1}}],["偏置向量在llm中已不再常用",{"2":{"89":1}}],["σ",{"2":{"152":1}}],["逻辑回归的前向传播作为计算图",{"2":{"152":1}}],["逻辑回归前向传播",{"2":{"152":1}}],["关于计算图的概念",{"2":{"151":1}}],["关键观察",{"0":{"38":1}}],["常见的",{"0":{"151":1},"1":{"154":1,"157":1,"158":1,"160":1,"161":1}}],["常被用于语言建模任务",{"2":{"96":1}}],["浮点数创建张量",{"2":{"150":1}}],["整数创建了张量",{"2":{"150":1}}],["整合到",{"2":{"58":1}}],["依此类推",{"2":{"149":1}}],["采用了大部分",{"2":{"148":1}}],["采用了一种称为",{"2":{"8":1}}],["操作符",{"2":{"151":1}}],["操作和计算这些多维数组",{"2":{"147":1}}],["操作看起来复杂",{"2":{"129":1}}],["阶数表示维度的数量",{"2":{"147":1}}],["阶段",{"2":{"15":1}}],["阶段的重点在于通过小型数据集实现教学用途的训练",{"2":{"15":1}}],["阶段中",{"2":{"15":1}}],["运行第2章代码的补充部分",{"2":{"146":1}}],["芯片的",{"2":{"166":1}}],["芯片加速",{"2":{"144":1}}],["芯片",{"2":{"144":3}}],["菜单",{"2":{"143":1}}],["菜单中",{"2":{"143":1}}],["路径为",{"2":{"143":1}}],["笔记本环境是",{"2":{"143":1}}],["未在第5章中涉及",{"2":{"186":1}}],["未识别到",{"2":{"143":1}}],["未经训练的",{"2":{"65":1}}],["资源库",{"2":{"142":1}}],["环境或安装本书后续章节使用的其他库的更多建议和说明",{"2":{"142":1}}],["环境已安装必要的依赖项",{"2":{"140":1}}],["替换为",{"2":{"141":1}}],["替代了本节中使用的传统",{"2":{"55":1}}],["许多科学计算库无法立即支持最新的",{"2":{"140":1}}],["许多预训练",{"2":{"13":1}}],["版本是",{"2":{"140":1}}],["版本",{"0":{"140":1},"2":{"140":3,"141":2,"164":1}}],["版本模型",{"2":{"60":1}}],["那么训练和使用",{"2":{"138":1}}],["那么为什么实际参数数量为",{"2":{"60":1}}],["垃圾邮件或非垃圾邮件",{"2":{"138":1}}],["垃圾邮件",{"2":{"138":1}}],["垃圾邮件过滤器是机器学习的一个应用",{"2":{"8":1}}],["监督学习的预测建模工作流程包括训练阶段",{"2":{"138":1}}],["音频或文本等非结构化数据",{"2":{"138":1}}],["音频和文本等数据的原始形式",{"2":{"19":1}}],["音频和文本",{"2":{"19":1}}],["虚拟助手中的语音识别",{"2":{"138":1}}],["电子邮件垃圾过滤",{"2":{"138":1}}],["致力于开发从数据中学习的算法",{"2":{"138":1}}],["人工智能",{"2":{"138":1}}],["人类专家可能需要手动从邮件文本中提取特征",{"2":{"8":1}}],["满足研究人员和开发者的需求",{"2":{"137":1}}],["灵活且高效的构建模块",{"2":{"137":1}}],["扩展了以数组为导向的编程库",{"2":{"137":1}}],["扩展到",{"2":{"57":1}}],["部分原因在于其用户友好的界面和高效性",{"2":{"136":1}}],["什么是",{"0":{"136":1},"1":{"137":1,"138":1,"139":1,"140":1,"141":1,"142":1,"143":1,"144":1,"145":1,"146":1},"2":{"138":1}}],["什么是大型语言模型",{"0":{"8":1}}],["附录d涵盖了更高级的训练功能",{"2":{"186":1}}],["附录",{"0":{"134":1,"176":1,"181":1,"187":1},"1":{"135":1,"136":1,"137":1,"138":1,"139":1,"140":1,"141":1,"142":1,"143":1,"144":1,"145":1,"146":1,"147":1,"148":1,"149":1,"150":1,"151":1,"152":1,"153":1,"154":1,"155":1,"156":1,"157":1,"158":1,"159":1,"160":1,"161":1,"162":1,"163":1,"164":1,"165":1,"166":1,"167":1,"168":1,"169":1,"170":1,"171":1,"172":1,"173":1,"174":1,"175":1,"177":1,"178":1,"179":1,"180":1,"182":1,"183":1,"184":1,"185":1,"186":1,"188":1,"189":1,"190":1}}],["附近",{"2":{"71":1}}],["掩码",{"2":{"133":1}}],["访问",{"2":{"143":1}}],["访问未来的标记",{"2":{"133":1}}],["访问了它们",{"2":{"88":1}}],["示例",{"0":{"130":1}}],["示例文本被拆分为",{"2":{"24":1}}],["下面是",{"2":{"129":1}}],["下一步是通过",{"2":{"108":1}}],["下一步",{"2":{"99":1}}],["下一个token的生成是通过将模型输出转换为概率分数",{"2":{"92":1}}],["下一个词",{"2":{"8":1,"42":1}}],["下一token的logits结果如下",{"2":{"82":1}}],["下一小节将回顾我们在上一章末尾设置的文本生成",{"2":{"69":1}}],["下一章中我们将进行训练",{"2":{"51":1}}],["下一章我们将详细讲解",{"2":{"12":1}}],["下一节预告",{"0":{"114":1}}],["下一节中我们会修改代码以计算所有上下文向量",{"2":{"104":1}}],["下一节中",{"2":{"103":1,"124":1}}],["下一节将泛化代码以计算输入序列中的所有上下文向量",{"2":{"109":1}}],["下一节将介绍生成策略",{"2":{"78":1}}],["下一节将聚焦于",{"2":{"77":1}}],["下一节将详细讨论这一内容",{"2":{"36":1}}],["下一节也是本章的最后一节",{"2":{"47":1}}],["下一词预测任务属于自监督学习的一种形式",{"2":{"14":1}}],["下一词预测",{"2":{"14":1}}],["顺序处理的",{"2":{"128":1}}],["只需更改以下两行代码",{"2":{"180":1}}],["只需更改其他输入参数",{"2":{"128":1}}],["只需进行很小的代码更改",{"2":{"180":1}}],["只需将",{"2":{"157":1}}],["只需调用",{"2":{"155":1}}],["只需记住",{"2":{"154":1}}],["只是会对代码做一些修改",{"2":{"100":1}}],["只是某些结构元素重复的次数不同",{"2":{"89":1}}],["只是参数量从",{"2":{"53":1}}],["沿列维度拼接这些上下文向量矩阵",{"2":{"127":1}}],["若启用多个工作进程",{"2":{"158":1}}],["若将",{"2":{"158":1}}],["若设置",{"2":{"127":1}}],["若我们使用",{"2":{"127":1}}],["堆叠多个单头注意力层",{"0":{"126":1}}],["单",{"0":{"165":1},"1":{"166":1,"167":1}}],["单个因果注意力模块可以看作是单头注意力",{"2":{"125":1}}],["单词",{"2":{"80":1}}],["避免了设备不匹配错误",{"2":{"124":1}}],["避免信息泄露的说明",{"0":{"120":1}}],["缓冲区会自动随模型移动到相应的设备",{"2":{"124":1}}],["虽非所有场景都必须",{"2":{"124":1}}],["虽然在这里的",{"2":{"160":1}}],["虽然本书前几章主要关注用于教学目的的",{"2":{"143":1}}],["虽然本章从零开始介绍",{"2":{"135":1}}],["虽然矩阵乘法并非绝对必要",{"2":{"133":1}}],["虽然llm和数据集都相对较小",{"2":{"86":1}}],["虽然此模块的输入和输出维度相同",{"2":{"57":1}}],["虽然词元嵌入为每个词元提供了一致的向量表示",{"2":{"49":1}}],["虽然这比原始",{"2":{"48":1}}],["虽然图",{"2":{"42":1}}],["虽然我们可以使用预训练模型",{"2":{"19":1}}],["虽然",{"2":{"16":1}}],["虽然从深度学习和",{"2":{"14":1}}],["右上",{"2":{"121":1}}],["右侧是带有捷径连接的网络",{"2":{"58":1}}],["右侧解码器段展示了类似",{"2":{"11":1}}],["左上",{"2":{"121":1}}],["左侧是不带捷径连接的",{"2":{"58":1}}],["左侧编码器段展示了类似",{"2":{"11":1}}],["遮罩",{"2":{"121":1}}],["遮罩应用在计算完注意力权重之后",{"2":{"121":1}}],["遮掩词预测",{"2":{"11":1}}],["∞",{"2":{"121":2}}],["零化对角线以上的元素",{"2":{"115":1}}],["零样本学习指的是模型能够在没有具体示例的情况下处理全新任务",{"2":{"11":1}}],["获得因果注意力中的遮罩注意力权重矩阵的一种方法是",{"2":{"115":1}}],["获取",{"2":{"20":1}}],["忽略当前标记之后的标记",{"2":{"115":1}}],["头",{"2":{"114":1,"125":1}}],["应该可以观察到两个实例生成相同的输出",{"2":{"113":1}}],["应用于实际的注意力权重矩阵",{"2":{"122":1}}],["应用于注意力权重矩阵",{"0":{"122":1}}],["应用于注意力得分",{"2":{"115":1}}],["应用遮罩",{"0":{"118":1}}],["应用因果注意力遮罩",{"2":{"115":1}}],["应用到先前计算的损失时",{"2":{"74":1}}],["应用位置嵌入",{"2":{"60":1}}],["应用一个",{"2":{"60":1}}],["应用",{"2":{"53":1,"71":1,"121":1,"161":1}}],["应用同样的操作",{"2":{"47":1}}],["产生了不同的输出",{"2":{"112":1}}],["变换输入矩阵",{"2":{"110":1}}],["变量定义",{"0":{"105":1}}],["变量中",{"2":{"77":1}}],["变量名简明扼要",{"2":{"53":1}}],["变量",{"2":{"26":1,"54":1}}],["考虑到下一章的",{"2":{"109":1}}],["考虑两个已经映射为",{"2":{"71":1}}],["搜索和检索信息",{"2":{"109":1}}],["概念源于信息检索和数据库领域",{"2":{"109":1}}],["概率的负平均对数概率相似",{"2":{"73":1}}],["概率分布是llm在每个token生成步骤中为每个词汇生成的概率得分",{"2":{"80":1}}],["概率分布",{"2":{"73":1}}],["概率分数的损失",{"2":{"72":1}}],["概率分数",{"2":{"71":1}}],["概率如下",{"2":{"71":1}}],["概率也用于我们在本节中实现的评估指标",{"2":{"71":1}}],["概率",{"2":{"71":2,"72":2,"161":1}}],["导致梯度接近零",{"2":{"108":1}}],["导致模型可能无法有效学习",{"2":{"86":1}}],["投影到",{"2":{"106":1}}],["特指l2范数",{"2":{"190":1}}],["特定于上下文的值",{"2":{"106":1}}],["特别是大量神经元之间的互连关系",{"2":{"138":1}}],["特别是模型中的注意力模块",{"2":{"104":1}}],["特别是如果你是第一次接触它",{"2":{"98":1}}],["特别是当句子复杂且依赖关系跨度较长时",{"2":{"95":1}}],["特别是在向后传播中",{"2":{"58":1}}],["特别是残差网络",{"2":{"58":1}}],["特别是",{"2":{"42":1,"78":1}}],["特别适用于连接多个不相关文本",{"2":{"36":1}}],["键",{"2":{"104":1,"109":2,"187":1}}],["键和值张量进行投影并重塑",{"2":{"129":1}}],["键和值的输入维度",{"2":{"110":1}}],["键和值",{"2":{"109":1,"110":1}}],["键和值向量",{"0":{"106":1},"2":{"104":1}}],["键和值矩阵的计算",{"2":{"89":1}}],["优化训练性能",{"0":{"163":1},"1":{"164":1,"165":1,"166":1,"167":1,"168":1,"169":1,"170":1}}],["优化器中",{"2":{"160":1}}],["优化器",{"2":{"159":1}}],["优化器会重置",{"2":{"86":1}}],["优质",{"2":{"104":1}}],["带可训练权重的自注意力机制是基于前面所介绍的概念构建的",{"2":{"104":1}}],["完整的练习代码示例可以在补充的github仓库中找到",{"2":{"176":1}}],["完全一致",{"2":{"103":1}}],["完成赋值后",{"2":{"113":1}}],["完成以上代码执行后",{"2":{"89":1}}],["完成从文本输入到文本输出的循环",{"2":{"70":1}}],["完成对输入文本",{"2":{"11":1}}],["完成不同任务",{"2":{"10":1}}],["循环来更高效和紧凑地实现计算",{"2":{"133":1}}],["循环来计算所有输入对之间的点积",{"2":{"101":1}}],["循环的执行速度通常较慢",{"2":{"101":1}}],["循环",{"2":{"101":1}}],["循环神经网络",{"2":{"95":1}}],["α",{"2":{"99":1,"108":1}}],["α2t​",{"2":{"99":1}}],["α2t",{"2":{"99":1}}],["α21​",{"2":{"99":1}}],["α21",{"2":{"99":1}}],["点积是一种简洁的方式",{"2":{"133":1}}],["点积是一种将两个向量按元素相乘并求和的简便方式",{"2":{"99":1}}],["点积衡量了两个向量的相似性",{"2":{"99":1}}],["ω22​",{"2":{"107":1}}],["ω22",{"2":{"107":1}}],["ω",{"2":{"99":1,"108":1}}],["及其连接",{"2":{"184":1}}],["及其他所有输入元素",{"2":{"99":1}}],["及其变体专长于",{"2":{"11":1}}],["底部所示",{"2":{"99":1}}],["元素分别表示为",{"2":{"99":1}}],["决定",{"2":{"99":1}}],["内容一致",{"2":{"110":1}}],["内容如图",{"2":{"99":1}}],["内容的起始位置",{"2":{"36":1}}],["系列",{"2":{"96":1}}],["系统提供了在动态计算图中自动计算梯度的功能",{"2":{"152":1}}],["系统的关键",{"2":{"15":1}}],["系统通过查看前面的词来学习预测句子中的下一个词",{"2":{"14":1}}],["系统之间的自然语言交流",{"2":{"9":1}}],["必须将整个编码后的输入信息保存在一个单一的隐藏状态中",{"2":{"96":1}}],["记忆单元",{"2":{"95":1}}],["记住",{"2":{"71":1}}],["试图在最终的隐藏状态中捕捉整个句子的含义",{"2":{"95":1}}],["适合处理像文本这样的序列数据",{"2":{"95":1}}],["适用于文本的嵌入模型并不适用于音频或视频数据",{"2":{"19":1}}],["出现之前",{"2":{"95":1,"96":1}}],["出现之前缺少注意力机制的架构存在的问题",{"2":{"95":1}}],["出于简洁考虑不再赘述",{"2":{"157":1}}],["出于演示目的",{"2":{"47":1}}],["出于教学目的",{"2":{"22":1}}],["聚焦其在机制层面的运作",{"2":{"94":1}}],["供",{"2":{"94":1}}],["供人们用作通用工具来编写",{"2":{"13":1}}],["探讨在神经网络中使用注意力机制的原因",{"2":{"94":1}}],["贪婪解码",{"2":{"92":1}}],["默认会在后台构建这样的图",{"2":{"153":1}}],["默认会创建精度为",{"2":{"150":1}}],["默认采用",{"2":{"150":1}}],["默认情况下",{"2":{"92":1}}],["默认行为",{"2":{"55":1}}],["开发load",{"2":{"89":1}}],["开始用于演示",{"2":{"104":1}}],["开始于遍历每个训练轮次",{"2":{"78":1}}],["开始复习",{"2":{"71":1}}],["开始",{"2":{"63":2,"69":1,"78":1,"153":1,"157":1}}],["开始实现各个模块",{"2":{"53":1}}],["开始计数",{"2":{"47":1,"157":1}}],["论坛寻求帮助",{"2":{"89":1}}],["论文中提到的",{"2":{"14":1}}],["论文未公开具体训练数据集",{"2":{"13":1}}],["论文作者未进一步说明细节",{"2":{"13":1}}],["服务器问题或",{"2":{"89":1}}],["才能在",{"2":{"88":1}}],["公布了其",{"2":{"88":1}}],["帮助",{"2":{"99":1}}],["帮助模型避免对训练数据的过拟合",{"2":{"86":1}}],["帮助我们在接下来的章节中编写必要的代码来构建完整的",{"2":{"53":1}}],["帮助我们理解像",{"2":{"9":1}}],["丢弃最后一个批次的训练加载器",{"2":{"157":1}}],["丢弃",{"2":{"86":1,"121":1}}],["状态字典",{"2":{"86":1}}],["幸运的是",{"2":{"86":1,"88":1}}],["保存模型后",{"2":{"162":1}}],["保存模型以便以后使用或继续训练往往是有帮助的",{"2":{"86":1}}],["保存",{"2":{"88":1}}],["保存pytorch模型相对简单",{"2":{"86":1}}],["保存和加载模型权重",{"2":{"67":1}}],["建议或反馈",{"2":{"196":1}}],["建议查阅官方文档",{"2":{"150":1}}],["建议访问本书的补充",{"2":{"142":1}}],["建议访问",{"2":{"141":1}}],["建议安装",{"2":{"140":1}}],["建议使用较新的前一或两个版本的",{"2":{"140":1}}],["建议读者使用更大的文本数据集来训练模型",{"2":{"190":1}}],["建议读者尝试使用不同大小的",{"2":{"91":1}}],["建议读者简单检查文件内容",{"2":{"88":1}}],["建议同时保存优化器状态",{"2":{"86":1}}],["建议在加载openai的预训练权重后重新进行此练习",{"2":{"84":1}}],["建议阅读附录",{"2":{"58":1}}],["细心的读者可能会记得我们在3",{"2":{"82":1}}],["非重叠的",{"2":{"169":1}}],["非垃圾邮件",{"2":{"138":1}}],["非线性关系建模",{"2":{"138":1}}],["非top",{"2":{"82":1}}],["非常适合学习用途",{"2":{"75":1}}],["排除所有其他token",{"2":{"82":1}}],["你还可以考虑使用pytorch的完全分片数据并行",{"2":{"172":1}}],["你将学习张量这一基础概念及其在",{"2":{"135":1}}],["你将征服本书中最具挑战性的一部分",{"2":{"97":1}}],["你的任务是将",{"2":{"113":1}}],["你学习了如何为训练",{"2":{"94":1}}],["你可以在终端中执行以下命令安装这些库",{"2":{"88":1}}],["你可能会好奇它与层归一化的区别",{"2":{"56":1}}],["你能想到一种更快速且更准确的方法来确定",{"2":{"81":1}}],["几乎100",{"2":{"80":1}}],["温度采样和top",{"2":{"83":1}}],["温度5的分布更均匀",{"2":{"80":1}}],["温度为1时",{"2":{"80":1}}],["温度为1时表示每个token的概率分数未进行缩放",{"2":{"80":1}}],["温度小于1时",{"2":{"80":1}}],["温度缩放实际上是将logits除以一个大于0的数",{"2":{"80":1}}],["温度缩放",{"0":{"80":1},"1":{"81":1}}],["温度缩放和top",{"2":{"79":2}}],["再逐步添加可训练权重",{"2":{"94":1}}],["再使用load",{"2":{"86":1}}],["再通过argmax函数找到对应的token",{"2":{"80":1}}],["再将高频子词合并为词来构建词汇表",{"2":{"41":1}}],["插入到generate",{"2":{"79":1}}],["推荐我的文章",{"2":{"172":1}}],["推荐以下书籍",{"2":{"172":1}}],["推荐使用本书github仓库中的独立脚本",{"2":{"170":1}}],["推荐使用",{"2":{"140":1}}],["推荐的方法是保存模型的state",{"2":{"86":1}}],["推动了今天许多技术的进步",{"2":{"138":1}}],["推动了对人类语言的理解",{"2":{"16":1}}],["推理阶段则禁用",{"2":{"121":1}}],["推理过程并不需要gpu",{"2":{"79":1}}],["放置在generate",{"2":{"79":1}}],["届时不会发生这种过拟合现象",{"2":{"78":1}}],["起初",{"2":{"78":1}}],["看看效果",{"2":{"78":1}}],["紧凑的打印格式",{"2":{"78":1}}],["遍历训练集中的批次并进行多轮迭代",{"2":{"78":1}}],["遍历权重参数",{"2":{"58":1}}],["损失函数",{"2":{"160":1,"165":1}}],["损失函数和优化器",{"2":{"137":1}}],["损失值降至0",{"2":{"159":1}}],["损失值相对较高",{"2":{"77":1}}],["损失将接近",{"2":{"77":1}}],["子集",{"2":{"77":1}}],["剩余元素的值会按",{"2":{"121":1}}],["剩余",{"2":{"77":1}}],["长度",{"2":{"76":1}}],["长度和特征大小",{"2":{"59":1}}],["支持的深度学习工作空间",{"2":{"135":1}}],["支持的",{"2":{"76":1}}],["支持的输入大小",{"2":{"48":1}}],["化并分割为指定长度的文本块",{"2":{"76":1}}],["准备输入文本",{"2":{"94":1}}],["准备输入文本包括文本的词元化",{"2":{"46":1}}],["准备",{"2":{"76":1}}],["$690",{"2":{"76":1}}],["$30",{"2":{"76":1}}],["云服务器每小时大约",{"2":{"76":1}}],["共八个步骤",{"2":{"78":1}}],["共计",{"2":{"76":1}}],["共有",{"2":{"48":1}}],["万亿",{"2":{"76":1}}],["万美元的语言模型",{"2":{"16":1}}],["万美元的云计算费用",{"2":{"13":1}}],["见附录",{"2":{"75":1}}],["见图",{"2":{"9":1,"104":1}}],["选择适合您操作系统的安装命令",{"2":{"141":1}}],["选择小的嵌入维度",{"2":{"99":1}}],["选择公共领域的文本可以避免使用权相关问题",{"2":{"75":1}}],["选择目标",{"2":{"73":1}}],["返回一个0到1之间的分数",{"2":{"152":1}}],["返回",{"2":{"74":1,"128":1,"151":1}}],["返回的均值张量将是一个二维向量",{"2":{"54":1}}],["困惑度比原始损失值更具解释性",{"2":{"74":1}}],["困惑度可通过",{"2":{"74":1}}],["困惑度衡量模型预测的概率分布与数据集实际分布的匹配程度",{"2":{"74":1}}],["困惑度是一种常用于语言建模任务的度量",{"2":{"74":1}}],["困惑度",{"0":{"74":1}}],["批次大小和",{"2":{"73":1}}],["批次大小",{"2":{"73":1}}],["批量归一化在批次维度上进行归一化",{"2":{"56":1}}],["批量大小是训练",{"2":{"45":1}}],["批量大小为",{"2":{"45":1}}],["给定的模型生成的",{"2":{"73":1}}],["给定一个文本样本",{"2":{"42":1}}],["衡量两个概率分布之间的差异",{"2":{"73":1}}],["负的平均对数概率等于平均对数概率乘以",{"2":{"72":1}}],["稍后我们将需要这个图来计算反向传播所需的梯度",{"2":{"152":1}}],["稍后我们将添加可训练权重",{"2":{"99":1}}],["稍后将在",{"2":{"72":1}}],["稍后在",{"2":{"54":1}}],["稍后在本书中我们会实现这一点",{"2":{"48":1}}],["改进了权重衰减的方法",{"2":{"78":1}}],["改进生成文本的训练函数的构建模块",{"2":{"71":1}}],["改变了自然语言处理领域",{"2":{"16":1}}],["组件",{"2":{"154":1}}],["组输出",{"2":{"71":1}}],["组成的向量v",{"2":{"190":1}}],["组成",{"2":{"43":1,"53":1,"54":1}}],["至第",{"2":{"71":1,"72":3}}],["至关重要",{"2":{"16":1,"71":1,"99":1}}],["移除批次维度",{"2":{"70":1}}],["移除空白字符可以减少内存和计算需求",{"2":{"24":1}}],["参考文献及延伸阅读",{"0":{"181":1},"1":{"182":1,"183":1,"184":1,"185":1,"186":1}}],["参见图a",{"2":{"141":1}}],["参见第",{"2":{"70":2}}],["参数量从124m",{"2":{"185":1}}],["参数量可从数百万到数十亿不等",{"2":{"65":1}}],["参数时进行合理选择至关重要",{"2":{"158":1}}],["参数是并行加载和预处理数据的关键",{"2":{"158":1}}],["参数控制",{"2":{"131":1}}],["参数中指定较小的批次数",{"2":{"77":1}}],["参数的示意图",{"2":{"54":1}}],["参数指定",{"2":{"54":1}}],["参数",{"0":{"64":1},"2":{"52":1,"54":1,"88":1,"190":1}}],["经过20步上升到最大值",{"2":{"188":1}}],["经过训练后",{"2":{"70":1}}],["经过预训练",{"2":{"16":1}}],["减少超出损失最小值的风险",{"2":{"189":1}}],["减少损失",{"2":{"77":1}}],["减少了张量的维度",{"2":{"54":1}}],["减小为",{"2":{"70":1}}],["允许输入序列中的每个位置在计算序列表示时关注该序列中的所有位置",{"2":{"96":1}}],["允许通过",{"2":{"68":1}}],["允许用于",{"2":{"20":1}}],["评估生成文本模型",{"0":{"69":1},"1":{"70":1,"71":1,"72":1,"73":1,"74":1,"75":1,"76":1,"77":1}}],["评估模型性能以及保存和加载模型权重",{"2":{"67":1}}],["评估的基本原理",{"2":{"15":1}}],["值的偏置",{"2":{"187":1}}],["值得注意的是",{"2":{"97":1}}],["值转换为概率分数时",{"2":{"71":1}}],["值",{"2":{"64":1,"109":2,"133":1,"155":1}}],["率",{"2":{"64":1}}],["目的是适应图示",{"2":{"71":1}}],["目标位置概率越高越好",{"2":{"71":1}}],["目标是增加正确目标",{"2":{"71":1}}],["目标是输入序列向后移动一个位置",{"2":{"71":1}}],["目标对批次的数据加载器",{"0":{"44":1}}],["目标对最简单直观的方法之一是创建两个变量",{"2":{"42":1}}],["目标对是什么样子的呢",{"2":{"42":1}}],["目标对",{"2":{"42":3,"49":1}}],["目前",{"2":{"63":1,"76":1}}],["得到以下结果",{"2":{"179":1}}],["得到包含两组上下文向量矩阵的张量",{"2":{"127":1}}],["得到键和值向量",{"2":{"104":1}}],["得到相同的结果",{"2":{"63":1}}],["得益于深度学习的进步",{"2":{"7":1}}],["步骤",{"0":{"101":1,"102":1,"103":1},"2":{"99":1}}],["步骤是多余的",{"2":{"63":1}}],["步得到",{"2":{"72":1}}],["步中通过对数变换并求平均值",{"2":{"72":1}}],["步计算目标张量对应的",{"2":{"72":1}}],["步将",{"2":{"71":1}}],["步和第",{"2":{"71":1}}],["步所示",{"2":{"71":1}}],["步",{"2":{"71":3,"72":4}}],["步开始",{"2":{"53":1}}],["确切的运行时间将取决于用于该实验的硬件",{"2":{"175":1}}],["确定概率分布中最大值的位置",{"2":{"63":1}}],["确保每个关键细节都不会被遗漏",{"2":{"196":1}}],["确保不同进程获取不同数据",{"2":{"170":1}}],["确保注意力机制的平均影响在训练和推理阶段保持一致",{"2":{"121":1}}],["确保模型不会过于依赖特定的隐藏层单元",{"2":{"121":1}}],["确保在训练",{"2":{"58":1}}],["确保层输出只有正值",{"2":{"54":1}}],["直至训练结束时达到最低点",{"2":{"189":1}}],["直接传递给",{"2":{"160":1}}],["直接进入第2章",{"2":{"135":1}}],["直到达到最低值",{"2":{"189":1}}],["直到达到用户指定的生成",{"2":{"63":1}}],["直到模型在六次迭代后生成了完整句子",{"2":{"63":1}}],["直观和易于使用",{"2":{"9":1}}],["形成下一次迭代的新输入序列",{"2":{"63":1}}],["过程从将输入文本编码为",{"2":{"63":1}}],["过程中的一个迭代步骤",{"2":{"63":1}}],["过滤",{"2":{"12":1}}],["过滤处理可能会被应用",{"2":{"10":1}}],["详见以下论文",{"2":{"185":1}}],["详见附录",{"2":{"11":1}}],["详细讲解了损失函数及其通过对数变换以便于数学优化的内容",{"2":{"186":1}}],["详细信息见以下论文",{"2":{"185":1}}],["详细说明了",{"2":{"63":1}}],["详情请参见此概述及相关api文档链接",{"2":{"172":1}}],["详情见附录b",{"2":{"78":1}}],["详情见附录",{"2":{"72":1}}],["占用",{"2":{"61":1}}],["占位类",{"2":{"60":1}}],["占位符所看到的那样",{"2":{"54":1}}],["占位符",{"2":{"53":1}}],["意味着数据在计算缩放之前不会居中",{"2":{"185":1}}],["意味着数据批次由",{"2":{"48":1}}],["意味着该张量有2行3列",{"2":{"151":1}}],["意味着原始",{"2":{"60":1}}],["规范化最终输出",{"2":{"60":1}}],["索引位置为3",{"2":{"80":1}}],["索引",{"2":{"60":1}}],["索引转换为稠密向量",{"2":{"60":1}}],["初始学习率为0",{"2":{"188":1}}],["初始化一个具有",{"2":{"132":1}}],["初始化与",{"0":{"132":1}}],["初始化更大规模的",{"0":{"62":1}}],["初始化了",{"2":{"60":1}}],["初版模型的前身",{"2":{"10":1}}],["构造函数",{"2":{"157":1}}],["构造函数中实例化",{"2":{"155":1}}],["构造函数中定义网络层",{"2":{"155":1}}],["构造函数根据提供的配置字典",{"2":{"60":1}}],["构成注意力机制的核心部分",{"2":{"110":1}}],["构建最终的",{"2":{"53":1}}],["构建大型语言模型",{"0":{"15":1}}],["构建在原始",{"2":{"11":1}}],["构建",{"2":{"10":1,"18":1}}],["构建和使用大型语言模型的阶段",{"0":{"10":1}}],["项指定",{"2":{"60":1}}],["次迭代中",{"2":{"63":1}}],["次",{"2":{"60":3}}],["黑箱",{"2":{"60":1}}],["黑色的对勾表示已经涵盖的部分",{"2":{"57":1}}],["称之为",{"2":{"60":1}}],["称为bert",{"2":{"182":1}}],["称为贪婪解码",{"2":{"63":1}}],["称为",{"2":{"59":1,"138":1}}],["称为嵌入",{"2":{"49":1}}],["显示了一个仅包含",{"2":{"71":1}}],["显示了本章中我们实现的不同概念的心智模型",{"2":{"59":1}}],["显示了使用",{"2":{"57":1}}],["旨在使llm预训练更加高效",{"2":{"186":1}}],["旨在通过惩罚较大的权重来最小化模型复杂度并防止过拟合",{"2":{"78":1}}],["旨在转换这些向量",{"2":{"59":1}}],["旨在创造能够执行需要类人智能的任务的机器",{"2":{"8":1}}],["到目前为止",{"2":{"85":1,"159":1}}],["到训练结束时",{"2":{"78":1}}],["到概率的转换之后",{"2":{"71":1}}],["到概率的完整过程",{"2":{"63":1}}],["到",{"2":{"71":2,"72":1,"99":3,"104":1,"109":1}}],["到第六次迭代时",{"2":{"63":1}}],["到第一层",{"2":{"58":1}}],["到最大输入长度减",{"2":{"48":1}}],["典型的训练循环",{"0":{"159":1},"1":{"160":1,"161":1},"2":{"58":1}}],["典型的例子如",{"2":{"10":1}}],["假设梯度矩阵为g",{"2":{"190":1}}],["假设您安装了支持",{"2":{"164":1}}],["假设以下输入句子已嵌入为",{"2":{"99":1}}],["假设我们计划训练一个llm",{"2":{"188":1}}],["假设我们要开发一个将文本从一种语言翻译成另一种语言的模型",{"2":{"95":1}}],["假设我们有两个gpu要用于训练一个神经网络",{"2":{"169":1}}],["假设我们有如下张量",{"2":{"130":1}}],["假设我们有一个仅包含",{"2":{"46":1}}],["假设我们有以下四个输入词元",{"2":{"46":1}}],["假设llm接收起始内容",{"2":{"80":1}}],["假设给定层的权重参数矩阵为",{"2":{"58":1}}],["打印权重梯度的平均绝对值",{"2":{"58":1}}],["创造了一条绕过某些层的替代路径",{"2":{"58":1}}],["创建一个小型玩具数据集",{"2":{"156":1}}],["创建一个新的生成函数",{"2":{"83":1}}],["创建因果注意力机制",{"2":{"115":1}}],["创建每个元素的富信息表示",{"2":{"99":1}}],["创建输入嵌入",{"2":{"48":1}}],["创建嵌入向量",{"2":{"47":1}}],["创建词元嵌入",{"0":{"46":1},"1":{"47":1}}],["创建词汇表",{"0":{"27":1}}],["创建",{"2":{"10":1,"149":1}}],["捷径连接帮助缓解深度神经网络",{"2":{"65":1}}],["捷径连接",{"2":{"65":1}}],["捷径连接层和多头注意力模块",{"2":{"64":1}}],["捷径连接是非常大规模模型",{"2":{"58":1}}],["捷径连接是为计算机视觉中的深层网络",{"2":{"58":1}}],["捷径连接对于克服深度神经网络中的梯度消失问题所带来的限制非常重要",{"2":{"58":1}}],["捷径连接通过跳过一个或多个层来为梯度流通提供了一条替代的",{"2":{"58":1}}],["捷径连接通过将一层的输入添加到它的输出上",{"2":{"58":1}}],["梯度裁剪",{"0":{"190":1}}],["梯度会累积",{"2":{"161":1}}],["梯度或链式法则",{"2":{"154":1}}],["梯度是包含多元函数",{"2":{"154":1}}],["梯度值保持稳定",{"2":{"58":1}}],["梯度值的平均绝对值",{"2":{"58":1}}],["梯度变得越来越小",{"2":{"58":1}}],["梯度",{"2":{"58":1}}],["梯度消失问题指的是在训练过程中",{"2":{"58":1}}],["激活",{"2":{"57":1,"58":1,"59":2}}],["激活函数结合了经典relu激活函数和正态分布累积分布函数的特性",{"2":{"185":1}}],["激活函数的全连接前馈网络",{"2":{"65":1}}],["激活函数的小型神经网络",{"2":{"57":1}}],["激活函数的实现",{"2":{"57":1}}],["激活函数可以通过多种方式实现",{"2":{"57":1}}],["激活函数在深度学习中被广泛使用",{"2":{"57":1}}],["激活函数实现前馈神经网络",{"0":{"57":1}}],["激活函数",{"2":{"55":1,"57":1,"58":1}}],["数学上",{"2":{"190":1}}],["数组",{"2":{"147":1,"148":1}}],["数量和词汇表大小",{"2":{"73":1}}],["数量",{"2":{"57":1,"63":1,"71":1,"73":1,"76":1}}],["数据加载器可在后台准备下一个批次",{"2":{"158":1}}],["数据加载器进行测试",{"2":{"44":1}}],["数据加载会导致瓶颈",{"2":{"158":1}}],["数据加载将在主进程中完成",{"2":{"158":1}}],["数据加载的基本理念",{"2":{"156":1}}],["数据科学和机器学习调查",{"2":{"136":1}}],["数据用于训练",{"2":{"77":1}}],["数据准备过程包括将输入文本划分为训练和验证集",{"2":{"76":1}}],["数据中心",{"2":{"53":1}}],["数据库连接等",{"2":{"157":1}}],["数据库",{"2":{"13":1}}],["数据集类",{"2":{"157":1}}],["数据集类的代码如下所示",{"2":{"42":1}}],["数据集上的训练和验证集损失",{"2":{"90":1}}],["数据集包含",{"2":{"13":1}}],["数据集详解",{"0":{"13":1}}],["数据集描述",{"2":{"12":1}}],["数据集名称",{"2":{"12":1}}],["轴表示函数输出",{"2":{"57":1}}],["轴表示函数输入",{"2":{"57":1}}],["绘制的",{"2":{"57":1}}],["近似",{"2":{"57":1}}],["近年来",{"2":{"9":1}}],["zhao等人",{"2":{"186":1}}],["zhang和sennrich",{"2":{"185":1}}],["zzz",{"2":{"110":1}}],["z^",{"2":{"99":7,"100":1,"103":1,"104":4,"106":1,"109":3}}],["z",{"2":{"99":14,"100":2,"103":2,"104":8,"106":2,"109":6,"152":2,"154":2}}],["zero",{"2":{"78":1,"159":1,"161":1,"165":1,"172":1,"188":1,"189":1,"190":1}}],["zeros",{"2":{"54":1,"99":1}}],["zip",{"2":{"57":1}}],["φ",{"2":{"57":2}}],["门控线性单元",{"2":{"57":1}}],["加速深度神经网络训练",{"2":{"147":1}}],["加速的",{"2":{"140":1}}],["加载主章节中保存的模型和优化器",{"2":{"180":1}}],["加载",{"2":{"89":1}}],["加载到",{"2":{"88":1}}],["加载预训练权重",{"0":{"88":1},"1":{"89":1,"90":1,"91":1}}],["加载预训练权重用于",{"2":{"76":1}}],["加载预训练权重时",{"2":{"60":1}}],["加载预训练的权重",{"2":{"67":1}}],["加权线性单元",{"2":{"57":1}}],["加入了更多数据源",{"2":{"13":1}}],["历史上",{"2":{"57":1}}],["用小的随机数初始化模型权重是有利的",{"2":{"155":1}}],["用作",{"2":{"57":1}}],["用于通过在训练过程中随机丢弃神经网络中的单元",{"2":{"184":1}}],["用于图像分类的最早视觉transformer",{"2":{"182":1}}],["用于改变张量的数据类型",{"2":{"175":1}}],["用于设置高效的数据加载管道",{"2":{"171":1}}],["用于设计和训练各种深度学习模型",{"2":{"137":1}}],["用于表示标量",{"2":{"171":1}}],["用于表示训练数据中未出现的未知单词",{"2":{"31":1}}],["用于生成多个进程",{"2":{"170":1}}],["用于生成输入",{"0":{"44":1}}],["用于标识每个进程",{"2":{"170":1}}],["用于gpu到gpu通信",{"2":{"170":1}}],["用于寻找最佳的超参数设置",{"2":{"160":1}}],["用于创建训练和测试数据集",{"2":{"156":1}}],["用于模型优化的自动微分功能",{"2":{"137":1}}],["用于模型训练期间的验证评估",{"2":{"77":1}}],["用于大型语言模型",{"2":{"133":1}}],["用于组合多个单头注意力模块",{"2":{"128":1}}],["用于组装",{"2":{"55":1}}],["用于将查询",{"2":{"110":1}}],["用于索引和搜索",{"2":{"109":1}}],["用于探测输入序列的其他部分",{"2":{"109":1}}],["用于下一个词的预测",{"2":{"95":1}}],["用于语言翻译",{"2":{"95":1}}],["用于检查两个张量或数组",{"2":{"89":1}}],["用于推理",{"2":{"86":1}}],["用于评估其训练能力",{"2":{"78":1}}],["用于跟踪模型在训练过程中的改进情况",{"2":{"78":1}}],["用于计算输入的中间变换",{"2":{"133":1}}],["用于计算训练和验证加载器返回批次的交叉熵损失",{"2":{"77":1}}],["用于计算模型输出与指定目标",{"2":{"58":1}}],["用于在训练期间随机忽略部分隐藏层单元",{"2":{"121":1}}],["用于在训练",{"2":{"121":1}}],["用于在训练过程中衡量生成文本的质量",{"2":{"67":1}}],["用于在",{"2":{"64":1}}],["用于在模型的反向传播中计算梯度",{"2":{"58":1}}],["用于预测序列中的下一个",{"2":{"60":1}}],["用于预训练llm的主函数",{"2":{"78":1}}],["用于预训练",{"2":{"12":1}}],["用于演示捷径连接的神经网络",{"2":{"58":1}}],["用于指导权重更新",{"2":{"58":1}}],["用于防止归一化过程中出现除零错误",{"2":{"54":1}}],["用于决定是否在多头注意力的",{"2":{"53":1}}],["用于批量输入和目标的数据集",{"0":{"43":1}}],["用于分隔两个不相关的文本源",{"2":{"31":1}}],["用于教学目的",{"2":{"16":1}}],["×",{"2":{"54":1,"180":1}}],["水平方向",{"2":{"54":1}}],["垂直方向",{"2":{"54":1}}],["注意力权重是通过点积计算的",{"2":{"133":1}}],["注意力权重作为加权因子来衡量每个值向量的重要性",{"2":{"109":1}}],["注意力权重决定了上下文向量在多大程度上依赖输入的不同部分",{"2":{"106":1}}],["注意力得分计算是类似于简化自注意力机制中的点积计算",{"2":{"107":1}}],["注意力可能集中在输入序列和输出序列之间的关系上",{"2":{"98":1}}],["注意力机制将输入元素转换为包含所有输入信息的增强上下文向量表示",{"2":{"133":1}}],["注意力机制的启发",{"2":{"96":1}}],["注意力机制是一个复杂的话题",{"2":{"94":1}}],["注意力机制",{"2":{"67":1,"94":1,"96":1}}],["注意力头数量",{"2":{"53":1,"187":1}}],["注意",{"2":{"54":1,"113":1,"121":1,"141":1,"147":1,"157":1}}],["修改后的训练函数",{"2":{"190":1}}],["修改第",{"2":{"164":1}}],["修改合并到",{"2":{"122":1}}],["修改之前用于llm文本生成的generate",{"2":{"83":1}}],["修改文本生成函数",{"0":{"83":1},"1":{"84":1,"85":1}}],["修改词汇表以包含特殊词元",{"0":{"32":1}}],["修正线性单元",{"2":{"54":1}}],["被采样32次",{"2":{"180":1}}],["被采样的次数",{"2":{"180":1}}],["被采样的频率吗",{"2":{"81":1}}],["被转换回",{"2":{"70":1}}],["被归一化",{"2":{"54":1}}],["被分配了最大的词元",{"2":{"38":1}}],["方差为",{"2":{"54":3,"55":1}}],["方法后",{"2":{"190":1}}],["方法相同",{"2":{"175":1}}],["方法使用这些梯度更新模型参数",{"2":{"160":1}}],["方法则用于返回数据集的长度",{"2":{"157":1}}],["方法定义了如何通过索引返回数据集中的单个样本",{"2":{"157":1}}],["方法通常不需要我们自己实现",{"2":{"155":1}}],["方法通过查询和键相乘计算注意力得分",{"2":{"110":1}}],["方法描述了输入数据如何在网络中传递并构成计算图",{"2":{"155":1}}],["方法描述了数据流动的过程",{"2":{"53":1}}],["方法会自动处理计算过程中的微积分",{"2":{"154":1}}],["方法轻松更改张量的精度",{"2":{"150":1}}],["方法中可以访问的属性",{"2":{"157":1}}],["方法中逐一调用每个层",{"2":{"155":1}}],["方法中指定它们的相互作用",{"2":{"155":1}}],["方法中",{"2":{"124":1,"128":1,"157":1}}],["方法中添加捷径连接",{"2":{"58":1}}],["方法初始化了可训练的权重矩阵",{"2":{"110":1}}],["方法直接访问模型中所有的可训练参数",{"2":{"68":1}}],["方法创建了与",{"2":{"60":1}}],["方法是将输入向量",{"2":{"99":1}}],["方法是通过更新模型的权重",{"2":{"72":1}}],["方法是",{"2":{"58":1}}],["方法将张量移动到",{"2":{"164":1}}],["方法将词元化和转换为词元",{"2":{"42":1}}],["方法将这些词元",{"2":{"29":1,"37":1}}],["方法编码文本",{"2":{"37":1}}],["方法成功将词元",{"2":{"29":1}}],["方法接收一批输入",{"2":{"60":1}}],["方法接收词元",{"2":{"28":1}}],["方法接收文本样本",{"2":{"28":1}}],["方法和",{"2":{"28":1,"157":1}}],["方法",{"2":{"19":1,"27":2,"28":1,"40":1,"60":1,"63":1,"96":1,"151":1,"157":1,"172":1}}],["轻松实现自动微分",{"2":{"54":1,"58":1}}],["轻松完成的任务",{"2":{"7":1}}],["权重和偏置",{"2":{"153":1,"162":1}}],["权重矩阵",{"2":{"106":1}}],["权重的",{"2":{"88":1}}],["权重指的是学习过程中会调整的可训练参数",{"2":{"68":1}}],["权重参数是定义网络连接的学习系数",{"2":{"106":1}}],["权重参数与注意力权重的区别",{"2":{"106":1}}],["权重参数",{"0":{"68":1}}],["权重共享减少了模型的总体内存占用和计算复杂性",{"2":{"60":1}}],["权重",{"2":{"54":1,"106":1}}],["归一化这些得分",{"2":{"110":1}}],["归一化每行",{"2":{"102":1}}],["归一化每行使得总和为",{"0":{"102":1}}],["归一化注意力得分",{"2":{"99":1}}],["归一化后的层输出现在包含了负值",{"2":{"54":1}}],["归一化并最终通过线性输出层生成",{"2":{"53":1}}],["归一化层激活值以稳定神经网络的训练",{"2":{"51":1}}],["配置通过一个",{"2":{"53":1}}],["多年来",{"2":{"196":1}}],["多gpu计算是可选的",{"0":{"169":1}}],["多个头是通过创建一个",{"2":{"129":1}}],["多个头是通过",{"2":{"128":1}}],["多头矩阵乘法的实现",{"0":{"130":1}}],["多头表示将注意力机制分成多个独立的",{"2":{"125":1}}],["多头",{"2":{"114":1}}],["多头注意力将注意力机制分成多个",{"2":{"114":1}}],["多头注意力将注意力机制分成多个头",{"2":{"94":1}}],["多头注意力",{"2":{"59":1}}],["多头注意力机制中的注意力头数量",{"2":{"53":1}}],["多种算法和框架可用于生成词嵌入",{"2":{"19":1}}],["kiros和hinton",{"2":{"185":1}}],["kudo和richardson",{"2":{"183":1}}],["karpathy",{"2":{"183":1,"185":1}}],["karpathy的github仓库minbpe",{"2":{"183":1}}],["kaggle",{"2":{"136":3}}],["k值",{"2":{"180":2}}],["kkk",{"2":{"110":1}}],["kwk​",{"2":{"104":2,"105":1,"110":2}}],["k设置吗",{"2":{"84":1}}],["k设置",{"2":{"84":2}}],["k=1",{"2":{"180":1}}],["k=50",{"2":{"89":1}}],["k=25",{"2":{"83":1}}],["k=none",{"2":{"83":1,"180":1}}],["k选择范围之外的token的logit值设为负无穷",{"2":{"82":1}}],["k过程",{"2":{"82":1}}],["k",{"2":{"80":2,"82":4,"83":2,"89":4,"104":1,"108":2,"129":1}}],["k采样和温度缩放是基于llm和期望的输出多样性和随机性来调整的设置",{"2":{"180":1}}],["k采样的论文",{"2":{"186":1}}],["k采样的过程",{"2":{"82":1}}],["k采样的概念",{"2":{"82":1}}],["k采样中",{"2":{"82":1}}],["k采样",{"0":{"82":1},"2":{"79":2,"83":1}}],["keepdim=true",{"2":{"54":8,"55":2,"63":1,"71":1,"83":1,"119":1}}],["keys",{"2":{"80":1,"89":4,"106":2,"107":3,"108":1,"110":3,"112":3,"116":3,"121":1,"123":3,"129":7,"133":1}}],["key",{"2":{"53":2,"89":4,"105":1,"106":3,"109":1,"110":3,"112":2,"116":1,"123":2,"129":2,"178":2}}],["know",{"2":{"29":2,"78":1,"79":1,"83":1}}],["q^",{"2":{"104":1,"106":1}}],["qqq",{"2":{"104":1,"110":1}}],["qwq​",{"2":{"104":2,"105":1,"110":2}}],["q",{"2":{"89":4,"104":2,"106":2,"183":3}}],["question",{"2":{"182":1}}],["queries",{"2":{"110":2,"112":2,"116":1,"123":2,"129":6,"133":1}}],["query",{"2":{"53":2,"89":4,"99":4,"105":1,"106":2,"107":2,"109":1,"110":3,"112":2,"116":1,"123":2,"129":2,"178":2}}],["quite",{"2":{"78":1,"190":1}}],["quot",{"2":{"38":2,"40":4,"41":18,"52":2,"53":14,"57":2,"60":2,"63":20,"64":2,"71":6,"78":2,"86":2,"89":18,"99":4,"104":2,"162":6,"164":4,"170":2,"187":2}}],["qkv",{"2":{"53":2,"59":2,"70":1,"89":1,"112":1,"123":1,"127":2,"129":1,"187":1}}],["比gpt",{"2":{"185":1}}],["比较运算符进行验证",{"2":{"161":1}}],["比较来计算损失",{"2":{"152":1}}],["比较",{"0":{"113":1}}],["比率",{"2":{"53":1,"121":1}}],["比如在序列到序列模型中",{"2":{"98":1}}],["比如输入的分词和嵌入层",{"2":{"52":1}}],["比如翻译请求及对应的正确翻译文本",{"2":{"10":1}}],["比如去除格式化字符或不明语言的文档",{"2":{"10":1}}],["比如",{"2":{"9":1,"10":1,"13":1,"14":1,"45":1,"82":1,"89":1}}],["比如写小说",{"2":{"9":1}}],["比如邮件垃圾分类或一些可以通过手工规则或简单模型捕捉的模式识别",{"2":{"7":1}}],["消费级",{"2":{"53":1}}],["年的",{"2":{"136":1}}],["年开发了所谓的",{"2":{"96":1}}],["年",{"2":{"53":2}}],["年推出",{"2":{"14":1}}],["根据图a",{"2":{"156":1}}],["根据您的观察",{"2":{"84":1}}],["根据我的经验",{"2":{"60":1}}],["根据",{"2":{"53":1,"136":1}}],["根据用户的要求创作诗歌",{"2":{"9":1}}],["集群来进行训练和推理",{"2":{"53":1}}],["基类提供了许多功能",{"2":{"155":1}}],["基础知识",{"2":{"135":1}}],["基本相同",{"2":{"53":1}}],["基于上面执行的",{"2":{"155":1}}],["基于概率分布选择",{"2":{"63":1}}],["基于目标值和输出值的接近程度计算损失",{"2":{"58":1}}],["基于第",{"2":{"53":1}}],["基于以上配置",{"2":{"53":1}}],["基于这些架构的",{"2":{"11":1}}],["基于这些人工定义的特征生成的数据集将用于训练模型",{"2":{"8":1}}],["基于之前提到的",{"2":{"11":1}}],["基于",{"2":{"7":1,"11":1,"16":1,"133":1}}],["百万参数",{"2":{"52":1}}],["原因在于一个叫做权重共享",{"2":{"60":1}}],["原论文提到的参数数量是",{"2":{"52":1}}],["原始为1024",{"2":{"187":1}}],["原始的",{"2":{"70":1}}],["原始模型也是用这种近似进行训练的",{"2":{"57":1}}],["原始",{"2":{"10":1,"11":1,"14":1,"80":1}}],["原始文本",{"2":{"10":1}}],["块之后",{"2":{"60":1}}],["块重复",{"2":{"60":1}}],["块在",{"2":{"60":1}}],["块在输出中保持了输入维度不变",{"2":{"59":1}}],["块将成为我们实现的",{"2":{"59":1}}],["块结合了层归一化",{"2":{"59":1}}],["块结合了多个组件",{"2":{"59":1}}],["块时保持不变",{"2":{"59":1}}],["块架构保持形状不变是其设计中的关键特点",{"2":{"59":1}}],["块并输入一些示例数据",{"2":{"59":1}}],["块组件",{"2":{"59":1}}],["块内部的操作",{"2":{"59":1}}],["块处理一个输入序列时",{"2":{"59":1}}],["块连接到",{"2":{"59":1}}],["块的",{"2":{"65":1}}],["块的输出进行标准化",{"2":{"60":1}}],["块的输出通过一个最终层归一化步骤",{"2":{"60":1}}],["块的输出是与输入相同维度的向量",{"2":{"59":1}}],["块的一部分",{"2":{"57":1}}],["块的数量",{"2":{"53":1}}],["块和层归一化部分使用了占位符",{"2":{"53":1}}],["块进行处理",{"2":{"53":1}}],["块中连接注意力和线性层",{"0":{"59":1}}],["块中使用的小型神经网络模块",{"2":{"57":1}}],["块中",{"2":{"53":1,"58":1}}],["块",{"2":{"52":2,"53":1,"59":3,"60":2,"62":3,"65":1}}],["块来创建不同规模的",{"2":{"51":1}}],["设备是执行计算并存储数据的地方",{"2":{"164":1}}],["设备",{"2":{"143":1,"164":1}}],["设计用于一次生成一个词",{"2":{"52":1}}],["设置top",{"2":{"180":1}}],["设置max",{"2":{"177":2}}],["设置分布式采样器",{"2":{"170":1}}],["设置当前进程的gpu设备",{"2":{"170":1}}],["设置主节点端口为",{"2":{"170":1}}],["设置主节点地址为",{"2":{"170":1}}],["设置高效的数据加载器",{"0":{"156":1},"1":{"157":1,"158":1}}],["设置深度学习环境和工作空间",{"2":{"135":1}}],["设置",{"0":{"158":1},"2":{"64":1,"89":2,"160":1}}],["设置决定了跨批次输入的移动位置数",{"2":{"44":1}}],["设置为4通常能在许多真实数据集中取得最佳性能",{"2":{"158":1}}],["设置为1或更大可能并无必要",{"2":{"158":1}}],["设置为大于",{"2":{"158":1}}],["设置为0时",{"2":{"158":1}}],["设置为输入窗口大小",{"2":{"44":1}}],["设置为",{"2":{"44":2,"54":1,"76":1,"106":1}}],["设置上下文大小为",{"2":{"44":1}}],["计算速度约快四倍",{"2":{"175":1}}],["计算的主要概念至关重要",{"2":{"164":1}}],["计算的核心概念",{"2":{"163":1}}],["计算预测准确率的函数",{"2":{"161":1}}],["计算预测值",{"2":{"63":1}}],["计算梯度",{"2":{"154":1}}],["计算图中计算损失梯度的最常用方式是从右向左应用链式法则",{"2":{"153":1}}],["计算图展示了神经网络中计算输出所需的计算步骤",{"2":{"152":1}}],["计算图",{"2":{"152":1}}],["计算完注意力得分后或将注意力权重应用到值向量后",{"2":{"121":1}}],["计算注意力得分",{"0":{"107":1},"2":{"107":1}}],["计算注意力权重和上下文向量后",{"2":{"130":1}}],["计算注意力权重矩阵",{"2":{"110":1}}],["计算注意力权重",{"0":{"116":1},"2":{"106":1}}],["计算查询",{"0":{"106":1},"2":{"104":1}}],["计算出输入向量的加权和作为上下文向量",{"2":{"104":1}}],["计算所有上下文向量",{"0":{"103":1}}],["计算所有输入对之间的点积",{"0":{"101":1}}],["计算所有输入元素的注意力权重和上下文向量",{"2":{"100":1}}],["计算上下文向量",{"0":{"109":1},"2":{"99":1}}],["计算中使用了",{"2":{"101":1}}],["计算中间值",{"2":{"99":1}}],["计算中添加偏置向量",{"2":{"53":1}}],["计算具有openai预训练权重的gptmodel在",{"2":{"90":1}}],["计算单个批次的损失",{"2":{"77":1}}],["计算训练集和验证集的损失",{"0":{"75":1},"1":{"76":1,"77":1}}],["计算训练和验证集损失",{"2":{"67":1}}],["计算文本生成损失",{"0":{"71":1},"1":{"72":1,"73":1,"74":1}}],["计算每个",{"2":{"62":1}}],["计算并比较前馈模块和多头注意力模块中的参数数量",{"2":{"61":1}}],["计算它们的嵌入值",{"2":{"60":1}}],["计算当前层的输出",{"2":{"58":1}}],["计算输入索引的标记和位置嵌入",{"2":{"53":1}}],["计算",{"0":{"164":1},"2":{"51":1,"74":1,"99":1,"120":1,"143":1,"160":1,"164":1}}],["绝对位置嵌入和相对位置嵌入",{"2":{"49":1}}],["绝对位置嵌入直接与序列中的特定位置关联",{"2":{"48":1}}],["检索对应的值",{"2":{"109":1}}],["检索与词元",{"2":{"49":1}}],["检查你的环境是否设置正确",{"2":{"146":1}}],["检查",{"0":{"143":1}}],["检查是否可以应用捷径连接",{"2":{"58":1}}],["检查它是否能还原原始输入",{"2":{"40":1}}],["小时",{"2":{"76":1}}],["小结",{"0":{"49":1}}],["小说创作",{"2":{"11":1}}],["主干",{"2":{"53":1}}],["主干的占位符架构开始",{"2":{"53":1}}],["主层的输入嵌入",{"2":{"48":1}}],["主要使用pytorch",{"2":{"196":1}}],["主要是指创建能够执行通常需要人类智能的任务的计算机系统",{"2":{"138":1}}],["主要是为了便于逐步讲解",{"2":{"109":1}}],["主要是一些固定模板代码",{"2":{"88":1}}],["主要步骤见图",{"2":{"72":1}}],["主要用于文本分类任务",{"2":{"11":1}}],["主要研究神经网络",{"2":{"7":1}}],["要评估过拟合程度",{"2":{"180":1}}],["要实验更大的模型",{"2":{"180":1}}],["要实例化其他gpt模型大小",{"2":{"179":1}}],["要实现图",{"2":{"77":1}}],["要求类标签从标签",{"2":{"157":1}}],["要将张量重塑为",{"2":{"151":1}}],["要在generate函数中强制实现确定性行为",{"2":{"180":1}}],["要在",{"2":{"144":1}}],["要定义文本的",{"2":{"70":1}}],["要小",{"2":{"48":1}}],["要理解本书内容并不需要对梯度有深奥的数学理解",{"2":{"54":1}}],["要理解",{"2":{"11":1}}],["位浮点张量",{"2":{"150":1}}],["位浮点数占用更少的内存和计算资源",{"2":{"150":1}}],["位浮点数在大多数深度学习任务中提供了足够的精度",{"2":{"150":1}}],["位浮点数主要是为了在精度和计算效率之间取得平衡",{"2":{"150":1}}],["位浮点数",{"2":{"61":1}}],["位整数张量转换为",{"2":{"150":1}}],["位整数数据类型",{"2":{"150":1}}],["位计算进行了优化",{"2":{"150":1}}],["位的张量",{"2":{"150":1}}],["位置嵌入张量包含四个",{"2":{"48":1}}],["位置嵌入被添加到词元嵌入向量中",{"2":{"48":1}}],["位于文本结尾",{"2":{"36":1}}],["存储的权重矩阵是转置形式",{"2":{"113":1}}],["存储容量约为",{"2":{"13":1}}],["存在两种主要的位置信息嵌入",{"2":{"49":1}}],["存在两种主要的位置信息嵌入方法",{"2":{"48":1}}],["向后遍历网络直到输入层",{"2":{"153":1}}],["向",{"2":{"48":1}}],["向量是",{"2":{"149":1}}],["向量是阶为1的张量",{"2":{"147":1}}],["向量",{"0":{"149":1},"2":{"11":1,"71":1,"104":1,"171":1}}],["前20步线性学习率预热后",{"2":{"189":1}}],["前归一化",{"2":{"185":1}}],["前向和反向传播在每个gpu上独立执行",{"2":{"169":1}}],["前向传播",{"2":{"58":1}}],["前两节介绍了增加llm生成文本多样性的两个概念",{"2":{"83":1}}],["前馈模块的参数数量大约是注意力模块的两倍",{"2":{"179":1}}],["前馈模块和注意力模块中的参数数量",{"0":{"61":1}}],["前馈模块和捷径连接",{"2":{"58":1}}],["前馈神经网络层",{"2":{"60":1}}],["前馈神经网络模块",{"2":{"57":1}}],["前馈网络",{"2":{"59":1}}],["前馈层和",{"2":{"59":1}}],["前面介绍的嵌入层工作方式是",{"2":{"48":1}}],["前称",{"2":{"9":1}}],["没有这些数据",{"2":{"86":1}}],["没有",{"2":{"54":1}}],["没有位置或顺序的概念",{"2":{"48":1}}],["没有编码器",{"2":{"14":1}}],["理想情况下",{"2":{"159":1}}],["理论上使用两个gpu可以将训练一个epoch的时间缩短一半",{"2":{"169":1}}],["理论上",{"2":{"48":1}}],["理解权衡并在设置",{"2":{"158":1}}],["理解张量",{"0":{"147":1},"1":{"148":1,"149":1,"150":1,"151":1}}],["理解词元之间的顺序和关系的能力",{"2":{"48":1}}],["理解词嵌入",{"0":{"19":1}}],["理解句子结构",{"2":{"23":1}}],["理解",{"2":{"7":1,"14":1}}],["矩阵以及更高维的数组",{"2":{"171":1}}],["矩阵是",{"2":{"149":1}}],["矩阵是阶为2的张量",{"2":{"147":1}}],["矩阵和张量",{"0":{"149":1}}],["矩阵中的一半元素随机设为零",{"2":{"121":1}}],["矩阵",{"2":{"47":1,"54":1}}],["行与之前计算的上下文向量",{"2":{"103":1}}],["行数",{"2":{"54":1}}],["行相同",{"2":{"46":1}}],["行和",{"2":{"46":1}}],["会计算图中所有叶节点的梯度",{"2":{"154":1}}],["会计算模型中每层的损失梯度",{"2":{"58":1}}],["会影响当前标记",{"2":{"120":1}}],["会话中",{"2":{"88":1}}],["会以约4",{"2":{"80":1}}],["会重新探讨并实现权重共享的概念",{"2":{"60":1}}],["会在后台构建这样的计算图",{"2":{"152":1}}],["会在训练过程中自动调整",{"2":{"54":1}}],["会在行间执行操作",{"2":{"54":1}}],["会将这些位置视为零概率",{"2":{"121":1}}],["会将这些",{"2":{"53":1}}],["会将",{"2":{"48":1}}],["会将一个唯一的嵌入添加到词元的嵌入中",{"2":{"48":1}}],["会发现它与矩阵的第",{"2":{"46":1}}],["会根据具体的模型变体和大小而变化",{"2":{"19":1}}],["维张量为",{"2":{"149":1}}],["维张量",{"2":{"149":3}}],["维而不是",{"2":{"128":1}}],["维度",{"2":{"105":1}}],["维度对应每个词汇表中的独特",{"2":{"53":1}}],["维上下文向量",{"2":{"103":1}}],["维向量",{"2":{"99":1,"106":1}}],["维向量的输入",{"2":{"59":1}}],["维嵌入维度",{"2":{"127":1}}],["维嵌入向量",{"2":{"122":1}}],["维嵌入",{"2":{"62":3,"99":1,"128":1}}],["维的上下文向量",{"2":{"127":1}}],["维的嵌入向量",{"2":{"99":1,"128":1}}],["维的输出向量转换回",{"2":{"60":1}}],["维的表示",{"2":{"57":1}}],["维的矩阵",{"2":{"54":1}}],["维的词元嵌入张量中",{"2":{"48":1}}],["维的",{"2":{"48":2}}],["维的向量转换回",{"2":{"53":1}}],["维的向量",{"2":{"48":2,"53":2}}],["维的向量中",{"2":{"48":2}}],["维的向量表示",{"2":{"48":1}}],["维",{"2":{"46":1,"48":1,"59":1,"60":1,"128":1,"132":1}}],["章编写的",{"2":{"104":1}}],["章对",{"2":{"104":1}}],["章讨论过的概念",{"2":{"95":1}}],["章的",{"2":{"76":1,"77":1}}],["章的数据加载实现中讨论过",{"2":{"71":1}}],["章",{"2":{"70":2,"71":2,"95":1}}],["章所述",{"2":{"60":1,"99":1}}],["章介绍的",{"2":{"53":1}}],["章加载的预训练权重",{"2":{"55":1}}],["章加载",{"2":{"53":1}}],["章实现的文本生成过程",{"2":{"70":1}}],["章实现",{"2":{"53":1}}],["章开始我们将开始实现这些模块",{"2":{"48":1}}],["章详细介绍",{"2":{"48":1}}],["章中数据加载器生成的批次输出",{"2":{"122":1}}],["章中讨论",{"2":{"96":1}}],["章中使用的短篇小说",{"2":{"76":1}}],["章中使用的",{"2":{"75":1}}],["章中用于实现",{"2":{"68":1}}],["章中用于实现多头注意力模块和第",{"2":{"68":1}}],["章中从",{"2":{"60":1}}],["章中所学到的",{"2":{"59":1}}],["章中编码的分词器",{"2":{"53":1}}],["章中已介绍",{"2":{"53":1}}],["章中的掩码多头注意力模块和我们在第",{"2":{"59":1}}],["章中的",{"2":{"53":1}}],["章中加载这些权重",{"2":{"53":1}}],["章中",{"2":{"46":1,"52":1,"60":2}}],["章节中",{"2":{"70":1}}],["章节",{"2":{"19":1}}],["增加了三种增强方法",{"2":{"190":1}}],["增加到",{"2":{"45":1}}],["增大",{"2":{"158":1}}],["增强其生成连贯且上下文相关输出的能力",{"2":{"11":1}}],["就像标准训练循环中的过程一样",{"2":{"190":1}}],["就通过比例缩小它们",{"2":{"190":1}}],["就能自动完成计算",{"2":{"164":1}}],["就不需要每次都重新运行训练过程",{"2":{"86":1}}],["就完成了图",{"2":{"71":1}}],["就可以防止批次之间的重叠",{"2":{"44":1}}],["就是这种定制化模型的典型例子",{"2":{"10":1}}],["较低的k值和温度更有帮助",{"2":{"180":1}}],["较低的困惑度表示模型预测更接近实际分布",{"2":{"74":1}}],["较大的top",{"2":{"180":1}}],["较大的点积会使",{"2":{"108":1}}],["较大的大型语言模型",{"2":{"55":1}}],["较高的温度值会使下一个token的概率分布更接近均匀分布",{"2":{"82":1}}],["较高和较低的温度",{"2":{"80":1}}],["较早的架构",{"2":{"59":1}}],["较早且广泛使用的一个例子是",{"2":{"19":1}}],["较小",{"2":{"44":1}}],["请访问",{"2":{"141":2}}],["请访问本章的在线代码资源库",{"2":{"89":1}}],["请理解两者的权重关系",{"2":{"113":1}}],["请参阅以下文章",{"2":{"172":1}}],["请参阅附录",{"2":{"42":1,"43":1}}],["请参见附录",{"2":{"57":1}}],["请注意",{"2":{"44":1,"52":1,"88":1,"135":1,"138":1,"142":1,"162":2}}],["按降序排列",{"2":{"82":1}}],["按照图",{"2":{"57":1}}],["按批次加载输入",{"2":{"43":1}}],["按词逐步处理文本",{"2":{"18":1}}],["结构必须与保存的原始模型完全匹配",{"2":{"162":1}}],["结构上的相似性",{"2":{"113":1}}],["结构以及各词之间的关系",{"2":{"8":1}}],["结果为",{"2":{"174":1}}],["结果gpt模型将无法生成连贯的文本",{"2":{"89":1}}],["结果将是一个",{"2":{"48":1}}],["结合所有其他输入元素的信息",{"2":{"99":1}}],["结合了带掩码的多头注意力模块和使用",{"2":{"65":1}}],["结合了我们之前介绍的多个概念",{"2":{"59":1}}],["结合使用",{"2":{"43":1}}],["我热烈邀请您参与",{"2":{"196":1}}],["我也希望对您同样有效",{"2":{"196":1}}],["我感到无比兴奋",{"2":{"196":1}}],["我深耕于深度学习领域",{"2":{"196":1}}],["我诚邀您与我一起踏上学习旅程",{"2":{"196":1}}],["我写了一个免费章节",{"2":{"172":1}}],["我建议继续阅读以了解如何将此数据集与",{"2":{"43":1}}],["我们改进了第5章的train",{"2":{"190":1}}],["我们先初始化一个新模型并为一个训练批次计算损失",{"2":{"190":1}}],["我们先简要介绍这些新导入的工具和distributeddataparallel类的用途",{"2":{"170":1}}],["我们先简要回顾代码细节",{"2":{"159":1}}],["我们引入了梯度裁剪",{"2":{"190":1}}],["我们引入的第一个技术是学习率预热",{"2":{"188":1}}],["我们引入另一种称为top",{"2":{"82":1}}],["我们加载",{"2":{"187":1}}],["我们实验的是最小的gpt",{"2":{"180":1}}],["我们实现一个简单的训练循环模板来说明预热过程",{"2":{"188":1}}],["我们实现一个简化的自注意力机制",{"2":{"99":1}}],["我们实现一个实用函数",{"2":{"77":1}}],["我们实现一个函数",{"2":{"58":1,"80":1}}],["我们实现了一个",{"2":{"128":1}}],["我们实现了一个简单的词元化方案以示例操作",{"2":{"37":1}}],["我们实现了一个简单的词元化器并将其应用于训练集中的一段文字",{"2":{"31":1}}],["我们实现了一种结合温度缩放的概率采样方法",{"2":{"82":1}}],["我们实现了数据采样",{"2":{"67":1}}],["我们实现了",{"2":{"61":1}}],["我们实现了图",{"2":{"58":1}}],["我们实现",{"2":{"57":1}}],["我们实现的词元化方案将文本拆分为单词和标点符号",{"2":{"24":1}}],["我们成功训练了一个模型",{"2":{"162":1}}],["我们成功地将",{"2":{"106":1}}],["我们学习了如何使用",{"2":{"161":1}}],["我们学习并编写了多头注意力机制",{"2":{"51":1}}],["我们希望选择一个学习率",{"2":{"159":1}}],["我们定义了一个自定义神经网络模型",{"2":{"156":1}}],["我们定义load",{"2":{"89":1}}],["我们以多层感知机",{"2":{"155":1}}],["我们以迭代的方式生成",{"2":{"63":1}}],["我们无需手动计算导数或梯度",{"2":{"154":1}}],["我们",{"2":{"154":1}}],["我们介绍了计算图的概念",{"2":{"153":1}}],["我们介绍了",{"2":{"152":1,"155":1}}],["我们也希望在训练和推理阶段之间添加模型评估",{"2":{"138":1}}],["我们也可以关闭张量值的科学计数法显示",{"2":{"54":1}}],["我们简要总结一下这些术语之间的关系",{"2":{"138":1}}],["我们简要回顾上一章中使用的generate",{"2":{"79":1}}],["我们添加了因果注意力掩码",{"2":{"133":1}}],["我们添加一个额外的",{"2":{"101":1}}],["我们堆叠多个单头注意力层",{"2":{"129":1}}],["我们指定了注意力头的数量",{"2":{"127":1}}],["我们指定了一个损失函数",{"2":{"58":1}}],["我们专注于神经网络中的因果注意力概念和实现",{"2":{"124":1}}],["我们专注于第二个输入元素",{"2":{"99":1}}],["我们依次添加了可训练权重和因果注意力遮罩",{"2":{"124":1}}],["我们新增了",{"2":{"124":1}}],["我们确保代码能够处理由多个输入组成的批次",{"2":{"122":1}}],["我们试图遮罩的",{"2":{"120":1}}],["我们遮罩了对角线以上的注意力权重",{"2":{"115":2}}],["我们随后计算上下文向量",{"2":{"110":1}}],["我们逐步计算了自注意力的输出",{"2":{"109":1}}],["我们这里选择不同的输入",{"2":{"105":1}}],["我们利用权重矩阵",{"2":{"104":1}}],["我们的目标是针对特定的输入元素",{"2":{"104":1}}],["我们的目标是计算每个输入元素",{"2":{"99":1}}],["我们完成了一个简单自注意力机制的代码演练",{"2":{"103":1}}],["我们为输入元素",{"2":{"104":1}}],["我们为第",{"2":{"100":1}}],["我们为了演示简便使用了较小的嵌入尺寸",{"2":{"48":1}}],["我们仔细匹配openai实现中的权重和我们的gptmodel实现",{"2":{"89":1}}],["我们仍需所有输入元素的键和值向量以便与查询",{"2":{"106":1}}],["我们仍需将其从",{"2":{"89":1}}],["我们仍然可以使用",{"2":{"54":1}}],["我们下载并加载了最小的",{"2":{"89":1}}],["我们讨论了如何通过数值评价训练进展并从零开始预训练llm",{"2":{"86":1}}],["我们仅计算了单个上下文向量",{"2":{"109":1}}],["我们仅打印了字典键",{"2":{"89":1}}],["我们仅关注与最高logits值关联的3个token",{"2":{"82":1}}],["我们仅仅实现了",{"2":{"63":1}}],["我们看到",{"2":{"82":1}}],["我们看到我们当前的",{"2":{"63":1}}],["我们现在将",{"2":{"122":1}}],["我们现在可以使用它并通过之前的generate函数生成新文本",{"2":{"89":1}}],["我们现在可以用pytorch中的multinomial函数替换argmax",{"2":{"80":1}}],["我们现在已经实现了",{"2":{"57":1}}],["我们始终通过torch",{"2":{"80":1}}],["我们刚刚创建的train",{"2":{"78":1}}],["我们有五行数据",{"2":{"157":1}}],["我们有",{"2":{"77":1}}],["我们首先对一个",{"2":{"121":1}}],["我们首先使用",{"2":{"116":1}}],["我们首先将模型从gpu传回cpu",{"2":{"79":1}}],["我们首先定义一个小的assign工具函数",{"2":{"89":1}}],["我们首先定义",{"2":{"77":1}}],["我们首先回顾了上一章的文本生成过程",{"2":{"69":1}}],["我们用三个权重矩阵",{"2":{"110":1}}],["我们用一个非常小的词汇表来讨论下一个token生成过程",{"2":{"80":1}}],["我们用一个实际示例来演示如何将词元",{"2":{"46":1}}],["我们用相似大小的文本块来训练模型",{"2":{"77":1}}],["我们计算每个训练集批次的损失",{"2":{"78":1}}],["我们计算了两个小文本输入的损失用于说明",{"2":{"74":1}}],["我们计算这些对数概率的平均值",{"2":{"72":1}}],["我们应用梯度裁剪",{"2":{"190":1}}],["我们应用额外的",{"2":{"121":1}}],["我们应用softmax函数将这些值转换为下一个token的概率",{"2":{"82":1}}],["我们应用",{"2":{"73":1}}],["我们如何最大化这些目标",{"2":{"72":1}}],["我们发现输出",{"2":{"71":1}}],["我们采用上一章介绍的",{"2":{"70":1}}],["我们设置了批量大小为2",{"2":{"157":1}}],["我们设置了在",{"2":{"157":1}}],["我们设置",{"2":{"70":1}}],["我们会使用更低的",{"2":{"121":1}}],["我们会将代码组织成一个紧凑的python",{"2":{"104":1}}],["我们会将此评估函数应用于整个训练数据集",{"2":{"71":1}}],["我们会将这些嵌入的权重初始化为随机值作为初始步骤",{"2":{"46":1}}],["我们会调整",{"2":{"63":1}}],["我们编写了一个简化的注意力机制代码",{"2":{"104":1}}],["我们编写了转换步骤以展示从",{"2":{"63":1}}],["我们编写的词元化器可以基于训练集中的词元进行词元化和去词元化",{"2":{"30":1}}],["我们重复该过程多次",{"2":{"63":1}}],["我们之前提到要初始化一个",{"2":{"60":1}}],["我们展示了",{"2":{"60":1}}],["我们打印这些",{"2":{"58":1}}],["我们来讨论捷径连接",{"2":{"58":1}}],["我们来创建初始位置嵌入",{"2":{"48":1}}],["我们稍后会这样做",{"2":{"57":1}}],["我们覆盖了实现",{"2":{"55":1}}],["我们选择使用adamw优化器",{"2":{"78":1}}],["我们选择较小的数据集是为了便于在普通笔记本电脑上快速运行代码示例",{"2":{"75":1}}],["我们选择这种方法以确保与",{"2":{"55":1}}],["我们选择设置",{"2":{"55":1}}],["我们初始化了一个有两个输入和两个输出的模型",{"2":{"159":1}}],["我们初始化了一个参数量为",{"2":{"62":1}}],["我们初始化权重矩阵",{"2":{"105":1}}],["我们初始化一个新的",{"2":{"57":1}}],["我们初始化一个拥有",{"2":{"53":1}}],["我们初始设置为",{"2":{"53":1}}],["我们对两条文本输入进行分词",{"2":{"53":1}}],["我们对词元化后的数据使用滑动窗口方法",{"2":{"49":1}}],["我们聚焦于",{"2":{"53":1}}],["我们使用了学习率为",{"2":{"159":1}}],["我们使用了一个非常小的文本数据集",{"2":{"75":1}}],["我们使用一个小型数据集",{"2":{"88":1}}],["我们使用pytorch的where函数",{"2":{"82":1}}],["我们使用",{"2":{"63":1,"121":1}}],["我们使用较小的嵌入维度来简化示例",{"2":{"52":1}}],["我们使用嵌入模型将这些原始数据转化为密集的向量表示",{"2":{"19":1}}],["我们已经重新实例化了第5章中使用的模型和数据加载器",{"2":{"187":1}}],["我们已经讨论了训练神经网络所需的所有内容",{"2":{"159":1}}],["我们已经介绍过使用",{"2":{"151":1}}],["我们已经介绍了如何预训练llm并使用它们生成文本",{"2":{"85":1}}],["我们已经有方法评估生成文本的质量",{"2":{"77":1}}],["我们已经通过第",{"2":{"72":1}}],["我们已经逐步编写了层归一化代码并应用到数据上",{"2":{"54":1}}],["我们已经从上到下概览了",{"2":{"53":1}}],["我们已经覆盖了几个关键方面",{"2":{"52":1}}],["我们已经创建了可以用于",{"2":{"42":1}}],["我们创建一个包含五个训练样本的简单玩具数据集",{"2":{"156":1}}],["我们创建一个字典",{"2":{"89":1}}],["我们创建的",{"2":{"48":1}}],["我们创建词汇表并展示前",{"2":{"26":1}}],["我们只需要创建另一个与",{"2":{"48":1}}],["我们从",{"2":{"150":1}}],["我们从初始化将要在本章中评估和训练的",{"2":{"70":1}}],["我们从设置",{"2":{"69":1}}],["我们从中提取出与下一个",{"2":{"63":1}}],["我们从第",{"2":{"48":1}}],["我们从此数据集中获取另一个批次",{"2":{"44":1}}],["我们假设这些词元",{"2":{"48":1}}],["我们假设词元",{"2":{"47":1}}],["我们还定义了20个预热步骤",{"2":{"188":1}}],["我们还可以添加一个",{"2":{"133":1}}],["我们还可以看到",{"2":{"46":1}}],["我们还将探讨llm预训练和微调中的常用流程与范式",{"2":{"196":1}}],["我们还将探讨其他评估模型质量的方法",{"2":{"70":1}}],["我们还将深入了解",{"2":{"135":1}}],["我们还将做一些其他修改",{"2":{"129":1}}],["我们还将了解基本的模型评估技术",{"2":{"67":1}}],["我们还将讨论如何使用和添加特殊的上下文词元",{"2":{"31":1}}],["我们还将学习",{"2":{"15":1}}],["我们还将学习如何让",{"2":{"9":1}}],["我们就可以创建图",{"2":{"42":1}}],["我们出于演示目的移除数据集中的前",{"2":{"42":1}}],["我们屏蔽了超过目标词之后的所有词",{"2":{"42":1}}],["我们详细介绍了词元化步骤以及将字符串词元转换为整数词元",{"2":{"42":1}}],["我们根据代码示例",{"2":{"32":1}}],["我们在每个gpu上创建一个模型副本",{"2":{"169":1}}],["我们在处理每个标记时会对未来标记进行遮罩",{"2":{"115":1}}],["我们在图中只展示了单个输入文本的",{"2":{"110":1}}],["我们在本章中调用了它",{"2":{"79":1}}],["我们在本章开始时提供了一个",{"2":{"60":1}}],["我们在模型中三个不同位置使用了",{"2":{"64":1}}],["我们在",{"2":{"60":1,"64":1,"130":1,"155":1}}],["我们在第",{"2":{"60":1,"95":1}}],["我们在创建下一个批次时将输入窗口向右移动",{"2":{"44":1}}],["我们在文本上滑动一个输入窗口",{"2":{"44":1}}],["我们在这些文本之间添加",{"2":{"31":1}}],["我们在词汇表中添加特殊词元以应对某些上下文",{"2":{"31":1}}],["我们通常使用第三个数据集",{"2":{"160":1}}],["我们通常采用更高维度的嵌入",{"2":{"19":1}}],["我们通过继承",{"2":{"157":1}}],["我们通过堆叠多个单头注意力模块创建了一个",{"2":{"129":1}}],["我们通过缩放注意力得分并使用",{"2":{"108":1}}],["我们通过绘制原始概率及不同温度值缩放后的概率来说明这一点",{"2":{"80":1}}],["我们通过softmax函数将logits转换为概率",{"2":{"80":1}}],["我们通过将训练数据集中的文本词元化来构建词汇表",{"2":{"26":1}}],["我们进一步修改词元化方案",{"2":{"24":1}}],["我们移除了空白字符",{"2":{"24":1}}],["我们修改正则表达式",{"2":{"23":1}}],["我们不能逐词翻译",{"2":{"95":1}}],["我们不希望随机丢弃网络学到的信息",{"2":{"86":1}}],["我们不会跳过任何单词",{"2":{"45":1}}],["我们不会将文本全部转为小写",{"2":{"23":1}}],["我们不需要为训练数据收集显式标签",{"2":{"14":1}}],["我们需要重新初始化第5章中训练的模型",{"2":{"187":1}}],["我们需要将投影维度d",{"2":{"178":1}}],["我们需要导入一些分布式训练所需的子模块",{"2":{"170":1}}],["我们需要介绍设备",{"2":{"164":1}}],["我们需要在批次维度上展平这些张量",{"2":{"73":1}}],["我们需要执行这些初始步骤",{"2":{"71":1}}],["我们需要实现一种数值方法来评估生成内容",{"2":{"70":1}}],["我们需要返回两个张量",{"2":{"42":1}}],["我们需要先构建一个所谓的",{"2":{"26":1}}],["我们需要一种方法将单词表示为连续数值向量",{"2":{"19":1}}],["我们需要简要回顾最初为机器翻译设计的",{"2":{"11":1}}],["我们特别关注了基于",{"2":{"18":1}}],["我们概述了大型语言模型",{"2":{"18":1}}],["我们可以定义一个find",{"2":{"190":1}}],["我们可以修改配置字典",{"2":{"179":1}}],["我们可以修改词元化器使用",{"2":{"31":1}}],["我们可以确保模型不会产生分歧",{"2":{"169":1}}],["我们可以确认训练数据集中没有出现单词",{"2":{"35":1}}],["我们可以访问对应的权重参数矩阵",{"2":{"155":1}}],["我们可以看到",{"2":{"155":1}}],["我们可以看到验证集损失从高值",{"2":{"78":1}}],["我们可以利用它来计算损失函数相对于模型参数",{"2":{"152":1}}],["我们可以利用庞大的未标注文本数据集来训练",{"2":{"14":1}}],["我们可以获得所有的键和值",{"2":{"106":1}}],["我们可以加载openai公开提供的权重",{"2":{"92":1}}],["我们可以影响生成文本的多样性和连贯性",{"2":{"92":1}}],["我们可以检查",{"2":{"89":1}}],["我们可以检查字符和",{"2":{"76":1}}],["我们可以从",{"2":{"88":1}}],["我们可以从数据加载器中采样数据",{"2":{"48":1}}],["我们可以同时保存模型和优化器的state",{"2":{"86":1}}],["我们可以按照图5",{"2":{"82":1}}],["我们可以用一个从概率分布中采样的函数来替换argmax",{"2":{"80":1}}],["我们可以用一个模型执行多种任务",{"2":{"14":1}}],["我们可以迭代数据加载器以确保它们正确创建",{"2":{"77":1}}],["我们可以重用第",{"2":{"77":1}}],["我们可以直接在",{"2":{"154":1}}],["我们可以直接对",{"2":{"63":1}}],["我们可以直接将这些位置嵌入添加到词元嵌入中",{"2":{"48":1}}],["我们可以收集模型参数张量中的总参数量",{"2":{"60":1}}],["我们可以这样实现",{"2":{"59":1,"63":1}}],["我们可以将这两个概念结合到一个",{"2":{"129":1}}],["我们可以将这些代码组织到一个",{"2":{"109":1}}],["我们可以将权重加载到一个新的gptmodel模型实例中",{"2":{"86":1}}],["我们可以将",{"2":{"63":1,"113":1}}],["我们可以将此函数实现为一个",{"2":{"57":1}}],["我们可以将其应用于一个词元",{"2":{"46":1}}],["我们可以将其绘制在二维散点图中以进行可视化",{"2":{"19":1}}],["我们可以在上一节的训练循环模板中添加余弦衰减功能",{"2":{"189":1}}],["我们可以在下一章将其插入",{"2":{"94":1}}],["我们可以在",{"2":{"46":1}}],["我们可以在标注数据上对该",{"2":{"10":1}}],["我们可以通过绘制学习率变化图来验证预热是否按预期工作",{"2":{"188":1}}],["我们可以通过编程方式计算参数数量",{"2":{"174":1}}],["我们可以通过张量的",{"2":{"150":1}}],["我们可以通过实现一个简单的",{"2":{"126":1}}],["我们可以通过测试输出来验证是否正确加载了模型权重",{"2":{"89":1}}],["我们可以通过先使用torch",{"2":{"86":1}}],["我们可以通过一种称为温度缩放的概念进一步控制分布和选择过程",{"2":{"80":1}}],["我们可以通过对概率分数应用",{"2":{"71":1}}],["我们可以通过以下train",{"2":{"78":1}}],["我们可以通过以下代码计算前馈模块和注意力模块中的参数数量",{"2":{"179":1}}],["我们可以通过以下代码打印目标",{"2":{"71":1}}],["我们可以通过以下代码重现图",{"2":{"54":1}}],["我们可以通过以下",{"2":{"53":1}}],["我们可以通过以下方式实例化",{"2":{"37":1}}],["我们可以通过",{"2":{"37":1,"58":1}}],["我们可以通过嵌入模型处理多种不同的数据格式",{"2":{"19":1}}],["我们可以创建词汇表的反向版本",{"2":{"27":1}}],["我们可以创建所有唯一词元的列表并按字母顺序排序",{"2":{"26":1}}],["我们可以使用以下代码计算gpt模型的训练集和验证集损失",{"2":{"180":1}}],["我们可以使用print",{"2":{"180":1}}],["我们可以使用distributedsampler来确保在使用ddp时",{"2":{"169":1}}],["我们可以使用修改后的注意力权重来计算上下文向量",{"2":{"121":1}}],["我们可以使用更新后的new",{"2":{"89":1}}],["我们可以使用现有的词汇表实例化新的词元化器对象",{"2":{"28":1}}],["我们可以使用",{"2":{"23":1,"89":1,"149":1,"151":1,"164":1}}],["我们可以使用特定的神经网络层或其他预训练的神经网络模型",{"2":{"19":1}}],["我们可以选择二维词嵌入",{"2":{"19":1}}],["我们提到过",{"2":{"14":1}}],["我们将深入探索llm的训练流程",{"2":{"196":1}}],["我们将深入探讨自注意力机制的内部工作原理",{"2":{"97":1}}],["我们将改进第5至7章中用于预训练和微调的训练函数",{"2":{"187":1}}],["我们将仅关注之前代码中需要调整的核心部分",{"2":{"170":1}}],["我们将了解分布式训练的最基本情况",{"2":{"169":1}}],["我们将了解如何将这些",{"2":{"60":1}}],["我们将简要介绍分布式训练的概念",{"2":{"168":1}}],["我们将简要介绍如何使用批量大小大于",{"2":{"45":1}}],["我们将首先介绍",{"2":{"163":1}}],["我们将首先在下一小节介绍一个简化版的自注意力",{"2":{"98":1}}],["我们将定义",{"2":{"136":1}}],["我们将先前实现的因果注意力类扩展为多头注意力",{"2":{"125":1}}],["我们将扩展此概念",{"2":{"124":1}}],["我们将扩展因果注意力机制并实现多头注意力",{"2":{"124":1}}],["我们将扩展到一个小型",{"2":{"52":1}}],["我们将因果注意力和",{"2":{"122":1}}],["我们将用代码实现该遮罩和归一化过程",{"2":{"115":1}}],["我们将用我们在本章后面实现的",{"2":{"60":1}}],["我们将其泛化以计算六词输入句",{"2":{"104":1}}],["我们将其设置为输入文本的最大长度",{"2":{"48":1}}],["我们将第二个输入",{"2":{"104":1}}],["我们将像之前一样逐步编写代码",{"2":{"104":1}}],["我们将为该注意力机制添加可训练的权重",{"2":{"104":1}}],["我们将添加可训练权重",{"2":{"103":1}}],["我们将按照图",{"2":{"100":1,"115":1}}],["我们将泛化此过程",{"2":{"99":1}}],["我们将主要孤立地研究这些注意力机制",{"2":{"94":1}}],["我们将进一步调整学习率",{"2":{"188":1}}],["我们将进一步增强自注意力机制",{"2":{"114":1}}],["我们将进一步处理此预训练模型",{"2":{"89":1}}],["我们将进一步测试包含未知词汇的文本",{"2":{"30":1}}],["我们将load",{"2":{"89":1}}],["我们将直接从本章的在线资源库中下载",{"2":{"88":1}}],["我们将重新使用这些预训练权重",{"2":{"88":1}}],["我们将结合这些概念",{"2":{"83":1}}],["我们将通过引入三个可训练的权重矩阵",{"2":{"104":1}}],["我们将通过两个小节来实现此自注意力机制",{"2":{"104":1}}],["我们将通过添加因果遮罩和多头机制进一步扩展这个自注意力机制",{"2":{"104":1}}],["我们将通过修改文本生成函数来实现这一功能",{"2":{"82":1}}],["我们将通过实际示例逐步讲解",{"2":{"71":1}}],["我们将采样限制在概率最高的前k个token内",{"2":{"82":1}}],["我们将采用一个简单的训练循环",{"2":{"78":1}}],["我们将采用预训练的",{"2":{"15":1}}],["我们将gpt模型实例",{"2":{"79":1}}],["我们将模型置于评估模式以关闭随机成分",{"2":{"79":1}}],["我们将模型设置为",{"2":{"63":1}}],["我们将介绍如何利用",{"2":{"163":1}}],["我们将介绍如何通过计算所谓的损失来数值化评估模型生成文本的表现",{"2":{"71":1}}],["我们将介绍",{"2":{"135":1}}],["我们将介绍对因果注意力机制的另一项小改动",{"2":{"121":1}}],["我们将介绍两种技术",{"2":{"79":1}}],["我们将介绍生成文本的策略",{"2":{"79":1}}],["我们将探讨llm所采用的采样方法",{"2":{"78":1}}],["我们将探讨在神经网络的不同层之间插入捷径连接的概念",{"2":{"57":1}}],["我们将训练",{"2":{"77":1}}],["我们将训练它来生成类似人类的文本",{"2":{"51":1}}],["我们将数据集分为训练集和验证集",{"2":{"76":1}}],["我们将把本附录中涉及的所有概念整合进llm训练函数中",{"2":{"190":1}}],["我们将把",{"2":{"121":1}}],["我们将把损失计算应用于整个训练和验证集",{"2":{"74":1}}],["我们将把这个",{"2":{"59":1}}],["我们将计算两个示例批次",{"2":{"72":1}}],["我们将讨论如何加载预训练权重",{"2":{"67":1}}],["我们将加载",{"2":{"60":1}}],["我们将加载预训练权重并适配更大的",{"2":{"52":1}}],["我们将对标准自注意力机制进行修改",{"2":{"115":1}}],["我们将对",{"2":{"60":1}}],["我们将这些权重加载到我们的",{"2":{"88":1}}],["我们将这些函数并排绘制出来",{"2":{"57":1}}],["我们将这些词元从",{"2":{"26":1}}],["我们将研究",{"2":{"55":1}}],["我们将准备输入数据并初始化一个新的",{"2":{"53":1}}],["我们将从因果注意力扩展到多头注意力",{"2":{"125":1}}],["我们将从计算一个上下文向量",{"2":{"104":1}}],["我们将从零开始编写这个自注意力机制代码",{"2":{"96":1}}],["我们将从openai加载一个更强大的预训练gpt模型到我们的gptmodel实例中",{"2":{"86":1}}],["我们将从",{"2":{"53":1,"76":1}}],["我们将从头开始编写一个",{"2":{"15":1}}],["我们将从头开始理解",{"2":{"9":1}}],["我们将编写围绕自注意力机制的剩余",{"2":{"94":1}}],["我们将编写代码",{"2":{"61":1,"63":1}}],["我们将编写",{"2":{"51":1,"96":1}}],["我们将词元",{"2":{"48":1}}],["我们将优化这些嵌入权重",{"2":{"46":1}}],["我们将输入文本示例重复一次",{"2":{"122":1}}],["我们将输入文本拆分为独立的词元",{"2":{"20":1}}],["我们将输入上下文编码为",{"2":{"63":1}}],["我们将输入逐层传递",{"2":{"58":1}}],["我们将输入收集到一个张量",{"2":{"42":1}}],["我们将修改",{"2":{"31":1}}],["我们将修改该词元化器",{"2":{"31":1}}],["我们将文本词元化并使用词汇表将词元转换为词元",{"2":{"27":1}}],["我们将文本转换为词元",{"2":{"20":1}}],["我们将",{"2":{"26":2,"160":1}}],["我们将利用",{"2":{"23":1}}],["我们将该文件命名为",{"2":{"20":1}}],["我们将使用distributedsampler",{"2":{"170":1}}],["我们将使用它来为每个gpu生成一个训练进程",{"2":{"170":1}}],["我们将使用它来生成多个进程",{"2":{"170":1}}],["我们将使用两个输入示例作为",{"2":{"71":1}}],["我们将使用两个独立",{"2":{"34":1}}],["我们将使用前几节创建的词元",{"2":{"46":1}}],["我们将使用上一节介绍的",{"2":{"42":1}}],["我们将使用一个名为",{"2":{"37":1}}],["我们将使用该词汇表将新文本转换为词元",{"2":{"27":1}}],["我们将使用",{"2":{"20":1,"42":1,"57":1}}],["我们将实现带有可训练权重的自注意力机制",{"2":{"98":1}}],["我们将实现四种不同的注意力机制变体",{"2":{"94":1}}],["我们将实现训练大语言模型",{"2":{"78":1}}],["我们将实现",{"2":{"59":1}}],["我们将实现层归一化",{"2":{"54":1}}],["我们将实现嵌入层",{"2":{"45":1}}],["我们将实现一个简化的自注意力机制版本",{"2":{"99":1}}],["我们将实现一个小型神经网络子模块",{"2":{"57":1}}],["我们将实现一个数据加载器",{"2":{"42":1}}],["我们将实现一种抽样和数据加载策略",{"2":{"18":1}}],["我们将实现并在小型数据集上训练",{"2":{"16":1}}],["我们将学习如何编写并预训练一个",{"2":{"15":1}}],["我们将学习如何加载开源的模型权重到我们实现的架构中",{"2":{"13":1}}],["我们将学习数据预处理的基本步骤",{"2":{"15":1}}],["我们将以",{"2":{"15":1}}],["我们将在2",{"2":{"154":1,"155":1}}],["我们将在后面计算这些权重",{"2":{"96":1}}],["我们将在后续实现模型训练时使用这个方法",{"2":{"68":1}}],["我们将在下一章中深入探讨",{"2":{"63":1}}],["我们将在下一章训练这个模型来生成类似人类的文本",{"2":{"52":1}}],["我们将在下一节中将这些",{"2":{"60":1}}],["我们将在下一节中将之前所学的概念",{"2":{"58":1}}],["我们将在本章开始实现一个",{"2":{"53":1}}],["我们将在本章的剩余部分更详细地讨论",{"2":{"11":1}}],["我们将在第",{"2":{"53":1,"104":1}}],["我们将在第七章详细讲解如何通过人类反馈微调模型以遵循指令",{"2":{"14":1}}],["我们将在第三章详细讲解和实现该机制",{"2":{"11":1}}],["我们将涵盖",{"2":{"10":1}}],["我们将逐步讨论和实现这种",{"2":{"8":1}}],["我们将逐步奠定基础",{"2":{"7":1}}],["每轮更新中都要调用",{"2":{"161":1}}],["每层的节点数量设置得较少",{"2":{"155":1}}],["每层包含一个线性层和一个",{"2":{"58":1}}],["每种情况下",{"2":{"81":1}}],["每组",{"2":{"71":1}}],["每次循环",{"2":{"175":2}}],["每次迭代中的",{"2":{"63":1}}],["每次生成一个",{"2":{"63":1}}],["每行的和为",{"2":{"119":1}}],["每行对应一个",{"2":{"59":1}}],["每行由分配给",{"2":{"43":1}}],["每条样本由",{"2":{"53":1}}],["每一列对应嵌入的一个维度",{"2":{"46":1}}],["每一行对应词汇表中的一个词元",{"2":{"46":1}}],["每个进程负责一个gpu",{"2":{"170":1}}],["每个进程都应该拥有自己的python解释器实例",{"2":{"170":1}}],["每个进程会接收到并保留模型的一个副本",{"2":{"169":1}}],["每个模型副本",{"2":{"169":1}}],["每个模型将从数据加载器中接收到一个迷你批次",{"2":{"169":1}}],["每个gpu接收的批次是不同的",{"2":{"169":1}}],["每个gpu将接收到一个模型的副本",{"2":{"169":1}}],["每个机器上可能有多个gpu",{"2":{"168":1}}],["每个训练示例访问一次",{"2":{"157":1}}],["每个样本有两个特征",{"2":{"156":1}}],["每个样本包含",{"2":{"48":1,"57":1}}],["每个节点代表对应层中的一个单元",{"2":{"155":1}}],["每个维度表示不同的特征",{"2":{"147":1}}],["每个对象表示一个独立的注意力头",{"2":{"129":1}}],["每个上下文向量矩阵的行表示标记的上下文向量",{"2":{"127":1}}],["每个自注意力机制都有自己的权重",{"2":{"126":1}}],["每个头独立操作",{"2":{"125":1}}],["每个头学习数据的不同方面",{"2":{"114":1}}],["每个标记现由一个",{"2":{"124":1}}],["每个标记为",{"2":{"122":1}}],["每个有",{"2":{"122":1}}],["每个词的预测仅依赖于前面的词",{"2":{"114":1}}],["每个词元",{"2":{"48":2}}],["每个输入元素",{"2":{"109":1}}],["每个元素表示输入对之间的注意力得分",{"2":{"101":1}}],["每个生成步骤选择的token对应于词汇表中具有最高概率得分的token",{"2":{"79":1}}],["每个示例包含三个",{"2":{"71":1}}],["每个文本包含",{"2":{"60":1}}],["每个块包含多头注意力",{"2":{"60":1}}],["每个",{"2":{"53":1,"57":1}}],["每个批次包含",{"2":{"48":1,"71":1,"77":1}}],["每个张量包含",{"2":{"44":1}}],["每个新词的选择都基于前面的序列",{"2":{"14":1}}],["定义main函数",{"2":{"170":1}}],["定义自定义",{"2":{"157":1}}],["定义如何加载每条数据记录",{"2":{"156":1}}],["定义深度学习",{"0":{"138":1}}],["定义了第二个输入元素",{"2":{"104":1}}],["定义了一个简化的",{"2":{"53":1}}],["定义了从数据集中获取单独行的方式",{"2":{"43":1}}],["定制化的",{"2":{"10":1}}],["+=",{"2":{"77":1,"78":2,"99":2,"161":2,"188":1,"189":1,"190":2}}],["+",{"2":{"43":4,"48":1,"53":1,"54":2,"57":2,"58":1,"59":2,"60":1,"80":1,"152":1,"154":1,"164":2,"174":5,"188":1,"189":3,"190":3}}],["安装命令可能会因操作系统的不同而略有差异",{"2":{"141":1}}],["安装",{"0":{"139":1},"1":{"140":1,"141":1,"142":1,"143":1,"144":1},"2":{"140":2,"143":1}}],["安装的更多信息",{"2":{"42":1}}],["安装该库",{"2":{"37":1}}],["张量驻留在设备中",{"2":{"164":1}}],["张量中一些最基本的操作",{"2":{"151":1}}],["张量中相同",{"2":{"63":1}}],["张量操作",{"0":{"151":1},"1":{"154":1,"157":1,"158":1,"160":1,"161":1}}],["张量数据类型",{"0":{"150":1},"2":{"175":1}}],["张量库的基本操作",{"2":{"148":1}}],["张量库的功能类似于数组库",{"2":{"147":1}}],["张量库",{"2":{"147":1,"152":1}}],["张量可以包含多维数据",{"2":{"147":1}}],["张量可以视为多维数组",{"2":{"42":1}}],["张量有二维",{"2":{"73":1}}],["张量有三维",{"2":{"73":1}}],["张量应用",{"2":{"63":1}}],["张量",{"2":{"54":2,"122":1,"124":1,"149":2,"151":1}}],["张量添加到每个",{"2":{"48":1}}],["张量是一种类似数组的数据结构",{"2":{"171":1}}],["张量是一种数学概念",{"2":{"147":1}}],["张量是数组结构的数据容器",{"2":{"149":1}}],["张量是数据容器",{"2":{"147":1}}],["张量是可以通过其阶数",{"2":{"147":1}}],["张量是",{"2":{"48":1}}],["张量包含相应的目标",{"2":{"43":1}}],["张量的",{"2":{"190":1}}],["张量的第一个维度为",{"2":{"128":1}}],["张量的形状",{"2":{"73":1}}],["张量的形式返回输入和目标",{"2":{"42":1}}],["张量的一定数量的词元",{"2":{"43":1}}],["箭头右边的词元",{"2":{"42":1}}],["输入特征",{"2":{"152":1}}],["输入和输出的维度通常相同",{"2":{"105":1}}],["输入和输出维度的一致性简化了架构的设计",{"2":{"57":1}}],["输入到模型中以计算两个输入示例的",{"2":{"71":1}}],["输入上下文逐步增长",{"2":{"63":1}}],["输入通过第一个线性层由",{"2":{"57":1}}],["输入文本重复",{"2":{"128":1}}],["输入文本为",{"2":{"99":1}}],["输入文本依次输入编码器",{"2":{"95":1}}],["输入文本首先被分割为单个词元",{"2":{"48":1}}],["输入文本的长度可能超过支持的上下文长度",{"2":{"48":1}}],["输入做准备",{"2":{"48":1}}],["输入大小至少为",{"2":{"44":1}}],["输入大小",{"2":{"44":1}}],["输入右移一个位置",{"2":{"42":1}}],["输出层",{"2":{"155":1,"174":1}}],["输出维度",{"2":{"127":1}}],["输出的第二个元素与我们之前计算的",{"2":{"107":1}}],["输出的线性层",{"2":{"53":1}}],["输出更高的值",{"2":{"72":1}}],["输出是一个上下文向量",{"2":{"59":1}}],["输出张量中最高分数的位置与",{"2":{"63":1}}],["输出张量中的值",{"2":{"54":1}}],["输出张量的形状为",{"2":{"60":1}}],["输出张量的形状与输入张量相同",{"2":{"57":1}}],["输出",{"2":{"54":1}}],["输出可以看到",{"2":{"32":1}}],["输出从数字转换回文本时",{"2":{"27":1}}],["输出对",{"2":{"18":1}}],["xiong等人",{"2":{"185":1}}],["x1",{"2":{"152":3,"154":2}}],["x^6",{"2":{"99":1}}],["x^5",{"2":{"99":1}}],["x^4",{"2":{"99":1}}],["x^3",{"2":{"99":1}}],["x^2",{"2":{"99":1}}],["x^1",{"2":{"99":1}}],["x^",{"2":{"99":10,"104":4}}],["xxx",{"2":{"99":1,"104":1,"110":1}}],["xticklabels",{"2":{"80":1}}],["xticks",{"2":{"80":1}}],["xl的配置",{"2":{"179":1}}],["xl",{"2":{"62":1,"89":1,"179":1,"180":1}}],["xlabel",{"2":{"57":1,"78":2,"188":1,"189":1}}],["x",{"2":{"42":6,"48":2,"52":2,"53":12,"54":7,"57":25,"58":9,"59":23,"60":8,"77":4,"80":12,"99":27,"101":4,"104":6,"106":3,"110":4,"112":4,"123":5,"127":2,"128":1,"129":5,"155":6,"156":1,"157":11,"161":1}}],["x3c",{"2":{"0":8,"32":4,"33":1,"34":2,"35":3,"37":3,"46":1,"47":1,"53":1,"54":8,"55":2,"60":1,"70":1,"77":1,"82":1,"83":1,"88":7,"110":1,"112":1,"116":1,"118":1,"119":1,"121":2,"122":1,"128":1,"155":1,"188":1,"189":1,"190":1}}],["短篇小说",{"2":{"187":1}}],["短篇小说进行词元化",{"2":{"42":1}}],["短篇小说中未出现",{"2":{"30":1}}],["短篇小说中的一段文字",{"2":{"29":1}}],["调试和概念演示中很有用",{"2":{"154":1}}],["调整到均值为",{"2":{"54":1}}],["调整词元化器",{"2":{"32":1}}],["调用train",{"2":{"180":1}}],["调用",{"2":{"40":1,"154":1,"155":1,"160":1}}],["重新归一化后的注意力权重分布实际上相当于仅在未遮罩的部分上进行",{"2":{"120":1}}],["重新归一化遮罩的注意力权重",{"0":{"119":1}}],["重新构建图",{"2":{"40":1}}],["重点是引入因果性和多头元素",{"2":{"114":1}}],["重置和计算梯度",{"2":{"78":1}}],["重要性由所谓的注意力权重决定",{"2":{"96":1}}],["重要性训练",{"2":{"65":1}}],["重要的是",{"2":{"57":1,"161":1}}],["重要的是要注意",{"2":{"46":1}}],["重复此采样1000次",{"2":{"80":1}}],["重复",{"2":{"60":1}}],["重复了六次编码器和解码器块",{"2":{"14":1}}],["函数设置max",{"2":{"190":1}}],["函数读取文件",{"2":{"162":1}}],["函数可以扩展到任意大小的数据集",{"2":{"161":1}}],["函数以提高效率和数值稳定性",{"2":{"160":1}}],["函数时",{"2":{"154":1}}],["函数创建新张量",{"2":{"151":1}}],["函数创建",{"2":{"149":1}}],["函数创建一个遮罩矩阵",{"2":{"117":1}}],["函数完成操作",{"2":{"121":1}}],["函数表现为阶跃函数",{"2":{"108":1}}],["函数计算注意力权重",{"2":{"108":1}}],["函数计算离散结果的损失",{"2":{"73":1}}],["函数来计算注意力权重",{"2":{"116":1}}],["函数来归一化这些得分",{"2":{"108":1}}],["函数来归一化",{"2":{"99":1}}],["函数来实现第",{"2":{"71":1}}],["函数到训练和验证集加载器",{"2":{"77":1}}],["函数遍历给定数据加载器中的所有批次",{"2":{"77":1}}],["函数在",{"2":{"73":1}}],["函数前的未缩放模型输出",{"2":{"73":1}}],["函数前",{"2":{"73":1,"121":2}}],["函数后得到",{"2":{"71":1}}],["函数生成文本",{"2":{"71":1}}],["函数对编码后的输入张量进行生成",{"2":{"63":1}}],["函数逐次生成一个",{"2":{"63":1}}],["函数将概率值转换为类别预测",{"2":{"161":1}}],["函数将为我们完成这些步骤",{"2":{"73":1}}],["函数将这些概率分数转换回文本",{"2":{"71":1}}],["函数将这些",{"2":{"71":1}}],["函数将",{"2":{"63":1}}],["函数中",{"2":{"63":1}}],["函数转换为概率分布",{"2":{"63":1}}],["函数并将其应用于不带捷径连接的模型",{"2":{"58":1}}],["函数实现",{"2":{"57":1}}],["函数的内部生成过程",{"2":{"71":1}}],["函数的输出可以看到",{"2":{"58":1}}],["函数的输出",{"2":{"57":1}}],["函数的特性",{"2":{"57":1}}],["函数进行比较",{"2":{"57":1}}],["函数如何配合工作",{"2":{"44":1}}],["函数",{"2":{"40":1,"55":1,"63":1,"70":1,"71":1,"72":1,"73":1,"77":2,"88":1,"154":1,"155":1,"161":2}}],["尝试使用不同的温度和top",{"2":{"84":1}}],["尝试使用不同的设置运行它",{"2":{"45":1}}],["尝试使用",{"2":{"40":1}}],["练习解答",{"0":{"176":1},"1":{"177":1,"178":1,"179":1,"180":1}}],["练习答案",{"0":{"173":1},"1":{"174":1,"175":1}}],["练习5",{"0":{"81":1,"84":1,"85":1}}],["练习",{"0":{"40":1,"45":1,"61":1,"62":1,"64":1,"87":1,"90":1,"91":1,"113":1,"132":1,"145":1,"146":1,"160":1,"167":1,"174":1,"175":1},"1":{"89":1,"90":1,"91":1,"148":1},"2":{"128":1,"177":1,"178":1,"179":1,"180":6}}],["且不进行温度缩放",{"2":{"180":1}}],["且最大类标签值不超过输出节点数量减",{"2":{"157":1}}],["且每年持续增长",{"2":{"136":1}}],["且剩余的元素进行了重新缩放",{"2":{"122":1}}],["且对角线以上的值为零",{"2":{"119":1}}],["且",{"2":{"38":1}}],["效率较高",{"2":{"37":1}}],["算法将不在预定义词汇表中的单词拆分为子词或单个字符序列",{"2":{"39":1}}],["算法",{"2":{"37":1}}],["库或包",{"2":{"139":1}}],["库的自动微分引擎",{"2":{"154":1}}],["库的延续",{"2":{"142":1}}],["库的名称是",{"2":{"142":1}}],["库的全面覆盖",{"2":{"135":1}}],["库的",{"2":{"40":1}}],["库基于",{"2":{"37":1}}],["库",{"2":{"37":1,"141":1}}],["取而代之的是使用字节对编码",{"2":{"36":1}}],["填充",{"2":{"36":1}}],["序列中的每个元素",{"2":{"59":1}}],["序列结束",{"2":{"36":1}}],["序列开始",{"2":{"36":1}}],["某些组件在训练和推理时行为不同",{"2":{"160":1}}],["某些输入标记比其他标记更为重要",{"2":{"96":1}}],["某些层的神经元",{"2":{"86":1}}],["某些",{"2":{"36":1}}],["除了使用ddp带来的少量设备间的通信开销外",{"2":{"169":1}}],["除了需要加载和预处理数据外",{"2":{"158":1}}],["除了因果注意力掩码用于将未来标记的权重归零外",{"2":{"133":1}}],["除了将",{"2":{"129":1}}],["除了传统的",{"2":{"57":1}}],["除了嵌入层之外",{"2":{"52":1}}],["除了图",{"2":{"46":1}}],["除了",{"2":{"36":1}}],["除文本补全外",{"2":{"11":1}}],["列出不同",{"2":{"89":1}}],["列出了",{"2":{"12":1}}],["列数",{"2":{"54":1}}],["列",{"2":{"46":1}}],["列表示嵌入维度",{"2":{"127":1}}],["列表包含用于",{"2":{"34":1}}],["列表",{"2":{"34":1,"54":1,"57":2,"58":1,"59":1,"60":1,"63":1,"70":1}}],["对清晰阐释复杂概念充满热情",{"2":{"196":1}}],["对比train",{"2":{"190":1}}],["对bahdanau注意力机制及语言翻译中的应用感兴趣的读者",{"2":{"184":1}}],["对从零开始编写和训练bpe分词器感兴趣的读者",{"2":{"183":1}}],["对嵌入空间",{"2":{"183":1}}],["对许多从业者和研究人员来说",{"2":{"136":1}}],["对象列表",{"2":{"129":1}}],["对象实现的",{"2":{"129":1}}],["对象中",{"2":{"61":1}}],["对象中的",{"2":{"61":1}}],["对角线上方的元素已成功置零",{"2":{"118":1}}],["对概率分数应用对数",{"2":{"72":1}}],["对",{"2":{"60":1}}],["对负值允许一个小的非零输出",{"2":{"57":1}}],["对负值的输入直接输出零",{"2":{"57":1}}],["对应阶数0",{"2":{"147":1}}],["对应词",{"2":{"99":1}}],["对应词汇表的大小",{"2":{"71":1}}],["对应一个",{"2":{"99":1}}],["对应位置的",{"2":{"71":1}}],["对应每个输入的",{"2":{"71":1}}],["对应于输入示例的数量",{"2":{"71":1}}],["对应于分词器的词汇大小",{"2":{"60":1}}],["对应图",{"2":{"71":1,"72":1}}],["对应模型的词汇大小",{"2":{"60":1}}],["对应两条文本样本",{"2":{"53":1}}],["对应的token被选择的概率约为60",{"2":{"80":1}}],["对应的概率分数",{"2":{"73":1}}],["对应的概率",{"2":{"72":1}}],["对应的",{"2":{"63":1,"72":1}}],["对应的向量",{"2":{"49":1,"63":1}}],["对应的嵌入向量",{"2":{"47":1}}],["对于一个由分量v",{"2":{"190":1}}],["对于那些对1",{"2":{"182":1}}],["对于需要较高的准确性应用场景",{"2":{"180":1}}],["对于需要微积分复习或入门的读者",{"2":{"172":1}}],["对于更高级的用例",{"2":{"172":1}}],["对于更高维度的张量没有特定的术语",{"2":{"149":1}}],["对于张量概念的更深入介绍",{"2":{"172":1}}],["对于希望在自己多gpu机器或云实例上运行代码的读者",{"2":{"170":1}}],["对于本书的学习内容来说",{"2":{"169":1}}],["对于本书来说",{"2":{"141":1}}],["对于大数据集",{"2":{"161":1}}],["对于内存中的张量数据集",{"2":{"157":1}}],["对于第二行的",{"2":{"115":1}}],["对于每个输入元素",{"2":{"99":1}}],["对于生成特定的输出标记",{"2":{"96":1}}],["对于翻译短句效果良好",{"2":{"96":1}}],["对于不熟悉",{"2":{"95":1}}],["对于温度设置1",{"2":{"80":1}}],["对于感兴趣的读者",{"2":{"75":1}}],["对于两个输入文本",{"2":{"71":1}}],["对于参数量为",{"2":{"60":1}}],["对于正值直接输出输入值",{"2":{"57":1}}],["对于嵌入维度",{"2":{"55":1}}],["对于二维张量而言",{"2":{"54":1}}],["对于二维张量",{"2":{"54":1}}],["对于学习实现",{"2":{"53":1}}],["对于",{"2":{"48":1,"60":1}}],["对于输入序列中的每个位置",{"2":{"48":1}}],["对于熟悉独热编码",{"2":{"47":1}}],["对这些词元",{"2":{"40":1}}],["对这些整数列表中的每个元素调用",{"2":{"40":1}}],["对词元化文本进行去词元化检查",{"0":{"35":1}}],["对样本文本进行词元化",{"2":{"34":1}}],["对支撑",{"2":{"7":1}}],["相乘",{"2":{"152":1}}],["相同大小的注意力模块",{"0":{"132":1}}],["相同",{"2":{"129":1,"132":1}}],["相反",{"2":{"80":2,"84":1}}],["相差很大",{"2":{"71":1}}],["相对位置嵌入则着重于词元之间的相对位置或距离",{"2":{"48":1}}],["相对位置嵌入和绝对位置嵌入",{"2":{"48":1}}],["相对于第一批次右移了一位",{"2":{"44":1}}],["相对于使用",{"2":{"19":1}}],["相当于",{"2":{"36":1}}],["相比于单gpu",{"2":{"169":1}}],["相比上一章唯一的调整是将上下文长度",{"2":{"70":1}}],["相比之下",{"2":{"57":1,"155":1,"185":1}}],["相比",{"2":{"33":1}}],["快速了解最相关的概念",{"2":{"148":1}}],["快速检查词汇表的最后",{"2":{"32":1}}],["快速查看前",{"2":{"25":1}}],["新电子邮件",{"2":{"138":1}}],["新闻中通常将大语言模型",{"2":{"138":1}}],["新增批次维度",{"2":{"123":1}}],["新的",{"2":{"33":1}}],["新词汇表大小为",{"2":{"32":1}}],["新文本生成",{"2":{"9":1}}],["便于在预训练阶段使用",{"2":{"69":1}}],["便于在",{"2":{"54":1}}],["便于",{"2":{"31":1}}],["识别模式和进行决策",{"2":{"138":1}}],["识别模式和做出决策",{"2":{"8":1}}],["识别文本源彼此独立",{"2":{"31":1}}],["添加批次维度",{"2":{"70":1}}],["添加到下次迭代的输入序列中",{"2":{"63":1}}],["添加到前面的输入中",{"2":{"63":1}}],["添加到所有唯一词元的列表中",{"2":{"32":1}}],["添加捷径连接",{"0":{"58":1}}],["添加",{"2":{"31":2}}],["添加特殊上下文词元",{"0":{"31":1},"1":{"32":1,"33":1,"34":1,"35":1,"36":1}}],["处理长序列的问题",{"0":{"95":1}}],["处理批次",{"2":{"78":1}}],["处理之前会经历词元化步骤",{"2":{"42":1}}],["处理未知单词的简单文本词元化器",{"0":{"33":1}}],["处理训练集中未包含的词",{"0":{"30":1}}],["处理文本数据",{"0":{"17":1},"1":{"18":1,"19":1,"20":1,"21":1,"22":1,"23":1,"24":1,"25":1,"26":1,"27":1,"28":1,"29":1,"30":1,"31":1,"32":1,"33":1,"34":1,"35":1,"36":1,"37":1,"38":1,"39":1,"40":1,"41":1,"42":1,"43":1,"44":1,"45":1,"46":1,"47":1,"48":1,"49":1},"2":{"11":1}}],["实验性支持",{"2":{"141":1}}],["实际概率为4",{"2":{"180":1}}],["实际上",{"2":{"152":1}}],["实际实现多头注意力时",{"2":{"126":1}}],["实际中",{"2":{"77":1}}],["实际使用了",{"2":{"71":1}}],["实际应用中",{"2":{"48":1}}],["实例的权重正确赋值给",{"2":{"113":1}}],["实例中",{"2":{"89":1}}],["实例",{"2":{"70":1,"113":1}}],["实例并传入分词后的数据",{"2":{"53":1}}],["实例化数据加载器",{"2":{"157":1}}],["实例化嵌入层后",{"2":{"46":1}}],["实例化并使用词元化器",{"0":{"29":1}}],["实践中试用新词元化器",{"0":{"34":1}}],["实现学习率预热可以稳定训练复杂模型",{"2":{"188":1}}],["实现的",{"2":{"129":1}}],["实现带有权重分割的多头注意力机制",{"0":{"129":1},"1":{"130":1,"131":1,"132":1}}],["实现带可训练权重的自注意力机制",{"0":{"104":1},"1":{"105":1,"106":1,"107":1,"108":1,"109":1,"110":1,"111":1,"112":1,"113":1,"114":1}}],["实现多头注意力的包装类",{"0":{"127":1}}],["实现多层神经网络",{"0":{"155":1},"2":{"57":1}}],["实现紧凑的因果注意力类",{"2":{"122":1}}],["实现紧凑的自注意力",{"2":{"109":1}}],["实现该方法的代码如下",{"2":{"121":1}}],["实现一个多头注意力模块",{"2":{"124":1}}],["实现一个因果注意力模块",{"2":{"94":1}}],["实现一个高效的数据加载器",{"2":{"42":1}}],["实现训练函数并对",{"2":{"67":1}}],["实现语言模型的简单生成循环",{"2":{"63":1}}],["实现输出形状为",{"2":{"63":1}}],["实现中使用的最终模块",{"2":{"124":1}}],["实现中使用了独立的层",{"2":{"60":1}}],["实现中",{"2":{"60":1,"76":1}}],["实现",{"0":{"111":1},"2":{"51":1,"58":1,"99":1,"109":1,"121":2,"143":1}}],["实现了",{"2":{"37":1,"156":1}}],["实现还使用其他特殊词元",{"2":{"36":1}}],["实现简单的文本词元化器",{"0":{"28":1}}],["实现本书的主要目标",{"2":{"7":1}}],["随着模型接触更多数据",{"2":{"110":1}}],["随着我们在本节中实现的",{"2":{"59":1}}],["随着我们从最后一层",{"2":{"58":1}}],["随着我们不断创新并探索这些模型的新用法",{"2":{"9":1}}],["随机屏蔽部分注意力权重以减少过拟合",{"2":{"94":1}}],["随后利用这些数据集创建数据加载器",{"2":{"156":1}}],["随后在",{"2":{"104":1}}],["随后",{"2":{"104":1}}],["随后将计算训练和验证集的交叉熵损失",{"2":{"75":1}}],["随后用于编码和解码文本",{"2":{"28":1}}],["jsexport",{"2":{"193":1}}],["json",{"2":{"88":2}}],["jiaweizzhao",{"2":{"186":1}}],["jmlr",{"2":{"184":1}}],["jupyter",{"2":{"143":1,"158":2}}],["j",{"2":{"101":4,"129":1,"170":2}}],["journey",{"2":{"99":3,"104":1,"115":2}}],["jointly",{"2":{"184":1}}],["join",{"2":{"28":1,"33":1,"34":1}}],["jack",{"2":{"21":1,"25":1}}],["执行模型的前向传播",{"2":{"155":1}}],["执行反向整数到字符串的映射",{"2":{"27":1}}],["执行查询任务",{"2":{"9":1}}],["让我们看看这在实践中如何实现",{"2":{"170":1}}],["让我们简要介绍",{"2":{"157":1}}],["让我们简要回顾一下像",{"2":{"63":1}}],["让我们检查模型的可训练参数总数",{"2":{"155":1}}],["让我们通过一个具体示例来说明计算图的概念",{"2":{"152":1}}],["让我们绘制一个简单的图表",{"2":{"78":1}}],["让我们回顾",{"2":{"73":1}}],["让我们计算",{"2":{"61":1}}],["让我们根据权重共享从总的",{"2":{"60":1}}],["让我们来看一下我们先前在模型中初始化的",{"2":{"60":1}}],["让我们先看看在",{"2":{"95":1}}],["让我们先看一下它的整体结构",{"2":{"60":1}}],["让我们先对模型架构本身进行进一步的分析",{"2":{"60":1}}],["让我们先检查均值和方差",{"2":{"54":1}}],["让我们实例化一个",{"2":{"59":1}}],["让我们对之前定义的四个输入",{"2":{"47":1}}],["让我们重复前面的代码",{"2":{"42":1}}],["让我们将这些知识结合起来",{"2":{"159":1}}],["让我们将这两个特殊词元",{"2":{"32":1}}],["让我们将其应用于训练集中未包含的新文本",{"2":{"30":1}}],["让我们将其应用于",{"2":{"25":1}}],["让我们使用adamw优化器和之前定义的train",{"2":{"78":1}}],["让我们使用此代码初始化一个不带捷径连接的神经网络",{"2":{"58":1}}],["让我们使用",{"2":{"29":1,"34":1,"60":1}}],["让我们从",{"2":{"29":1}}],["让我们在实践中尝试使用",{"2":{"63":1}}],["让我们在代码中实现图",{"2":{"60":1}}],["让我们在",{"2":{"27":1}}],["映射到输出维度",{"2":{"110":1}}],["映射到查询",{"2":{"104":1}}],["映射到连续的向量空间中",{"2":{"19":1}}],["映射回相应的文本词元",{"2":{"27":1}}],["字节",{"2":{"61":1}}],["字节对编码未知单词",{"0":{"40":1}}],["字节对编码",{"0":{"37":1},"1":{"38":1,"39":1,"40":1,"41":1,"42":1,"43":1,"44":1,"45":1},"2":{"37":1}}],["字典对象",{"2":{"162":2}}],["字典传递到我们的",{"2":{"89":1}}],["字典传入",{"2":{"53":1}}],["字典包含实际的权重张量",{"2":{"89":1}}],["字典包含与唯一整数标签关联的独立词元",{"2":{"27":1}}],["字典存储了",{"2":{"89":1}}],["字典初始化一个",{"2":{"60":1}}],["字典配置",{"2":{"57":1}}],["字典",{"2":{"53":1,"59":1,"70":1,"89":1}}],["字典中定义了一个全局的",{"2":{"64":1}}],["字典中的",{"2":{"60":1}}],["字典中",{"2":{"53":1,"70":1}}],["字典来配置小型",{"2":{"53":1}}],["字符串转换为整数表示",{"2":{"26":1}}],["字符的短篇小说分解为单词和特殊字符",{"2":{"21":1}}],["02424",{"2":{"186":1}}],["02155",{"2":{"182":1}}],["0231",{"2":{"54":1}}],["0或1",{"2":{"152":1}}],["0d",{"2":{"147":1}}],["07909",{"2":{"183":1}}],["0795",{"2":{"107":1}}],["0772",{"2":{"130":1}}],["0786",{"2":{"121":1}}],["0754",{"2":{"112":1}}],["0763",{"2":{"112":1}}],["0760",{"2":{"112":1}}],["0702",{"2":{"112":1}}],["0703",{"2":{"112":1}}],["07467",{"2":{"185":1}}],["0749",{"2":{"112":1}}],["0748",{"2":{"112":1}}],["0713",{"2":{"112":1}}],["0711",{"2":{"60":1}}],["0739",{"2":{"112":1}}],["092881",{"2":{"182":1}}],["09617",{"2":{"182":1}}],["0944",{"2":{"161":1}}],["0981",{"2":{"128":2}}],["0988",{"2":{"102":1}}],["09",{"2":{"88":1,"161":1}}],["0906",{"2":{"108":1}}],["0900",{"2":{"60":1}}],["0908",{"2":{"53":1}}],["06450",{"2":{"185":1}}],["064",{"2":{"179":1}}],["0620",{"2":{"130":2}}],["0632",{"2":{"128":2}}],["0693",{"2":{"112":1}}],["0679",{"2":{"112":1}}],["0685",{"2":{"112":1}}],["0605",{"2":{"101":4}}],["0615",{"2":{"82":1}}],["0610",{"2":{"53":1}}],["06d",{"2":{"78":1,"190":1}}],["06",{"2":{"71":1}}],["05",{"2":{"71":5,"99":1,"190":1}}],["0558",{"2":{"60":1}}],["0532",{"2":{"60":1}}],["0373",{"2":{"190":1}}],["03762",{"2":{"182":1,"184":1}}],["03507",{"2":{"186":1}}],["0307",{"2":{"161":1}}],["03",{"2":{"159":1,"165":1,"186":1}}],["03d",{"2":{"159":4,"165":4,"170":3}}],["0390",{"2":{"130":1}}],["0398",{"2":{"54":1}}],["0388",{"2":{"60":1}}],["0318",{"2":{"60":1}}],["0335",{"2":{"60":1}}],["0332",{"2":{"60":1}}],["08415",{"2":{"185":1}}],["0843",{"2":{"128":2}}],["08691",{"2":{"184":1}}],["0865",{"2":{"99":1,"101":4}}],["0817",{"2":{"154":2}}],["0898",{"2":{"154":2}}],["0882",{"2":{"121":1}}],["0874",{"2":{"121":1}}],["08763",{"2":{"186":1}}],["0876",{"2":{"54":2}}],["08",{"2":{"54":3}}],["00838",{"2":{"186":1}}],["002",{"2":{"159":8,"165":8}}],["003",{"2":{"159":8,"165":8}}],["00159",{"2":{"186":1}}],["0018",{"2":{"161":1}}],["001",{"2":{"159":5,"165":5}}],["001398873864673078",{"2":{"58":1}}],["00",{"2":{"88":26,"159":1,"165":1}}],["009",{"2":{"60":1}}],["00752",{"2":{"182":1}}],["0078",{"2":{"121":1}}],["0076",{"2":{"60":1}}],["0077",{"2":{"53":1}}],["0051",{"2":{"161":1}}],["0058",{"2":{"128":2}}],["005049646366387606",{"2":{"58":1}}],["0057",{"2":{"53":1}}],["000130",{"2":{"190":1}}],["0001提高到0",{"2":{"188":1}}],["0001",{"2":{"188":2}}],["0001201116101583466",{"2":{"58":1}}],["000本公共领域书籍以供llm训练的说明",{"2":{"186":1}}],["000本公共领域书籍训练模型",{"2":{"78":1}}],["000个标记时",{"2":{"185":1}}],["00027",{"2":{"182":1,"186":1}}],["00020173587836325169",{"2":{"58":1}}],["0009",{"2":{"161":1}}],["0004",{"2":{"78":1}}],["000",{"2":{"75":1,"76":1,"80":1,"159":3,"165":3,"179":1}}],["0007152041653171182",{"2":{"58":1}}],["000085",{"2":{"78":1}}],["000080",{"2":{"78":1}}],["00001",{"2":{"88":1}}],["000015",{"2":{"78":1,"190":1}}],["000010",{"2":{"78":1,"190":1}}],["00000",{"2":{"88":1}}],["000005",{"2":{"78":1,"190":1}}],["000000",{"2":{"78":1,"190":1}}],["0000000298",{"2":{"54":1}}],["00002",{"2":{"71":1}}],["0000",{"2":{"54":7,"55":4,"82":6,"99":1,"102":6,"118":15,"119":16,"121":16,"122":25}}],["04745",{"2":{"185":1}}],["0473",{"2":{"184":1}}],["0477",{"2":{"60":1}}],["0491",{"2":{"161":1}}],["0402",{"2":{"130":1}}],["0400",{"2":{"53":1}}],["04m",{"2":{"88":2}}],["04833",{"2":{"186":1}}],["04805",{"2":{"182":1}}],["048",{"2":{"78":1}}],["044715",{"2":{"57":1}}],["0435",{"2":{"53":1}}],["0166",{"2":{"190":1}}],["0162",{"2":{"53":1}}],["01373",{"2":{"186":1}}],["0139",{"2":{"53":1}}],["01906",{"2":{"184":1}}],["0198",{"2":{"121":1}}],["0177",{"2":{"121":1}}],["0186",{"2":{"121":1}}],["0189",{"2":{"54":1}}],["01",{"2":{"88":2,"187":1,"188":4}}],["01116",{"2":{"186":1}}],["0111",{"2":{"53":1}}],["0",{"2":{"27":1,"37":1,"43":1,"46":13,"47":7,"48":1,"53":36,"54":37,"55":3,"57":3,"58":26,"60":44,"63":3,"70":3,"71":9,"72":3,"73":1,"77":4,"78":6,"80":8,"82":10,"83":2,"88":5,"89":5,"99":37,"101":56,"102":36,"103":21,"106":1,"107":1,"108":7,"109":2,"110":15,"112":13,"116":37,"117":16,"118":36,"119":35,"121":78,"122":35,"123":1,"124":2,"128":50,"129":2,"130":30,"131":6,"141":4,"149":1,"150":3,"152":3,"154":7,"155":11,"156":7,"157":13,"158":1,"159":7,"161":21,"164":3,"165":6,"170":1,"178":4,"187":2,"188":3,"189":3,"190":11}}],[">",{"2":{"27":1,"42":10,"83":1,"190":2}}],["转置和重塑的结果不同",{"2":{"151":1}}],["转置张量",{"2":{"151":1}}],["转为",{"2":{"72":1}}],["转换的实用函数",{"2":{"70":1}}],["转换成人类可读的文本",{"2":{"63":1}}],["转换为概率分布",{"2":{"63":1}}],["转换为",{"2":{"60":1}}],["转换为连续的向量空间",{"2":{"49":1}}],["转换为连续向量表示",{"2":{"45":1}}],["转换为相同的向量表示",{"2":{"48":1}}],["转换为了连续向量表示",{"2":{"48":1}}],["转换为三维嵌入向量",{"2":{"47":1}}],["转换为嵌入向量",{"2":{"46":3,"48":1}}],["转换为嵌入向量之前的中间步骤",{"2":{"26":1}}],["转换为文本",{"2":{"42":1}}],["转换回原始文本",{"2":{"29":1}}],["转换回文本",{"2":{"27":1,"29":1,"37":1,"63":1,"71":1}}],["转换回对应的文本词元",{"2":{"27":1}}],["转化为嵌入",{"2":{"19":1}}],["各个单词和特殊字符已整齐分开",{"2":{"25":1}}],["个节点",{"2":{"157":1}}],["个注意力头的多头注意力模块",{"2":{"132":1}}],["个注意力头和",{"2":{"127":1}}],["个标记",{"2":{"110":1,"122":1,"128":1}}],["个上下文向量",{"2":{"100":1}}],["个元素的输入序列",{"2":{"99":1}}],["个元素的上下文向量",{"2":{"99":1}}],["个文件将被下载",{"2":{"88":1}}],["个文本样本组成",{"2":{"48":1}}],["个验证批次",{"2":{"77":1}}],["个训练集批次",{"2":{"77":1}}],["个目标",{"2":{"71":1}}],["个预测的",{"2":{"71":1}}],["个词或",{"2":{"74":1}}],["个词汇表单词",{"2":{"71":1}}],["个词元以供参考",{"2":{"26":1}}],["个词元",{"2":{"25":1,"42":1,"44":1,"48":2}}],["个多头注意力头",{"2":{"62":3}}],["个梯度值",{"2":{"58":1}}],["个值压缩回",{"2":{"57":1}}],["个样本和",{"2":{"77":1}}],["个样本",{"2":{"57":1}}],["个输入标记从",{"2":{"106":1}}],["个输入元素作为查询的注意力权重",{"2":{"100":1}}],["个输入元素计算了注意力权重和上下文向量",{"2":{"100":1}}],["个输入作为查询",{"2":{"99":1}}],["个输入批次",{"2":{"71":1}}],["个输入文本",{"2":{"60":1}}],["个输入值的示例",{"2":{"58":1}}],["个输入和",{"2":{"54":1}}],["个输出值",{"2":{"58":1}}],["个输出",{"2":{"54":1}}],["个层输出",{"2":{"54":1}}],["个",{"2":{"53":1,"57":1,"60":1,"62":3,"70":2,"71":2,"76":1,"77":1}}],["个批次",{"2":{"48":2}}],["个单词",{"2":{"46":1}}],["个单词的小型词汇表",{"2":{"46":1}}],["个位置",{"2":{"44":1}}],["个条目",{"2":{"32":1}}],["个独立词元",{"2":{"24":1}}],["个以上点赞的外链页面内容",{"2":{"13":1}}],["|██████████████████████|",{"2":{"88":1}}],["|████████████████████████████|",{"2":{"88":1}}],["|███████████████████████████|",{"2":{"88":1}}],["|██████████████████████████|",{"2":{"88":1}}],["|█████████████████████████|",{"2":{"88":1}}],["|█████████████████████|",{"2":{"88":1}}],["|███████|",{"2":{"88":1}}],["|endoftext|>",{"2":{"32":2,"34":2,"35":1,"37":3,"70":1}}],["|endoftext|",{"2":{"31":4,"32":1,"34":1,"36":3,"38":2,"49":1}}],["|unk|>",{"2":{"32":2,"33":1,"35":2}}],["|unk|",{"2":{"31":3,"32":1,"33":1,"36":2,"38":1,"39":1,"49":1}}],["|",{"2":{"23":1,"24":2,"25":2,"28":2,"33":2,"159":14,"165":14,"170":2}}],["逗号和句号",{"2":{"23":1}}],["区分专有名词和普通名词",{"2":{"23":1}}],["命令并通过以下语法按空白字符拆分文本",{"2":{"23":1}}],["足以说明文本处理的主要思想",{"2":{"22":1}}],["代表当前关注的项目",{"2":{"109":1}}],["代表",{"2":{"42":1}}],["代表相似概念的词通常会在嵌入空间中靠近",{"2":{"19":1}}],["代码更改仅需在训练函数中用galore",{"2":{"186":1}}],["代码生成",{"2":{"180":1}}],["代码的",{"2":{"144":1}}],["代码执行的能力",{"2":{"144":1}}],["代码合并",{"2":{"129":1}}],["代码解释",{"0":{"124":1}}],["代码清单a",{"2":{"170":2}}],["代码清单",{"0":{"110":1,"112":1,"123":1,"127":1},"2":{"149":1,"152":1,"154":1,"155":1,"156":1,"157":3,"159":2,"161":1,"165":1}}],["代码清单5",{"2":{"78":1,"83":1,"89":1}}],["代码如下",{"2":{"99":1,"189":1}}],["代码中还使用了",{"2":{"160":1}}],["代码中",{"2":{"78":1,"110":1}}],["代码来创建数据加载器",{"2":{"77":1}}],["代码实现如下",{"2":{"70":1}}],["代码",{"2":{"24":1,"88":1,"94":1}}],["代码示例",{"0":{"21":1,"27":1,"28":1,"33":1,"43":1,"44":1}}],["判决",{"2":{"20":1}}],["时尤为重要",{"2":{"124":1}}],["时减少过拟合",{"2":{"121":1}}],["时的关键步骤",{"2":{"115":1}}],["时的整体架构中的位置",{"2":{"104":1}}],["时的不确定性",{"2":{"74":1}}],["时应在张量的哪个维度上执行操作",{"2":{"54":1}}],["时需要实验的折中参数和超参数",{"2":{"45":1}}],["时需要使用大型且多样化的训练集以扩展词汇表",{"2":{"30":1}}],["时",{"2":{"19":1,"31":1,"44":1,"54":1,"58":1,"95":1,"104":1,"140":1,"153":1,"158":1}}],["高亮显示的行所示",{"2":{"100":1}}],["高质量",{"2":{"70":1}}],["高质量文本",{"2":{"12":1}}],["高斯误差线性单元",{"2":{"57":1}}],["高维嵌入在可视化方面存在挑战",{"2":{"19":1}}],["训练大约需要5分钟",{"2":{"190":1}}],["训练大语言模型",{"0":{"78":1}}],["训练15个epoch",{"2":{"188":1}}],["训练初期使用较小的权重更新",{"2":{"188":1}}],["训练神经网络",{"2":{"161":1}}],["训练神经网络时",{"2":{"153":1}}],["训练完成后",{"2":{"161":1}}],["训练模型时将使用这些加载器进行数据迭代",{"2":{"156":1}}],["训练好的模型可以用于新观测",{"2":{"138":1}}],["训练好的模型可用于预测新观测的标签",{"2":{"138":1}}],["训练数据集包括电子邮件及其由人类标识的",{"2":{"138":1}}],["训练了一个小型",{"2":{"88":1}}],["训练集和验证集的损失可以用来评估llm生成文本的质量",{"2":{"92":1}}],["训练集和验证集的损失均迅速下降",{"2":{"78":1}}],["训练集损失在第二轮之后继续下降",{"2":{"78":1}}],["训练损失大幅下降",{"2":{"78":1}}],["训练此类",{"2":{"76":1}}],["训练将是后续章节的重点",{"2":{"65":1}}],["训练或推理过程中的批量大小可能会受到硬件或具体用例的限制",{"2":{"56":1}}],["训练拥有多层的深度神经网络有时会非常具有挑战性",{"2":{"54":1}}],["训练准备输入文本的最后一步是将词元",{"2":{"46":1}}],["训练的批次",{"2":{"76":1}}],["训练的目标是将这些值尽量提高",{"2":{"71":1}}],["训练的输入",{"2":{"42":1,"49":1}}],["训练的一部分的优势在于嵌入会根据特定任务和数据进行优化",{"2":{"19":1}}],["训练中",{"2":{"22":1}}],["训练任务",{"2":{"20":1}}],["训练",{"0":{"165":1},"1":{"166":1,"167":1},"2":{"20":1,"77":1}}],["训练过程可能会非常耗时",{"2":{"168":1}}],["训练过程中打印的输出如下",{"2":{"78":1}}],["训练过程中进行优化",{"2":{"46":1}}],["训练过程",{"2":{"8":1}}],["词汇量大小为",{"2":{"53":1}}],["词汇量大小",{"2":{"53":1}}],["词汇表大小",{"2":{"187":1}}],["词汇表中每个",{"2":{"71":1}}],["词汇表流程以适应页面展示",{"2":{"71":1}}],["词汇表的总大小为",{"2":{"38":1}}],["词汇表和词元化器",{"2":{"31":1}}],["词汇表",{"2":{"26":1}}],["词嵌入的维度可以从一维到上千维不等",{"2":{"19":1}}],["词元被分配了相对较大的",{"2":{"38":1}}],["词元处理词汇表外的词",{"2":{"36":1}}],["词元来实现简化",{"2":{"36":1}}],["词元填充较短的文本",{"2":{"36":1}}],["词元充当标记",{"2":{"31":1}}],["词元化后的训练集总词元数为",{"2":{"42":1}}],["词元化器能够高效地处理未知单词",{"2":{"49":1}}],["词元化器能够正确编码和解码未知单词",{"2":{"38":1}}],["词元化器创建的",{"2":{"48":1}}],["词元化器中的",{"2":{"46":1}}],["词元化器的",{"2":{"42":1}}],["词元化器对整个",{"2":{"42":1}}],["词元化器对未知单词",{"2":{"40":1}}],["词元化器可以解析任何单词而不需要特殊的",{"2":{"39":1}}],["词元化器将未知单词拆分为子词和字符",{"2":{"39":1}}],["词元化器",{"2":{"36":1,"37":1}}],["词元化器实现通常包含两个常用方法",{"2":{"28":1}}],["词元化",{"2":{"12":1}}],["词元",{"2":{"12":1,"31":1,"33":1,"34":1,"38":1,"39":1,"47":1,"48":3}}],["词元数量",{"2":{"12":1}}],["句子或段落嵌入常用于检索增强生成",{"2":{"19":1}}],["段落甚至整个文档的嵌入",{"2":{"19":1}}],["嵌入表示",{"2":{"124":1}}],["嵌入空间",{"2":{"106":1}}],["嵌入和输出层会带来更好的训练和模型性能",{"2":{"60":1}}],["嵌入的",{"2":{"53":1}}],["嵌入的核心在于将离散对象",{"2":{"19":1}}],["嵌入维度增加时",{"2":{"108":1}}],["嵌入维度",{"2":{"53":1,"187":1}}],["嵌入将离散数据",{"2":{"49":1}}],["嵌入到",{"2":{"48":1}}],["嵌入层",{"2":{"64":1}}],["嵌入层和线性输出层的形状",{"2":{"60":1}}],["嵌入层的权重",{"2":{"60":1}}],["嵌入层的权重矩阵包含小的随机值",{"2":{"46":1}}],["嵌入层将词元",{"2":{"48":1}}],["嵌入层执行查找操作",{"2":{"47":1}}],["嵌入层与矩阵乘法的关系",{"0":{"47":1}}],["嵌入层本质上是一个通过词元",{"2":{"46":1}}],["嵌入所需的预处理步骤",{"2":{"20":1}}],["嵌入所需的步骤",{"2":{"19":1}}],["嵌入大小不同",{"2":{"89":1}}],["嵌入大小",{"2":{"19":1}}],["嵌入",{"2":{"19":1,"60":1}}],["了解更多信息",{"2":{"183":1}}],["了解了如何将张量移动到",{"2":{"165":1}}],["了解",{"2":{"164":1}}],["了解详情",{"2":{"75":1}}],["了解张量",{"2":{"19":1}}],["了解到这些模型是在大量文本上预训练的",{"2":{"18":1}}],["无论如何",{"2":{"140":1}}],["无论是在输入向量的第一个位置还是第三个位置",{"2":{"48":1}}],["无论是传统机器学习还是深度学习",{"2":{"8":1}}],["无法直接访问编码器的早期隐藏状态",{"2":{"95":1}}],["无法直接处理原始文本",{"2":{"19":1}}],["无法识别序列中词元的位置信息",{"2":{"48":1}}],["无需修改类实现",{"2":{"128":1}}],["无需进一步归一化",{"2":{"121":1}}],["无需深入了解这种架构",{"2":{"95":1}}],["无需使用",{"2":{"38":1}}],["无需记住正则表达式语法",{"2":{"23":1}}],["深度",{"2":{"138":1}}],["深度神经网络的训练机制",{"2":{"135":1}}],["深度神经网络模型",{"2":{"19":1}}],["深度学习特别适合处理图像",{"2":{"138":1}}],["深度学习中的",{"2":{"138":1}}],["深度学习的新手提供入门知识",{"2":{"135":1}}],["深度学习库概述",{"2":{"135":1}}],["深度学习模型无法直接处理视频",{"2":{"19":1}}],["深度学习是机器学习的一个子类别",{"2":{"138":2}}],["深度学习是机器学习的一个子集",{"2":{"8":1}}],["深度学习是机器学习的一个分支",{"2":{"8":1}}],["深度学习是机器学习和人工智能的一个分支",{"2":{"7":1}}],["您的参与对提升学习体验至关重要",{"2":{"196":1}}],["您需要具备python编程的基础",{"2":{"196":1}}],["您能想到哪些应用适合较高的温度和top",{"2":{"84":1}}],["您能想到哪些应用适合较低的温度和top",{"2":{"84":1}}],["您可能知道较小的批量大小在训练过程中需要较少的内存",{"2":{"45":1}}],["您可以将以下代码",{"2":{"166":1}}],["您可以将其复制并粘贴到文本文件中",{"2":{"20":1}}],["您可以在",{"2":{"47":1}}],["您可以使用以下代码检查已安装的版本",{"2":{"37":1}}],["您还将了解字节对编码等高级词元化方法",{"2":{"18":1}}],["您将学习如何准备用于训练",{"2":{"18":1}}],["然后我们将输入数据分成不同的迷你批次",{"2":{"169":1}}],["然后展示如何在单个",{"2":{"163":1}}],["然后根据生成的查询",{"2":{"110":1}}],["然后解码器使用当前隐藏状态逐词生成翻译",{"2":{"95":1}}],["然后在内部将该层分割成单独的注意力头",{"2":{"129":1}}],["然后在下一章中编写",{"2":{"96":1}}],["然后在本章最后一节中加载openai的预训练模型权重",{"2":{"86":1}}],["然后在标记数据集上进行微调",{"2":{"51":1}}],["然后可以将该函数应用于训练集",{"2":{"161":1}}],["然后可以运行以下代码检查你的",{"2":{"144":1}}],["然后可以通过逆向词汇表将其映射回文本",{"2":{"80":1}}],["然后可以解码为文字",{"2":{"53":1}}],["然后再传递给解码器",{"2":{"96":1}}],["然后再深入文本评估",{"2":{"69":1}}],["然后再通过第二个线性变换压缩回原始维度",{"2":{"57":1}}],["然后通过",{"2":{"63":1}}],["然后将每个头的结果拼接起来",{"2":{"129":1}}],["然后将它们的输出组合起来",{"2":{"126":1}}],["然后将其传递给softmax函数来计算概率分数",{"2":{"80":1}}],["然后将其输入到",{"2":{"63":1}}],["然后将文本",{"2":{"76":1}}],["然后将这个",{"2":{"63":1}}],["然后将词元",{"2":{"48":1}}],["然后进入线性输出层",{"2":{"60":1}}],["然后",{"2":{"40":1,"57":1,"86":1,"98":1,"138":1,"154":1,"169":1,"174":1}}],["然后对其进行编码",{"2":{"18":1}}],["然而请注意",{"2":{"128":1}}],["然而在实际应用中",{"2":{"57":1}}],["然而在",{"2":{"57":1}}],["然而",{"2":{"7":2,"11":1,"16":1,"19":2,"36":1,"48":2,"57":1,"59":1,"60":1,"77":1,"78":3,"82":1,"86":1,"101":1,"120":1,"138":1,"139":1,"155":1,"190":1}}],["之间无缝切换",{"2":{"137":1}}],["之间的关系和依赖性",{"2":{"98":1}}],["之间的接近程度",{"2":{"58":1}}],["之所以如此受欢迎",{"2":{"136":1}}],["之前我们演示了如何将单个词元",{"2":{"47":1}}],["之前的词汇表大小为",{"2":{"32":1}}],["之前",{"2":{"18":1,"73":1,"151":1}}],["之后可以输入到",{"2":{"59":1}}],["之后",{"2":{"18":1}}],["将超过该阈值的梯度缩小到预设的最大值",{"2":{"190":1}}],["将text",{"2":{"187":1}}],["将top",{"2":{"82":1,"180":1}}],["将第5章中的124m模型替换为1558m模型",{"2":{"180":1}}],["将矩阵乘法在",{"2":{"167":1}}],["将学习如何保存和恢复训练后的模型",{"2":{"161":1}}],["将负梯度按比例添加到参数中",{"2":{"160":1}}],["将模型视为计算图",{"0":{"152":1}}],["将模型切换到评估模式",{"2":{"86":1}}],["将根据输入提示生成全新的文本",{"2":{"138":1}}],["将两个向量逐元素相乘并求和",{"2":{"133":1}}],["将两个输入的值归一化",{"2":{"55":1}}],["将它们组合成多头注意力层",{"2":{"129":1}}],["将它们分解为子词单元或单独的字符",{"2":{"49":1}}],["将输入分成多个头",{"2":{"129":1}}],["将输入词元编码为",{"2":{"48":1}}],["将遮罩位置设置为负无穷大",{"2":{"121":1}}],["将遮罩位置的值设置为负无穷大",{"2":{"121":1}}],["将每层的参数相加",{"2":{"174":1}}],["将每行元素除以该行的和",{"2":{"119":1}}],["将每个标记转换为",{"2":{"53":1}}],["将每个批次中的每个词元嵌入到",{"2":{"48":1}}],["将对角线以上的元素置零",{"2":{"118":1}}],["将对角线以上的值置零",{"2":{"117":1}}],["将生成的遮罩矩阵与注意力权重相乘",{"2":{"118":1}}],["将多个因果注意力模块堆叠成多头注意力模块",{"2":{"94":1}}],["将openai的模型权重加载到我们的gptmodel实例gpt中",{"2":{"89":1}}],["将openai的权重加载到我们的gpt模型代码中",{"2":{"89":1}}],["将params字典中的权重加载到gptmodel实例gpt中",{"2":{"89":1}}],["将导致分布更加尖锐",{"2":{"80":1}}],["将logits除以1",{"2":{"80":1}}],["将温度升至5使分布更均匀",{"2":{"80":1}}],["将温度降低至0",{"2":{"80":1}}],["将训练集和验证集的损失并列显示",{"2":{"78":1}}],["将项目规模放在实际背景中",{"2":{"76":1}}],["将该损失计算应用于整个文本数据集的过程",{"2":{"75":1}}],["将当前上下文裁剪到模型的最大上下文大小",{"2":{"63":1}}],["将此",{"2":{"63":1}}],["将这些输出张量转换为文本",{"2":{"61":1}}],["将这些独立的词元按字母顺序排序并移除重复项后",{"2":{"26":1}}],["将序列传递给",{"2":{"60":1}}],["将块的输入与输出相加",{"2":{"59":1}}],["将原始输入加回来",{"2":{"59":1}}],["将",{"0":{"122":1},"2":{"54":1,"60":1,"63":1,"88":1,"115":1,"158":1}}],["将数据通过",{"2":{"53":1}}],["将数据转化为向量格式的过程通常称为",{"2":{"19":1}}],["将文本词元转换为词元",{"2":{"46":1}}],["将文本拆分为词元并通过词汇表进行字符串到整数的映射",{"2":{"27":1}}],["将文本拆分为单词和子词词元",{"2":{"18":1}}],["将接收到的输入",{"2":{"42":1}}],["将单词分解为子词单元",{"2":{"36":1}}],["将单词转化为词元",{"2":{"19":1}}],["将未知单词替换为",{"2":{"33":1}}],["将其与概率采样和温度缩放相结合",{"2":{"82":1}}],["将其转换为令牌id",{"2":{"78":1}}],["将其转换回文本词元并将这些词元拼接为自然文本",{"2":{"28":1}}],["将其拆分为独立词元并通过词汇表将词元转换为词元",{"2":{"28":1}}],["将词元",{"2":{"27":2,"45":1}}],["将词元转换为词元",{"0":{"26":1},"1":{"27":1,"28":1,"29":1,"30":1}}],["将词元转换为供大型语言模型输入的向量",{"2":{"18":1}}],["将词元化应用于整个短篇小说",{"0":{"25":1}}],["将短篇小说作为文本示例导入",{"0":{"21":1}}],["将嵌入优化为",{"2":{"19":1}}],["将不同的数据类型",{"2":{"19":1}}],["将在第7章中详细讨论",{"2":{"182":1}}],["将在第",{"2":{"48":1}}],["将在这些嵌入向量上添加一个小的修改",{"2":{"47":1}}],["将在",{"2":{"8":1}}],["一篇有趣的博客文章显示",{"2":{"185":1}}],["一直是研究领域中最广泛使用的深度学习库",{"2":{"136":1}}],["一样使用",{"2":{"112":1}}],["一致",{"2":{"107":1}}],["一个张量库",{"2":{"171":1}}],["一个",{"2":{"164":1}}],["一个数",{"2":{"147":1}}],["一个流行的类",{"2":{"143":1}}],["一个是只支持",{"2":{"140":1}}],["一个紧凑的因果注意力类",{"0":{"123":1}}],["一个紧凑的自注意力类",{"0":{"110":1}}],["一个简单的无可训练权重的自注意力机制",{"0":{"99":1}}],["一个更具多样性的文本生成函数",{"2":{"83":1}}],["一个输入张量包含",{"2":{"42":1}}],["一词",{"2":{"78":1,"115":1}}],["一种更高效的创建多头注意力模块的方法是使用批量矩阵乘法",{"2":{"133":1}}],["一种包含大量重复",{"2":{"65":1}}],["一种常见的神经网络归一化方法",{"2":{"56":1}}],["一系列",{"2":{"53":1}}],["一旦前向和反向传播完成",{"2":{"169":1}}],["一旦",{"2":{"16":1}}],["一是",{"2":{"7":1}}],["翻译成另一种语言",{"2":{"95":1}}],["翻译或摘要的能力",{"2":{"16":1}}],["翻译等任务",{"2":{"14":1}}],["突现",{"2":{"16":1}}],["突现行为",{"2":{"14":1}}],["展示gpt模型中的字节对分词器的工作原理",{"2":{"183":1}}],["展示了transformer架构不仅适用于文本输入",{"2":{"182":1}}],["展示了训练代码",{"2":{"159":1}}],["展示了多头注意力模块的结构",{"2":{"126":1}}],["展示了这种自注意力机制在实现",{"2":{"104":1}}],["展示了这些不同领域的层级关系",{"2":{"8":1}}],["展示了自注意力在",{"2":{"96":1}}],["展示了在从一种语言",{"2":{"95":1}}],["展示了在前一节计算交叉熵损失之后",{"2":{"75":1}}],["展示了我们在本章中将要编码的不同注意力机制",{"2":{"94":1}}],["展示了我们在本章中组装",{"2":{"55":1}}],["展示了计算损失的几个步骤",{"2":{"72":1}}],["展示了文本评估函数的实现步骤",{"2":{"71":1}}],["展示了从输入文本到",{"2":{"71":1}}],["展示了使用",{"2":{"70":1}}],["展示了构建",{"2":{"67":1}}],["展示了一个包含",{"2":{"99":1}}],["展示了一个",{"2":{"59":1}}],["展示了一个深度神经网络的比较",{"2":{"58":1}}],["展示了一个简化版的",{"2":{"11":1}}],["展示了前馈神经网络中层输出的扩展和收缩过程",{"2":{"57":1}}],["展示了当我们传入一些输入时",{"2":{"57":1}}],["展示了层归一化的示意图",{"2":{"54":1}}],["展示了编码一个",{"2":{"94":1}}],["展示了编码",{"2":{"53":1}}],["展示了编写一个",{"2":{"51":1}}],["展示了",{"2":{"52":1,"60":1,"63":3,"71":1}}],["展示了通过使用注意力机制",{"2":{"96":1}}],["展示了通过",{"2":{"9":1}}],["展平",{"2":{"130":1}}],["展现了",{"2":{"16":1}}],["标题为",{"2":{"185":1}}],["标量是",{"2":{"149":1}}],["标量",{"0":{"149":1},"2":{"147":1}}],["标记数",{"2":{"128":1}}],["标记嵌入和位置嵌入层",{"2":{"53":1}}],["标识文本的开头",{"2":{"36":1}}],["标识某个片段的开始或结束",{"2":{"31":1}}],["标签",{"2":{"16":1,"138":1}}],["标注数据集由文本和关联的类别标签组成",{"2":{"10":1}}],["标注数据集由指令和答案对组成",{"2":{"10":1}}],["核心的注意力机制代码",{"2":{"15":1}}],["分配给这些属性",{"2":{"157":1}}],["分布式训练是指将模型训练过程分布到多个gpu和机器上进行",{"2":{"168":1}}],["分布的峰值更明显",{"2":{"80":1}}],["分布会更尖锐",{"2":{"80":1}}],["分词后的文本首先被转换为",{"2":{"60":1}}],["分词器将输入文本转换为一系列",{"2":{"70":1}}],["分词器",{"2":{"53":1}}],["分词器相同",{"2":{"53":1}}],["分别有30个和20个节点",{"2":{"174":1}}],["分别结合了高斯分布和",{"2":{"57":1}}],["分别是",{"2":{"46":1}}],["分母为输入数量",{"2":{"55":1}}],["分隔词元的",{"2":{"34":1}}],["分三步进行",{"2":{"15":1}}],["分类",{"2":{"14":1}}],["分类微调",{"2":{"10":1}}],["奠定了基础",{"2":{"15":1}}],["学习率逐步降低",{"2":{"189":1}}],["学习率以线性预热阶段开始",{"2":{"189":1}}],["学习率从较低的初始值开始",{"2":{"188":1}}],["学习率达到峰值0",{"2":{"188":1}}],["学习率预热在前20个训练步骤中逐步提高学习率",{"2":{"188":1}}],["学习率预热",{"0":{"188":1}}],["学习率是一个超参数",{"2":{"159":1}}],["学习文本数据处理",{"2":{"135":1}}],["学习构建这些上下文向量",{"2":{"99":1}}],["学习过程的起点",{"2":{"46":1}}],["学习",{"2":{"14":1}}],["仅包含124m参数",{"2":{"180":1}}],["仅需更改几行代码",{"2":{"164":1,"165":1}}],["仅在训练期间使用",{"2":{"121":1}}],["仅保留该词之前",{"2":{"115":1}}],["仅考虑序列中之前和当前的输入",{"2":{"115":1}}],["仅仅三年后",{"2":{"96":1}}],["仅通过更新配置文件",{"2":{"62":1}}],["仅用于演示目的",{"2":{"44":1}}],["仅使用解码器模块",{"2":{"16":1}}],["仅使用了数据的一部分",{"2":{"13":1}}],["仅做了少量修改",{"2":{"14":1}}],["于",{"2":{"14":1}}],["层或",{"2":{"160":1}}],["层位于索引位置",{"2":{"155":1}}],["层中",{"2":{"155":1}}],["层可以进一步优化",{"2":{"111":1}}],["层改进",{"0":{"111":1}}],["层的自注意力类",{"0":{"112":1}}],["层的",{"2":{"88":1}}],["层使用单独的",{"2":{"64":1}}],["层组成的深度神经网络",{"2":{"58":1}}],["层网络",{"2":{"58":1}}],["层归一化",{"2":{"58":1,"59":1,"65":1}}],["层归一化每个输入独立于批次大小进行归一化",{"2":{"56":1}}],["层归一化与批量归一化的比较",{"0":{"56":1}}],["层归一化代码正常工作",{"2":{"55":1}}],["层归一化类",{"2":{"54":1}}],["层归一化通常在多头注意力模块的前后以及最终输出层之前应用",{"2":{"54":1}}],["层归一化的主要思想是将神经网络层的激活值",{"2":{"54":1}}],["层数",{"2":{"53":1,"187":1}}],["层",{"2":{"14":1,"53":1,"58":1,"60":1,"64":1,"160":1}}],["939",{"2":{"190":1}}],["934",{"2":{"190":1}}],["933",{"2":{"78":1}}],["978",{"2":{"172":2}}],["9737",{"2":{"130":1}}],["9736e",{"2":{"54":1}}],["959",{"2":{"177":1}}],["9509",{"2":{"161":1}}],["9544",{"2":{"99":3,"101":4}}],["9所示",{"2":{"155":1}}],["91",{"2":{"161":1}}],["9156",{"2":{"130":1}}],["9178",{"2":{"46":1,"47":1}}],["9450",{"2":{"101":2}}],["9422",{"2":{"101":4}}],["9485",{"2":{"60":1}}],["9484",{"2":{"53":1}}],["9kib",{"2":{"88":1}}],["9000",{"2":{"157":4}}],["90",{"2":{"77":2,"80":1,"88":2,"187":1}}],["98758347829183",{"2":{"77":1}}],["9836e",{"2":{"71":1}}],["9802",{"2":{"54":1}}],["9802e",{"2":{"54":2}}],["98110580444336",{"2":{"77":1}}],["981",{"2":{"34":1}}],["9949",{"2":{"161":1}}],["9982",{"2":{"161":1}}],["9991",{"2":{"161":1}}],["9995",{"2":{"101":2}}],["9912",{"2":{"130":1}}],["9951",{"2":{"72":1}}],["99",{"2":{"21":1}}],["9",{"0":{"49":1,"163":1,"164":1,"165":1,"168":1},"1":{"164":1,"165":1,"166":2,"167":2,"168":1,"169":2,"170":2},"2":{"15":2,"31":3,"57":3,"72":1,"76":1,"77":2,"78":4,"99":1,"155":1,"156":2,"159":2,"164":1}}],["9693",{"2":{"161":1}}],["961",{"2":{"78":1}}],["9666",{"2":{"46":2,"47":1}}],["96",{"2":{"14":1}}],["9268",{"2":{"130":1}}],["922",{"2":{"45":2,"48":1}}],["92",{"2":{"13":1}}],["自动微分功能以及深度学习工具",{"2":{"171":1}}],["自动微分",{"2":{"159":1}}],["自动微分简化计算",{"0":{"153":1},"1":{"154":1}}],["自定义的llm",{"2":{"182":1}}],["自定义",{"2":{"157":1}}],["自2019年以来",{"2":{"136":1}}],["自带的",{"2":{"99":1}}],["自我",{"0":{"98":1},"2":{"98":1}}],["自适应优化器",{"2":{"86":1}}],["自回归模型将先前的输出作为未来预测的输入",{"2":{"14":1}}],["自注意力机制作为缩放点积注意力的概念首次提出于最初的transformer论文",{"2":{"184":1}}],["自注意力机制将上下文向量表示计算为输入的加权和",{"2":{"133":1}}],["自注意力机制可以识别并分析输入序列中元素之间的关系",{"2":{"59":1}}],["自注意力涉及可训练的权重矩阵",{"2":{"110":1}}],["自注意力实现的第一步是计算注意力得分",{"2":{"99":1}}],["自注意力中的",{"0":{"98":1}}],["自注意力是基于",{"2":{"96":1,"97":1}}],["自注意力是一种机制",{"2":{"96":1}}],["自注意力",{"2":{"11":1}}],["像",{"2":{"14":1}}],["节的训练循环修改为在",{"2":{"165":1}}],["节的训练循环以在",{"2":{"164":1}}],["节的带可训练权重的自注意力机制打下基础",{"2":{"99":1}}],["节中的",{"2":{"126":1}}],["节中编写的基本自注意力机制相比",{"2":{"104":1}}],["节中",{"2":{"98":1,"104":1}}],["节中实现的",{"2":{"59":1,"60":1}}],["节实现的训练过程中更新模型权重实现该目标",{"2":{"72":1}}],["节实现的",{"2":{"60":1}}],["节所述",{"2":{"59":1}}],["节所讨论的那样",{"2":{"14":1}}],["节使用的小词汇表中产生的",{"2":{"47":1}}],["节",{"2":{"42":1,"43":1,"46":1,"48":1,"54":1,"57":1,"58":2,"72":1,"95":1,"104":1,"109":1,"121":1}}],["节介绍的原始",{"2":{"14":1}}],["节详细讲解",{"2":{"8":1}}],["722",{"2":{"179":1}}],["7267",{"2":{"63":1}}],["7节典型训练循环中看到这一点",{"2":{"155":1}}],["7节的典型训练循环中重新讨论这种训练循环的实现",{"2":{"154":1}}],["7所示",{"2":{"152":1}}],["792",{"2":{"179":1}}],["7939",{"2":{"110":1}}],["79",{"2":{"80":1}}],["754748503367106",{"2":{"180":1}}],["7548",{"2":{"161":1}}],["752",{"2":{"174":2}}],["7599",{"2":{"122":1}}],["7500",{"2":{"82":2}}],["75",{"2":{"80":1,"159":1,"165":1}}],["7559e",{"2":{"71":1}}],["707",{"2":{"190":1}}],["7070",{"2":{"99":1,"101":4}}],["7000",{"2":{"157":2}}],["7003",{"2":{"130":2}}],["7058",{"2":{"130":1}}],["70mib",{"2":{"88":1}}],["70",{"2":{"76":1}}],["7026",{"2":{"45":2,"48":1,"177":2}}],["774m到1",{"2":{"185":1}}],["774m",{"2":{"89":2}}],["77",{"2":{"88":2,"99":1,"151":2}}],["7722",{"2":{"72":3,"73":1}}],["7705",{"2":{"60":1}}],["773",{"2":{"29":1}}],["7176",{"2":{"161":1}}],["7179",{"2":{"130":1}}],["7154",{"2":{"101":4}}],["716",{"2":{"63":2}}],["7195",{"2":{"60":1}}],["7120",{"2":{"60":1}}],["7130",{"2":{"53":1}}],["7342",{"2":{"161":1}}],["7349",{"2":{"53":1}}],["7388",{"2":{"130":1}}],["738",{"2":{"34":1}}],["768",{"2":{"53":2,"57":7,"59":5,"60":2,"70":1,"89":3,"132":1,"178":2,"187":1}}],["762",{"2":{"52":1,"78":1}}],["763",{"2":{"29":1}}],["7",{"0":{"15":1,"46":1,"63":1,"133":1,"159":1},"1":{"47":1,"64":1,"160":1,"161":1},"2":{"14":2,"27":2,"29":2,"34":1,"55":2,"58":1,"60":1,"65":1,"71":4,"72":6,"73":1,"78":1,"80":1,"82":1,"88":2,"99":6,"149":1,"152":1,"156":1,"157":1,"164":2,"165":1,"175":1}}],["789",{"2":{"112":1}}],["7891",{"2":{"110":1}}],["781",{"2":{"78":1}}],["7860",{"2":{"53":1}}],["7849",{"2":{"46":1,"47":1}}],["78",{"2":{"13":1,"88":1}}],["任务进行预训练",{"2":{"14":1}}],["任务中都展示了较强的通用能力",{"2":{"7":1}}],["任务中都展现出了显著的性能提升",{"2":{"7":1}}],["令人惊讶的是",{"2":{"14":1}}],["链接",{"2":{"14":1,"20":1,"37":1,"47":1}}],["up",{"2":{"190":2}}],["update",{"2":{"89":3}}],["upper",{"2":{"78":1}}],["urlopen",{"2":{"187":1}}],["urlretrieve",{"2":{"88":1}}],["url",{"2":{"88":3,"187":2}}],["urllib",{"2":{"88":2,"187":2}}],["utils",{"2":{"43":1,"157":2,"170":1,"190":2}}],["utf",{"2":{"21":1,"42":1,"44":1,"76":1,"187":3}}],["units",{"2":{"183":1,"185":1}}],["unit",{"2":{"57":2,"185":1}}],["unbiased=false",{"2":{"54":1,"55":2}}],["unsqueeze",{"2":{"63":1,"70":1}}],["unsafeviewbackward0>",{"2":{"53":1,"60":1}}],["unsupervised",{"2":{"14":1,"52":1,"185":2}}],["understanding",{"2":{"14":2,"182":1}}],["using",{"2":{"172":1}}],["us",{"2":{"83":1,"89":1}}],["use",{"2":{"58":7}}],["used",{"2":{"0":1}}],["usedata",{"2":{"0":3}}],["usage",{"2":{"0":1}}],["最大梯度值明显减小",{"2":{"190":1}}],["最大梯度值为",{"2":{"190":1}}],["最高提高到0",{"2":{"188":1}}],["最早的编码器风格transformer模型",{"2":{"182":1}}],["最早在以下论文中提出",{"2":{"14":1}}],["最小gpt",{"2":{"178":1}}],["最好使用",{"2":{"155":1}}],["最显著的差别是引入了在模型训练过程中更新的权重矩阵",{"2":{"104":1}}],["最初遵循了",{"2":{"151":1}}],["最初是用",{"2":{"142":1}}],["最初是通过",{"2":{"88":1}}],["最初",{"2":{"58":1}}],["最初的",{"2":{"16":1}}],["最终嵌入维度为",{"2":{"127":1}}],["最终目标是实现一个紧凑且高效的多头注意力模块",{"2":{"94":1}}],["最终",{"2":{"60":1}}],["最终将各个组件集成到",{"2":{"53":1}}],["最终使模型从训练数据中学习",{"2":{"52":1}}],["最终得到用于",{"2":{"48":1}}],["最后讨论如何使用多个",{"2":{"163":1}}],["最后是监控步骤",{"2":{"78":1}}],["最后计算并平均损失",{"2":{"77":1}}],["最后对文本块行进行随机化并组织成批次供模型训练使用",{"2":{"76":1}}],["最后一节将这些技术整合到第5章中开发的训练函数中",{"2":{"187":1}}],["最后一步",{"0":{"109":1},"2":{"99":1}}],["最后一个批次大小过小会影响收敛性",{"2":{"157":1}}],["最后一个数字",{"2":{"71":1}}],["最后一个维度",{"2":{"60":1}}],["最后一层返回一个单一输出值",{"2":{"58":1}}],["最后定义了一个不带偏置的线性输出头",{"2":{"60":1}}],["最后的层归一化层",{"2":{"53":1}}],["最后",{"2":{"15":1,"18":1,"40":1,"61":1,"63":1,"70":1,"94":1,"110":1,"137":1}}],["最后值得注意的是",{"2":{"14":1}}],["最核心的启示是",{"2":{"12":1}}],["现有的llm也可以通过微调来适应特定领域",{"2":{"182":1}}],["现在通过值向量的加权和计算上下文向量",{"2":{"109":1}}],["现在都被嵌入为一个",{"2":{"48":1}}],["现在让我们看看如何保存训练后的模型",{"2":{"162":1}}],["现在让我们实例化一个带有跳跃连接的模型",{"2":{"58":1}}],["现在让我们使用",{"2":{"58":1}}],["现在让我们使用嵌入层将这些词元",{"2":{"48":1}}],["现在让我们更详细地了解",{"2":{"14":1}}],["现在我们通过将注意力得分除以键嵌入维度的平方根进行缩放",{"2":{"108":1}}],["现在我们可以应用",{"2":{"77":1}}],["现在我们已经具备了实现",{"2":{"59":1}}],["现在我们已经实现了一个基本的词元化器",{"2":{"25":1}}],["现在我们使用更实际和有用的嵌入尺寸",{"2":{"48":1}}],["现在",{"2":{"26":1,"47":1,"48":2,"53":1,"60":1,"63":1,"77":1,"88":1,"159":1,"170":1}}],["现代llm中使用的一种流行的layernorm变体是rmsnorm",{"2":{"185":1}}],["现代",{"2":{"16":1,"60":1}}],["现今的",{"2":{"11":1}}],["现今它们被广泛应用于各个领域",{"2":{"9":1}}],["已通过",{"2":{"141":1}}],["已在第",{"2":{"53":1}}],["已公开了",{"2":{"53":1}}],["已作为开源模型发布",{"2":{"13":1}}],["已考虑到可能的四舍五入误差",{"2":{"13":1}}],["好消息是",{"2":{"13":1}}],["成形状",{"2":{"130":1}}],["成本非常高昂",{"2":{"13":1}}],["成功的背后有两个关键因素",{"2":{"7":1}}],["yamlgpt",{"2":{"179":1}}],["yamltraining",{"2":{"180":1}}],["yamltensor",{"2":{"177":1}}],["yamltotal",{"2":{"155":1}}],["yamltoken",{"2":{"89":1}}],["yamlepoch",{"2":{"159":1,"165":1}}],["yaml5",{"2":{"157":1}}],["yes",{"2":{"78":1,"190":1}}],["ylabel",{"2":{"57":1,"78":1,"80":1,"188":1,"189":1}}],["y",{"2":{"42":7,"57":7,"77":4,"152":3,"154":2,"156":2,"157":11,"161":2}}],["ycombinator",{"2":{"13":1}}],["youtube",{"2":{"172":1,"186":1}}],["yourself",{"2":{"32":1}}],["your",{"2":{"32":1,"99":3,"104":1,"115":1}}],["younger",{"2":{"32":1}}],["you",{"2":{"11":1,"29":2,"30":1,"34":2,"35":1,"37":2,"53":1,"70":2,"71":2,"78":6,"79":3,"80":7,"82":1,"83":3,"89":2,"182":1,"184":1,"190":4}}],["讨论",{"2":{"13":1}}],["human",{"2":{"182":1}}],["hyena",{"2":{"182":1}}],["html",{"2":{"150":1,"151":1,"172":1,"184":3}}],["http",{"2":{"14":1}}],["https",{"2":{"13":5,"16":1,"88":1,"89":1,"136":3,"141":4,"142":1,"143":1,"150":1,"151":1,"170":1,"172":5,"176":1,"182":12,"183":7,"184":8,"185":10,"186":14,"187":1}}],["h",{"2":{"129":1,"170":2}}],["hprams",{"2":{"88":1}}],["holds",{"2":{"53":1}}],["hot",{"2":{"47":1}}],["highlighted",{"2":{"193":2}}],["highlighting",{"0":{"193":1},"2":{"193":2}}],["highest",{"2":{"190":4}}],["his",{"2":{"190":2}}],["hierarchical",{"2":{"186":1}}],["hierarchy",{"2":{"182":1}}],["him",{"2":{"78":1,"190":2}}],["himself",{"2":{"42":3}}],["hidden",{"2":{"41":1}}],["hendricks和gimpel",{"2":{"185":1}}],["henetflix",{"2":{"71":1}}],["he和hofmann",{"2":{"184":1}}],["help",{"2":{"63":2}}],["hello",{"2":{"23":4,"24":2,"30":3,"34":2,"35":1,"37":2,"63":7}}],["head",{"2":{"53":3,"60":4,"89":3,"125":1,"127":2,"128":2,"129":4,"130":1,"190":2}}],["heads=4",{"2":{"127":1}}],["heads=2",{"2":{"127":2,"128":3,"131":1,"178":1}}],["heads=cfg",{"2":{"59":1}}],["heads",{"2":{"53":2,"59":1,"70":1,"89":4,"127":5,"128":1,"129":10,"130":1,"178":2,"179":1,"187":1}}],["he",{"2":{"27":1,"29":2,"78":2,"79":1,"190":2}}],["harmdevries",{"2":{"185":1}}],["harm",{"2":{"185":1}}],["hayden",{"2":{"172":1}}],["has",{"2":{"27":1,"58":11}}],["had",{"2":{"21":1,"25":1,"78":2,"83":1}}],["hackernews",{"2":{"13":1}}],["h0jwoz",{"2":{"16":1}}],["帖子中",{"2":{"13":1}}],["则无法判断模型是否对训练数据过拟合",{"2":{"180":1}}],["则模型不会对训练集进行显式过拟合",{"2":{"180":1}}],["则可以将处理一个epoch的速度提高到八倍",{"2":{"169":1}}],["则可能是因为计算机没有兼容的",{"2":{"143":1}}],["则可能来自",{"2":{"13":1}}],["则将这些参数应用到模型中",{"2":{"162":1}}],["则会启动多个工作进程以并行加载数据",{"2":{"158":1}}],["则会在列间执行操作",{"2":{"54":1}}],["则处理数据的随机化和批量组装",{"2":{"156":1}}],["则说明你的",{"2":{"144":1}}],["则说明一切准备就绪",{"2":{"143":1}}],["则结果是",{"2":{"127":1}}],["则是进入",{"2":{"73":1}}],["则是一个平滑的非线性函数",{"2":{"57":1}}],["则在自注意力和前馈网络之后应用层归一化",{"2":{"59":1}}],["则在广泛的",{"2":{"7":1}}],["则此层将有",{"2":{"58":1}}],["则选择性地添加图",{"2":{"58":1}}],["则需要一种方法来将数据集划分到这些不同的进程中",{"2":{"170":1}}],["则需要",{"2":{"53":1}}],["则其参数总数为",{"2":{"52":1}}],["则包含来自",{"2":{"13":1}}],["很可能源自",{"2":{"13":1}}],["语句的输出如下",{"2":{"48":1}}],["语句将返回以下内容",{"2":{"48":1}}],["语句将输出嵌入层的权重矩阵",{"2":{"46":1}}],["语言间的翻译模式并执行翻译任务",{"2":{"14":1}}],["语料库则由英语维基百科内容构成",{"2":{"13":1}}],["语义",{"2":{"12":1}}],["研究人员发现构建自然语言处理的深度神经网络并不需要",{"2":{"96":1}}],["研究人员在",{"2":{"96":1}}],["研究人员起初没有预料到这一点",{"2":{"14":1}}],["研究论文",{"2":{"13":1}}],["研究表明",{"2":{"10":1}}],["进入余弦衰减",{"2":{"189":1}}],["进度条工具来跟踪下载过程",{"2":{"88":1}}],["进一步遮罩注意力权重",{"2":{"121":1}}],["进一步说明了该过程的原理",{"2":{"70":1}}],["进一步扩展了训练数据",{"2":{"13":1}}],["进行分布式训练",{"2":{"163":1}}],["进行比较",{"2":{"152":1}}],["进行深度学习",{"0":{"141":1}}],["进行了描述",{"2":{"126":1}}],["进行训练",{"2":{"104":1}}],["进行不同任务",{"2":{"95":1}}],["进行预训练",{"2":{"67":1}}],["进行配置",{"2":{"59":1}}],["进行编码并打印各个词元",{"2":{"40":1}}],["进行拆分",{"2":{"23":1}}],["进行示例操作",{"2":{"23":1}}],["进行微调得出的模型",{"2":{"14":1}}],["进行进一步训练",{"2":{"10":1}}],["进行上下文分析或创作连贯的原创文本",{"2":{"7":1}}],["总和为",{"2":{"155":1}}],["总的来说",{"2":{"58":1}}],["总是映射到相同的向量表示",{"2":{"48":1}}],["总结如图a",{"2":{"138":1}}],["总结",{"0":{"16":1,"65":1,"92":1,"133":1,"171":1}}],["总结长篇内容",{"2":{"9":1}}],["总计约",{"2":{"13":1}}],["甚至自动驾驶汽车等技术",{"2":{"138":1}}],["甚至无法正确收敛",{"2":{"86":1}}],["甚至一些需要常识性知识的任务",{"2":{"12":1}}],["甚至编写计算机代码",{"2":{"9":1}}],["其l2范数描述为",{"2":{"190":1}}],["其计算出在单个rtx",{"2":{"185":1}}],["其处理数据集的速度更快",{"2":{"169":1}}],["其操作在同一设备上执行",{"2":{"164":1}}],["其次",{"2":{"137":1}}],["其隐藏状态",{"2":{"95":1}}],["其前一步的输出作为当前步的输入",{"2":{"95":1}}],["其他token更频繁地被选择",{"2":{"80":1}}],["其他特殊词元",{"0":{"36":1}}],["其精确形式定义为",{"2":{"57":1}}],["其",{"2":{"46":1}}],["其主要目的是将非数值数据转换为神经网络能够处理的格式",{"2":{"19":1}}],["其核心思想是注意力机制",{"2":{"16":1}}],["其单向",{"2":{"14":1}}],["其数量大致相当于文本中的单词和标点符号数量",{"2":{"12":1}}],["其中三条数据属于类别0",{"2":{"156":1}}],["其中使用相似的概念来存储",{"2":{"109":1}}],["其中gpt是一个gptmodel实例",{"2":{"89":1}}],["其中非top",{"2":{"82":1}}],["其中每个元素",{"2":{"99":1}}],["其中每个组件后面都接有一个捷径连接",{"2":{"59":1}}],["其中每一行表示一个输入上下文",{"2":{"42":1}}],["其中两个典型的例子是",{"2":{"57":1}}],["其中第一行列出第一个输入的层输出",{"2":{"54":1}}],["其中实现了一个神经网络层",{"2":{"54":1}}],["其中包含可训练的权重矩阵",{"2":{"133":1}}],["其中包含两个输入文本",{"2":{"122":1}}],["其中包含从",{"2":{"48":1}}],["其中包含机器学习和深度学习等子领域",{"2":{"8":1}}],["其中",{"2":{"8":1,"42":1,"54":1,"57":2}}],["861",{"2":{"190":1}}],["86",{"2":{"177":3}}],["8633",{"2":{"74":1}}],["8显示了偏导数",{"2":{"154":1}}],["8所示",{"2":{"153":1}}],["825",{"2":{"190":1}}],["8203",{"2":{"110":1}}],["8210",{"2":{"109":1,"110":2}}],["8296",{"2":{"101":4}}],["8111",{"2":{"107":1}}],["812",{"2":{"29":1}}],["843",{"2":{"190":1}}],["8434",{"2":{"99":1,"101":4}}],["8424",{"2":{"130":2}}],["8400",{"2":{"46":1,"47":1}}],["800gb",{"2":{"182":1,"186":1}}],["800",{"2":{"179":1}}],["8000消费级gpu上训练gpt",{"2":{"185":1}}],["8000",{"2":{"53":1}}],["8040",{"2":{"110":1}}],["8053",{"2":{"110":1}}],["80",{"2":{"99":1}}],["8573",{"2":{"130":1}}],["8524",{"2":{"107":2}}],["85",{"2":{"99":1}}],["8569",{"2":{"161":1}}],["856",{"2":{"78":1}}],["8993",{"2":{"130":1}}],["89",{"2":{"80":2,"99":1}}],["8节中会详细介绍",{"2":{"147":1}}],["8节",{"2":{"78":1}}],["8xa100",{"2":{"76":1}}],["8x4x256",{"2":{"48":1}}],["8x4",{"2":{"48":1}}],["83",{"2":{"61":2}}],["8812",{"2":{"37":1}}],["8887",{"2":{"37":1}}],["87",{"2":{"99":1}}],["8719",{"2":{"54":3}}],["873",{"2":{"29":1}}],["872",{"2":{"29":1}}],["8",{"0":{"16":1,"48":1,"65":1,"162":1},"2":{"12":2,"14":2,"21":1,"28":2,"42":1,"44":1,"48":7,"57":4,"63":1,"75":1,"76":1,"78":2,"80":1,"99":1,"149":1,"153":1,"156":2,"157":1,"175":3,"187":3,"190":2}}],["网站",{"2":{"141":1}}],["网站爬取数据",{"2":{"12":2}}],["网络中的文本生成解码器部分可以选择性地访问所有输入标记",{"2":{"96":1}}],["网络难以学习到数据中的潜在模式",{"2":{"54":1}}],["网络图书语料库",{"2":{"12":2}}],["亿和",{"2":{"65":1}}],["亿呢",{"2":{"60":1}}],["亿的",{"2":{"62":1}}],["亿的最大",{"2":{"60":1}}],["亿的词元子集",{"2":{"13":1}}],["亿参数",{"2":{"60":2,"65":1}}],["亿参数架构中重复多次",{"2":{"59":1}}],["亿参数的内存需求",{"2":{"61":2}}],["亿参数的",{"2":{"53":1,"57":1,"60":2,"76":1}}],["亿扩展到",{"2":{"53":1}}],["亿个参数的小型版本",{"2":{"52":1}}],["亿个参数",{"2":{"14":1}}],["亿词元",{"2":{"13":2}}],["亿",{"2":{"12":5,"52":1,"53":1,"65":2}}],["第1章",{"0":{"182":1}}],["第1隐藏层",{"2":{"155":1}}],["第5章讨论了llm的预训练",{"2":{"186":1}}],["第5章",{"0":{"180":1,"186":1}}],["第4章",{"0":{"179":1,"185":1}}],["第3章",{"0":{"178":1,"184":1}}],["第2章",{"0":{"177":1,"183":1}}],["第2隐藏层",{"2":{"155":1}}],["第四步",{"0":{"119":1}}],["第三维度则是每个标记的",{"2":{"128":1}}],["第三步",{"0":{"108":1,"118":1}}],["第三轮添加",{"2":{"63":1}}],["第二维度对应每个输入的",{"2":{"128":1}}],["第二步",{"0":{"107":1,"117":1}}],["第二批",{"2":{"71":1}}],["第二批次的词元",{"2":{"44":1}}],["第二轮添加",{"2":{"63":1}}],["第二行包含第二个输入行的均值",{"2":{"54":1}}],["第二个隐藏层",{"2":{"174":1}}],["第二个小节将以更复杂但计算更高效的方式实现同样的多头注意力模块",{"2":{"125":1}}],["第二个数字",{"2":{"71":1}}],["第二个词元会有另一个不同的嵌入",{"2":{"48":1}}],["第二个张量存储目标词元",{"2":{"44":1}}],["第二章",{"2":{"11":1}}],["第一行的输出表示该训练样本属于类别0的概率为99",{"2":{"161":1}}],["第一步",{"0":{"116":1}}],["第一步是实现真正的层归一化类",{"2":{"53":1}}],["第一步是将原始文本分割成词元",{"2":{"49":1}}],["第一批",{"2":{"71":1}}],["第一批次输入中的第二个",{"2":{"44":1}}],["第一轮添加",{"2":{"63":1}}],["第一个隐藏层",{"2":{"174":1}}],["第一个小节将通过堆叠多个",{"2":{"125":1}}],["第一个词元会有特定的位置嵌入",{"2":{"48":1}}],["第一个张量存储输入词元",{"2":{"44":1}}],["第一版的基础模型",{"2":{"12":1}}],["第",{"2":{"15":1,"53":1,"71":3,"72":1,"99":1}}],["表明模型在训练集上收敛了",{"2":{"159":1}}],["表中的相应设置更新我们之前定义的完整",{"2":{"89":1}}],["表中显示的比例总和约为",{"2":{"13":1}}],["表示梯度向量在模型参数空间中的长度或幅度",{"2":{"190":1}}],["表示进程总数",{"2":{"170":1}}],["表示当前进程的id",{"2":{"170":1}}],["表示使用nccl",{"2":{"170":1}}],["表示在计算图中用于计算变量的最后一个函数",{"2":{"155":1}}],["表示输入项的实际内容或表示",{"2":{"109":1}}],["表示一个特定的词",{"2":{"99":1}}],["表示之间转换",{"2":{"70":1}}],["表示下一个",{"2":{"60":1}}],["表示张量的最后一个维度",{"2":{"54":1}}],["表示有",{"2":{"53":1}}],["表示嵌入维度",{"2":{"53":1,"54":1}}],["表示模型能够处理的最大输入标记数量",{"2":{"53":1}}],["表示",{"2":{"42":1,"48":1}}],["表",{"2":{"12":2,"13":1}}],["表现优异但应用面较窄",{"2":{"7":1}}],["涵盖了广泛的主题和语言",{"2":{"12":1}}],["利用",{"2":{"111":1,"117":1}}],["利用自监督学习",{"2":{"16":1}}],["利用大型数据集",{"0":{"12":1},"1":{"13":1}}],["利用了其处理和生成类似人类文本的能力",{"2":{"8":1}}],["仍远未达到人类的通用智能水平",{"2":{"138":1}}],["仍然适用",{"2":{"138":1}}],["仍然是最可能的token",{"2":{"80":1}}],["仍然是一个",{"2":{"60":1}}],["仍然非常重要",{"2":{"14":1}}],["仍然需要收集标签数据",{"2":{"8":1}}],["仍有待观察",{"2":{"11":1}}],["竞争",{"2":{"11":1}}],["型",{"2":{"11":1}}],["能够对层输出进行建模",{"2":{"185":1}}],["能够在特定领域的任务上优于通用llm",{"2":{"182":1}}],["能够将向量和矩阵推广到更高维度",{"2":{"147":1}}],["能够为高级用户提供调节模型底层细节的能力",{"2":{"136":1}}],["能够学习生成",{"2":{"104":1}}],["能够从数据中学习并在特定任务上提升表现",{"2":{"103":1}}],["能够逐词生成",{"2":{"94":1}}],["能够保存llm非常重要",{"2":{"86":1}}],["能够生成连贯的文本序列",{"2":{"11":1}}],["能够生成文本",{"2":{"8":1}}],["能否在能力上与",{"2":{"11":1}}],["都有一个键用于与查询匹配",{"2":{"109":1}}],["都被表示为一个固定大小的向量",{"2":{"59":1}}],["都将映射为相同的嵌入向量",{"2":{"48":1}}],["都基于",{"2":{"11":1}}],["都是更复杂和平滑的激活函数",{"2":{"57":1}}],["都是",{"2":{"11":1,"89":1}}],["也深表感谢",{"2":{"196":1}}],["也不必担心",{"2":{"95":1,"154":1}}],["也需要安装它",{"2":{"88":1}}],["也需要相对较大的存储容量",{"2":{"61":1}}],["也有一定概率被采样",{"2":{"80":1}}],["也称为反向模式自动微分或反向传播",{"2":{"153":1}}],["也称为监督学习",{"2":{"138":1}}],["也称为",{"2":{"137":1,"152":1}}],["也称为遮罩注意力",{"2":{"115":1}}],["也称为解码策略",{"2":{"79":1}}],["也称为激活值",{"2":{"54":1}}],["也可以加速模型训练中的评估",{"2":{"77":1}}],["也可以轻松执行",{"2":{"75":1}}],["也可以用于计算机视觉",{"2":{"11":1}}],["也是一种深度神经网络",{"2":{"138":1}}],["也是一个深度学习库",{"2":{"137":1}}],["也是本书余下内容的主要工具",{"2":{"135":1}}],["也是我们稍后用以更新模型权重",{"2":{"71":1}}],["也是如此",{"2":{"60":1}}],["也被翻译成相应的文本",{"2":{"63":1}}],["也仍然可以参与学习过程",{"2":{"57":1}}],["也就是单位方差",{"2":{"54":1}}],["也就是所谓的词元嵌入",{"2":{"48":1}}],["也就是说",{"2":{"14":1}}],["也能完成翻译任务",{"2":{"14":1}}],["也并非所有",{"2":{"11":1}}],["同样",{"2":{"11":1}}],["同时在通用llm基准测试中也保持了良好的表现",{"2":{"182":1}}],["同时比",{"2":{"150":1}}],["同时保持",{"2":{"128":1}}],["同时保持它们的维度不变",{"2":{"59":1}}],["同时也是实现",{"2":{"97":1}}],["同时避免批次之间的重叠",{"2":{"45":1}}],["同时了解如何加载开源模型权重",{"2":{"16":1}}],["同时提供不同",{"2":{"14":1}}],["同时展现了非凡的多功能性",{"2":{"11":1}}],["同时",{"2":{"9":1,"79":1}}],["需要使用在gpt",{"2":{"180":1}}],["需要通过观察损失值进行调整",{"2":{"159":1}}],["需要简要讨论如何在",{"2":{"156":1}}],["需要创建多个自注意力机制",{"2":{"126":1}}],["需要启用这些偏置向量",{"2":{"89":1}}],["需要",{"2":{"53":1}}],["需要将文本数据转换为数值向量",{"2":{"49":1}}],["需要预测的目标词元",{"2":{"42":2}}],["需要先准备训练数据集",{"2":{"18":1}}],["需要大量资源",{"2":{"13":1}}],["需要注意的是",{"2":{"11":1,"15":1,"42":1,"96":1,"104":1,"164":1}}],["需要澄清的是",{"2":{"7":1}}],["微调过程中跳过昂贵的预训练阶段",{"2":{"13":1}}],["微调或改变任务特定的模型架构",{"2":{"11":1}}],["微调的两大常用类型包括",{"2":{"10":1}}],["6节中提到",{"2":{"182":1}}],["6节中的代码来计算参数数量和内存需求",{"2":{"179":1}}],["68",{"2":{"179":1}}],["6806",{"2":{"60":1}}],["6所示",{"2":{"147":1}}],["6×6",{"2":{"121":1}}],["65",{"2":{"159":1,"165":1}}],["6584",{"2":{"130":1}}],["6503",{"2":{"103":1}}],["6565",{"2":{"101":4}}],["6515",{"2":{"99":1,"103":2}}],["6247",{"2":{"179":1}}],["6202",{"2":{"128":2}}],["6206",{"2":{"122":1}}],["6298",{"2":{"103":1}}],["62",{"2":{"65":1,"80":1}}],["621",{"2":{"61":2}}],["637",{"2":{"179":1}}],["6300",{"2":{"128":2}}],["6310",{"2":{"101":4}}],["6315",{"2":{"46":1}}],["63",{"2":{"60":1,"61":2,"80":1,"88":1,"175":1}}],["6398",{"2":{"53":1}}],["6783e",{"2":{"71":1}}],["6755",{"2":{"53":1}}],["6720",{"2":{"53":1}}],["66",{"2":{"88":1,"99":1}}],["661",{"2":{"78":1}}],["6621",{"2":{"53":1}}],["6622",{"2":{"53":1,"60":1}}],["6654",{"2":{"101":2}}],["665",{"2":{"53":1}}],["6496",{"2":{"103":1}}],["64",{"2":{"99":1,"150":3}}],["645",{"2":{"45":2,"48":1}}],["642",{"2":{"34":1}}],["6194",{"2":{"122":1}}],["616",{"2":{"78":1}}],["6100",{"2":{"53":1,"60":1,"71":2}}],["6109",{"2":{"53":2,"60":2}}],["617",{"2":{"37":1}}],["6159",{"2":{"54":1}}],["615",{"2":{"29":1}}],["6936",{"2":{"60":1}}],["69",{"2":{"29":1}}],["6000",{"2":{"157":2}}],["6047",{"2":{"53":1}}],["608",{"2":{"29":1}}],["60",{"2":{"12":1,"75":1}}],["6",{"0":{"14":1,"42":1,"44":1,"60":1,"91":1,"92":1,"125":1,"126":1,"129":1,"156":1},"1":{"43":1,"44":1,"45":1,"47":1,"61":1,"62":1,"126":1,"127":1,"128":1,"129":1,"130":2,"131":2,"132":2,"157":1,"158":1},"2":{"11":2,"14":1,"26":2,"43":1,"44":1,"46":3,"48":1,"52":1,"53":2,"54":5,"55":1,"59":1,"60":2,"71":3,"72":3,"73":2,"76":1,"78":4,"80":3,"82":4,"96":2,"101":2,"106":3,"121":2,"122":2,"124":1,"128":2,"131":1,"147":1,"149":1,"151":5,"156":3,"157":2,"164":1,"180":2,"190":6}}],["编程语言创建的",{"2":{"142":1}}],["编写",{"0":{"52":1,"60":1},"1":{"53":1,"61":1,"62":1}}],["编写一个类似",{"2":{"51":1}}],["编写代码等",{"2":{"11":1}}],["编码因果注意力机制",{"2":{"115":1}}],["编码了整个输入序列的压缩表示",{"2":{"95":1}}],["编码注意力机制",{"0":{"93":1},"1":{"94":1,"95":1,"96":1,"97":1,"98":1,"99":1,"100":1,"101":1,"102":1,"103":1,"104":1,"105":1,"106":1,"107":1,"108":1,"109":1,"110":1,"111":1,"112":1,"113":1,"114":1,"115":1,"116":1,"117":1,"118":1,"119":1,"120":1,"121":1,"122":1,"123":1,"124":1,"125":1,"126":1,"127":1,"128":1,"129":1,"130":1,"131":1,"132":1,"133":1}}],["编码",{"2":{"67":1}}],["编码词元位置",{"0":{"48":1}}],["编码器将源语言的词序列作为输入",{"2":{"95":1}}],["编码器",{"2":{"95":3,"96":1}}],["编码器在每一步更新其隐藏状态",{"2":{"95":1}}],["编码器的任务是读取并处理整个文本",{"2":{"95":1}}],["编码器会将源语言文本编码为向量",{"2":{"11":1}}],["编码器模块处理输入文本并将其编码为一系列数值表示",{"2":{"11":1}}],["编码器处理输入文本并生成文本的嵌入表示",{"2":{"11":1}}],["编码器和解码器子模块的示意图",{"2":{"11":1}}],["编码器和解码器由所谓的",{"2":{"11":1}}],["编码器和解码器",{"2":{"11":2}}],["侧重于原始",{"2":{"11":1}}],["来防止过拟合",{"2":{"184":1}}],["来设置随机数生成器种子",{"2":{"157":1}}],["来定义的数学对象",{"2":{"147":1}}],["来生成文本",{"2":{"138":1}}],["来生成词元嵌入向量",{"2":{"46":1}}],["来对文本进行分类",{"2":{"138":1}}],["来对模型进行正则化",{"2":{"59":1}}],["来自所有头的上下文向量会被转置回形状",{"2":{"130":1}}],["来逐步实现自注意力机制",{"2":{"104":1}}],["来计算更高效的输入表示",{"2":{"96":1}}],["来实现多头注意力机制",{"2":{"129":1}}],["来实现",{"2":{"77":1}}],["来跨最后一个维度进行归一化",{"2":{"54":1}}],["来源",{"2":{"16":1}}],["来检测有害内容",{"2":{"11":1}}],["来建模数据中的复杂模式和抽象特征",{"2":{"8":1}}],["截至本文撰写时",{"2":{"53":1}}],["截至本书撰写时",{"2":{"11":1}}],["截图显示",{"2":{"9":1}}],["上获得的结果相似",{"2":{"165":1}}],["上运行",{"2":{"165":1}}],["上运行相对简单",{"2":{"164":1}}],["上执行加法",{"2":{"164":1}}],["上并在",{"2":{"164":1}}],["上调用",{"2":{"154":1}}],["上使用",{"0":{"144":1}}],["上述命令将自动安装支持",{"2":{"140":1}}],["上述嵌入层的方法实际上只是实现独热编码后通过全连接层进行矩阵乘法的更高效方式",{"2":{"47":1}}],["上加速计算的功能",{"2":{"137":1}}],["上",{"2":{"113":1,"164":2,"166":1}}],["上的运行时间进行比较",{"2":{"167":1}}],["上的训练循环",{"2":{"165":1}}],["上的",{"0":{"166":1},"2":{"76":1}}],["上的补充代码中查看相关实现",{"2":{"47":1}}],["上则需要",{"2":{"53":1}}],["上训练模型",{"2":{"135":1,"163":1}}],["上训练",{"2":{"53":1}}],["上表现出色",{"2":{"11":1}}],["上下文管理器",{"2":{"155":1}}],["上下文向量在自注意力中扮演着重要角色",{"2":{"99":1}}],["上下文向量可以被视为一个富含信息的嵌入向量",{"2":{"99":1}}],["上下文长度",{"2":{"53":1,"70":1}}],["上下文分析",{"2":{"12":1}}],["上下文和模式",{"2":{"7":1}}],["上下文相关的文本",{"2":{"7":1}}],["5e",{"2":{"190":1}}],["5b",{"2":{"185":1}}],["5节中提到的数据集感兴趣的读者",{"2":{"182":1}}],["5所示",{"2":{"143":1}}],["529",{"2":{"190":1}}],["5299",{"2":{"128":2}}],["5266",{"2":{"103":1}}],["5=2",{"2":{"121":1}}],["5=21",{"2":{"121":1}}],["5478",{"2":{"128":2}}],["5440",{"2":{"107":1}}],["541",{"2":{"29":1,"78":1}}],["5903",{"2":{"130":2}}],["5910",{"2":{"103":1}}],["5931",{"2":{"103":1}}],["5975",{"2":{"45":2,"48":1}}],["5到a",{"2":{"78":1}}],["5382",{"2":{"161":1}}],["5331",{"2":{"130":1}}],["5321",{"2":{"128":2}}],["536",{"2":{"60":1}}],["5307",{"2":{"53":1}}],["5675",{"2":{"128":2}}],["5671",{"2":{"103":1}}],["5645",{"2":{"103":1}}],["5647",{"2":{"54":1}}],["5660",{"2":{"60":1}}],["5683",{"2":{"99":1,"103":2}}],["568",{"2":{"45":2,"48":1}}],["5159",{"2":{"130":1}}],["5100",{"2":{"82":2}}],["5102",{"2":{"53":1}}],["51",{"2":{"80":1}}],["5181",{"2":{"60":1}}],["5173",{"2":{"54":1}}],["5198",{"2":{"54":1}}],["5145",{"2":{"42":1,"76":2}}],["559617757797241",{"2":{"180":1}}],["557",{"2":{"179":1}}],["5577",{"2":{"107":1}}],["5526",{"2":{"128":2}}],["5517",{"2":{"119":1,"121":1}}],["5510",{"2":{"103":1}}],["55",{"2":{"99":2}}],["558收敛到0",{"2":{"78":1}}],["5581",{"2":{"53":1}}],["5548",{"2":{"53":1}}],["554",{"2":{"37":1}}],["550",{"2":{"12":1}}],["5786",{"2":{"130":1}}],["5790",{"2":{"103":1}}],["5775",{"2":{"82":1}}],["57",{"2":{"34":1,"99":1}}],["570",{"2":{"13":1}}],["5874",{"2":{"128":2}}],["5872",{"2":{"54":1}}],["582",{"2":{"80":1}}],["5835",{"2":{"53":1}}],["5810",{"2":{"46":1,"47":1}}],["5891",{"2":{"45":2,"48":1,"128":2}}],["588",{"2":{"37":1,"71":2}}],["58",{"2":{"29":1,"99":1}}],["5000",{"2":{"157":4}}],["5077",{"2":{"128":2}}],["5042",{"2":{"72":1}}],["50257",{"2":{"48":1,"53":6,"60":4,"70":1,"71":5,"73":2,"89":2,"187":1}}],["50256",{"2":{"37":1,"38":1,"71":1}}],["502",{"2":{"45":2,"48":1}}],["50",{"2":{"26":1,"27":2,"38":1,"42":2,"46":1,"48":1,"60":4,"121":2,"155":4}}],["5",{"0":{"12":1,"37":1,"43":1,"59":1,"69":1,"70":1,"71":1,"75":1,"78":1,"79":1,"80":1,"82":1,"83":1,"86":1,"87":1,"88":2,"90":2,"91":1,"92":1,"115":1,"155":1,"186":1},"1":{"13":1,"38":1,"39":1,"40":1,"41":1,"42":1,"43":1,"44":1,"45":1,"70":1,"71":1,"72":2,"73":2,"74":2,"75":1,"76":2,"77":2,"80":1,"81":2,"82":1,"83":1,"84":2,"85":2,"87":1,"89":3,"90":3,"91":3,"116":1,"117":1,"118":1,"119":1,"120":1,"121":1,"122":1,"123":1,"124":1},"2":{"11":2,"14":1,"24":2,"29":2,"32":2,"34":1,"37":1,"43":1,"44":1,"46":3,"47":2,"48":1,"53":1,"54":8,"57":2,"58":5,"60":3,"67":3,"69":3,"70":3,"71":13,"72":7,"73":1,"75":1,"76":1,"77":2,"78":2,"80":3,"82":1,"88":2,"89":2,"96":2,"98":1,"104":1,"108":1,"110":1,"112":1,"115":1,"116":1,"121":4,"122":1,"123":1,"129":1,"143":1,"149":1,"151":5,"156":3,"157":1,"159":2,"161":1,"164":2,"165":1,"170":1,"180":7,"189":1,"190":3}}],["双向编码器表示",{"2":{"11":1}}],["该方法通过设置一个阈值",{"2":{"190":1}}],["该方法在训练过程中以余弦曲线的方式调节学习率",{"2":{"189":1}}],["该节开始处介绍的神经网络有多少参数",{"2":{"160":1}}],["该节解释了",{"2":{"43":1}}],["该工具能提升效率",{"2":{"158":1}}],["该层在禁用偏置项时可以高效执行矩阵乘法",{"2":{"111":1}}],["该增强的上下文向量",{"2":{"99":1}}],["该序列通常是文本",{"2":{"99":1}}],["该架构的自注意力机制受到了",{"2":{"96":1}}],["该机制对",{"2":{"96":1}}],["该机制允许模型权衡序列中不同词或标记的重要性",{"2":{"11":1}}],["该权重张量对应于gpt",{"2":{"89":1}}],["该函数进行分布式训练",{"2":{"170":1}}],["该函数使用llm一次生成一个token",{"2":{"79":1}}],["该函数计算模型预测输出",{"2":{"72":1}}],["该",{"2":{"71":1}}],["该偏移策略对于教模型预测序列中的下一个",{"2":{"71":1}}],["该过程重复进行",{"2":{"63":1}}],["该索引即为",{"2":{"63":1}}],["该输出层将",{"2":{"60":1}}],["该图结合了我们在本章中讨论的所有概念",{"2":{"60":1}}],["该类还实现了前向传播",{"2":{"59":1}}],["该模块能够并行地执行多个因果注意力机制",{"2":{"124":1}}],["该模块在",{"2":{"59":1}}],["该模型在1",{"2":{"182":1}}],["该模型启发了现代llm",{"2":{"182":1}}],["该模型训练了",{"2":{"76":1}}],["该模型会生成形状为",{"2":{"54":1}}],["该模型是",{"2":{"12":1}}],["该模型能够完成用户提供的未完句子的续写",{"2":{"10":1}}],["该操作包括减去均值并除以方差的平方根",{"2":{"54":1}}],["该加载器迭代输入数据集并以",{"2":{"42":1}}],["该子词在",{"2":{"41":1}}],["该词汇表基于整个训练集构建",{"2":{"27":1}}],["该词汇表定义了每个唯一词元到唯一整数值的映射",{"2":{"26":1}}],["该转换是将词元",{"2":{"26":1}}],["该文本可在",{"2":{"20":1}}],["该领域以前主要依赖于基于规则的系统和简单的统计方法",{"2":{"16":1}}],["该集合可能包含受版权保护的作品",{"2":{"13":1}}],["此节将通用化此计算",{"2":{"100":1}}],["此方法使我们能够专注于基础内容",{"2":{"88":1}}],["此方法用于训练",{"2":{"37":1}}],["此损失作为训练进展和成功的指示器",{"2":{"70":1}}],["此修改减少了训练模型的计算需求",{"2":{"70":1}}],["此组合信息形成的张量被传入多个",{"2":{"60":1}}],["此小型前馈神经网络内部的嵌入大小如何变化",{"2":{"57":1}}],["此处为gpt",{"2":{"179":1}}],["此处为",{"2":{"80":2}}],["此处为均值或方差",{"2":{"54":1}}],["此处未展示",{"2":{"11":1}}],["此前",{"2":{"48":1}}],["此图省略了词元化步骤",{"2":{"42":1}}],["此图特别展示了将原始数据转换为三维数值向量的过程",{"2":{"19":1}}],["此外",{"2":{"9":1,"10":1,"11":1,"13":1,"15":1,"19":1,"31":1,"36":1,"48":1,"57":2,"67":1,"68":1,"70":1,"75":1,"76":1,"80":1,"88":1,"89":1,"111":1,"129":1,"130":1,"150":2,"156":1,"158":1,"172":1,"174":1,"196":1}}],["可选的模型评估",{"2":{"165":1}}],["可更直观地了解模型在预测序列下一个",{"2":{"74":1}}],["可为我们完成图",{"2":{"72":1}}],["可工作的",{"2":{"60":1}}],["可能已对我从零编码的方式有所了解",{"2":{"196":1}}],["可能导致不期望的效果",{"2":{"161":1}}],["可能导致模型训练时产生显著的速度瓶颈",{"2":{"158":1}}],["可能无法带来明显的速度提升",{"2":{"158":1}}],["可能在等待数据加载时处于闲置状态",{"2":{"158":1}}],["可能是由于网络连接不稳定",{"2":{"89":1}}],["可能看起来过小而无法训练一个",{"2":{"76":1}}],["可能会有所更改",{"2":{"184":1}}],["可能会让人觉得未来标记",{"2":{"120":1}}],["可能会让您有很多关于输入如何预处理和编码的疑问",{"2":{"11":1}}],["可能会出现一些小的数值误差",{"2":{"54":1}}],["可能相对复杂",{"2":{"37":1}}],["可以确保梯度的范数不超过1",{"2":{"190":1}}],["可以确保输出张量保留与输入张量相同的形状",{"2":{"54":1}}],["可以降低模型在训练过程中遇到较大",{"2":{"188":1}}],["可以加速计算",{"2":{"171":1}}],["可以按以下方式从磁盘恢复它",{"2":{"162":1}}],["可以随意命名文件并选择文件后缀",{"2":{"162":1}}],["可以应用于测试集",{"2":{"161":1}}],["可以应用于训练集本身及任何新的文本样本",{"2":{"27":1}}],["可以实现一个",{"2":{"161":1}}],["可以手动与真实标签进行比较",{"2":{"161":1}}],["可以直接对",{"2":{"161":1}}],["可以用",{"2":{"161":2}}],["可以用于我们的玩具数据集",{"2":{"157":1}}],["可以用于从大量文本中高效检索知识",{"2":{"9":1}}],["可以用于机器翻译",{"2":{"9":1}}],["可以如下迭代访问",{"2":{"157":1}}],["可以显著减少训练时间",{"2":{"168":1}}],["可以显著加速深度学习相关的计算",{"2":{"143":1}}],["可以显式调用",{"2":{"155":1}}],["可以计算损失相对于模型参数",{"2":{"154":1}}],["可以高效地创建",{"2":{"147":1}}],["可以利用其加速",{"2":{"144":1}}],["可以选择",{"2":{"143":1}}],["可以选择一些云计算提供商",{"2":{"143":1}}],["可以选择将其移除",{"2":{"23":1}}],["可以自动计算张量操作的梯度",{"2":{"137":1}}],["可以从其三个主要组件来理解它",{"2":{"137":1}}],["可以跳过本附录",{"2":{"135":1}}],["可以像第5章的train",{"2":{"190":1}}],["可以像以前一样使用",{"2":{"124":1,"128":1}}],["可以像这样使用该类",{"2":{"110":1}}],["可以看到第二行",{"2":{"110":1}}],["可以看到相似词聚集在一起",{"2":{"19":1}}],["可以避免小梯度",{"2":{"108":1}}],["可以展示点积的计算过程",{"2":{"99":1}}],["可以逐词生成",{"2":{"94":1}}],["可以进一步改善文本生成效果",{"2":{"82":1}}],["可以参考以下论文",{"2":{"183":1}}],["可以参考andrej",{"2":{"183":1}}],["可以参考我书中",{"2":{"183":1}}],["可以参考官方文档",{"2":{"151":1}}],["可以参考训练一个相对流行的开源",{"2":{"76":1}}],["可以参考附录a中的a",{"2":{"78":1}}],["可以参考附录",{"2":{"46":1}}],["可以得出该模型的总大小为",{"2":{"61":1}}],["可以训练生成类似人类的文本",{"2":{"51":1}}],["可以添加特殊词元",{"2":{"49":1}}],["可以处理任何文本",{"2":{"39":1}}],["可以将第",{"2":{"165":1}}],["可以将该隐藏状态视为一个嵌入向量",{"2":{"95":1}}],["可以将不在词汇表中的单词分解为更小的子词单元或字符",{"2":{"38":1}}],["可以将句子或文档中的下一个词视为模型要预测的标签",{"2":{"14":1}}],["可以通过将字符串逐个输入编码器来获取各个token",{"2":{"177":1}}],["可以通过以下代码检查运行时是否支持",{"2":{"164":1}}],["可以通过以下代码验证",{"2":{"157":1}}],["可以通过设置随机种子来使随机数生成器可复现",{"2":{"155":1}}],["可以通过我的文章",{"2":{"148":1}}],["可以通过堆叠多个因果注意力模块实例来创建多头注意力模块",{"2":{"133":1}}],["可以通过并行处理所有头来改进这一实现",{"2":{"128":1}}],["可以通过矩阵乘法一步获得输出",{"2":{"109":1}}],["可以通过",{"2":{"37":1,"68":1}}],["可以使用以下代码将设备设置为",{"2":{"165":1}}],["可以使用模型进行预测",{"2":{"161":1}}],["可以使用张量的",{"2":{"150":1}}],["可以使用矩阵乘法实现相同效果",{"2":{"101":1}}],["可以使用torch",{"2":{"86":1}}],["可以使用来自",{"2":{"75":1}}],["可以使用相同的",{"2":{"65":1}}],["可以使用",{"2":{"20":1,"151":1,"155":1,"161":1}}],["可以根据输入解决多种任务",{"2":{"11":1}}],["可以在以下论文中找到详细的信息",{"2":{"184":1}}],["可以在我的网站上找到",{"2":{"172":1}}],["可以在传输命令中指定设备",{"2":{"164":1}}],["可以在",{"2":{"143":1}}],["可以在应用",{"2":{"121":1}}],["可以在附录",{"2":{"54":1}}],["可以在特定任务上优于通用",{"2":{"16":1}}],["可以在相对较小的数据集上进行微调",{"2":{"13":1}}],["可以在一个较小的标注数据集上进行微调",{"2":{"10":1}}],["可以在庞大的文本数据上进行训练",{"2":{"7":1}}],["机器学习和深度学习中的典型预测建模工作流程",{"2":{"138":1}}],["机器学习的引入和进步极大地增强了",{"2":{"138":1}}],["机器学习的核心理念是使计算机能够从数据中学习并进行预测或决策",{"2":{"138":1}}],["机器学习技术还驱动了诸如在线零售和流媒体服务中的推荐系统",{"2":{"138":1}}],["机器学习在",{"2":{"138":1}}],["机器学习则是",{"2":{"138":1}}],["机器学习是",{"2":{"138":1}}],["机器学习是核心领域",{"2":{"8":1}}],["机制的实现方式",{"2":{"96":1}}],["机制的比率",{"2":{"53":1}}],["机制连接",{"2":{"11":1}}],["efficient",{"2":{"184":1,"186":1}}],["effort",{"2":{"53":1,"70":2,"71":5,"78":6,"79":3,"80":8,"82":1,"83":3,"89":2,"190":4}}],["era",{"2":{"182":1}}],["error",{"2":{"57":1,"185":2}}],["ep",{"2":{"78":6,"190":5}}],["epoch+1",{"2":{"78":1,"159":1,"165":1,"170":1,"190":1}}],["epoch",{"2":{"78":1,"157":1,"159":10,"165":7,"170":2,"188":1,"189":1,"190":1}}],["epochs=n",{"2":{"190":1}}],["epochs=num",{"2":{"78":1}}],["epochs=1以再训练一个epoch",{"2":{"180":1}}],["epochs",{"2":{"78":11,"159":3,"165":3,"170":5,"188":3,"189":1,"190":5}}],["eps",{"2":{"54":3}}],["eps=1e",{"2":{"53":1}}],["evaluation",{"2":{"172":1}}],["evaluate",{"2":{"78":5,"190":2}}],["eval",{"2":{"63":2,"70":1,"78":9,"79":1,"86":2,"89":1,"159":1,"160":2,"161":2,"165":1,"170":1,"187":1,"190":6}}],["every",{"2":{"53":2,"70":2,"71":3,"78":6,"79":3,"80":8,"82":1,"83":3,"89":2,"190":4}}],["empty",{"2":{"99":1,"101":1}}],["embd",{"2":{"89":1}}],["embeds",{"2":{"53":4,"60":4}}],["embeddings",{"2":{"48":11,"99":1}}],["embeddingbackward0>",{"2":{"46":1,"47":1}}],["embedding",{"2":{"46":4,"47":1,"48":8,"53":2,"54":1,"60":4,"88":1,"89":2}}],["emb",{"2":{"53":12,"54":4,"55":1,"57":5,"59":4,"60":11,"70":1,"78":1,"89":8,"179":1,"187":1}}],["emergent",{"2":{"14":1}}],["established",{"2":{"42":4}}],["eos",{"2":{"36":2}}],["eli",{"2":{"172":1}}],["element",{"2":{"99":1}}],["elements",{"2":{"60":1}}],["eleuther",{"2":{"13":1}}],["else",{"2":{"33":1,"58":1,"77":3,"83":1,"165":1,"166":2,"187":2,"188":1,"189":1,"190":1}}],["extensions",{"2":{"192":1,"195":1}}],["extension",{"0":{"192":1},"1":{"193":1,"194":1,"195":1}}],["extend",{"2":{"32":1}}],["exists",{"2":{"187":1}}],["exact",{"2":{"184":1}}],["exampledeepneuralnetwork",{"2":{"58":3}}],["example",{"2":{"11":1,"54":2,"55":1,"121":2}}],["examples",{"0":{"0":1,"192":1},"1":{"1":1,"2":1,"3":1,"4":1,"5":1,"193":1,"194":1,"195":1},"2":{"161":3}}],["exquisitely",{"2":{"78":1,"79":1}}],["export",{"2":{"193":1}}],["expert",{"2":{"182":1}}],["expected",{"2":{"164":1}}],["exp",{"2":{"74":1,"99":2}}],["e",{"2":{"28":1,"41":1,"53":1,"63":1,"78":1,"129":3,"152":1,"155":1,"159":1,"170":2,"188":1,"190":1}}],["edith",{"2":{"20":1,"25":1,"26":2,"29":1,"75":1}}],["environ",{"2":{"170":2}}],["entropy",{"2":{"72":1,"73":5,"77":1,"152":1,"154":1,"159":1,"160":1,"165":1}}],["enc",{"2":{"42":9}}],["encoding",{"2":{"37":1,"44":1,"47":1,"53":1,"70":1,"79":1}}],["encoding=",{"2":{"21":1,"42":1,"44":1,"76":1,"187":2}}],["encoder",{"2":{"88":1,"183":1}}],["encoded",{"2":{"63":8,"70":4,"78":1}}],["encode",{"2":{"27":1,"28":3,"29":1,"30":1,"33":1,"34":1,"35":1,"37":2,"42":2,"43":1,"53":2,"63":1,"70":1,"76":1,"177":2}}],["enumerate",{"2":{"27":2,"32":2,"57":1,"77":1,"80":2,"99":3,"101":2,"157":2,"159":1,"161":1,"165":1}}],["enough",{"2":{"21":1}}],["en",{"2":{"13":1}}],["ein",{"2":{"11":1}}],["解码策略以控制随机性",{"0":{"79":1},"1":{"80":1,"81":1,"82":1,"83":1,"84":1,"85":1}}],["解码回文本",{"2":{"63":1}}],["解码器进行了修改",{"2":{"96":1}}],["解码器使用该隐藏状态生成输出",{"2":{"95":1}}],["解码器然后使用这个隐藏状态开始逐词生成翻译句子",{"2":{"95":1}}],["解码器",{"2":{"95":5}}],["解码器的总体概念",{"2":{"95":1}}],["解码器架构之一",{"2":{"95":1}}],["解码器网络",{"2":{"95":1}}],["解码器再将这些向量解码为目标语言文本",{"2":{"11":1}}],["解码器模块则使用这些编码向量生成输出文本",{"2":{"11":1}}],["解码器只需生成最后一个词",{"2":{"11":1}}],["解码器则使用这些表示逐词生成翻译文本",{"2":{"11":1}}],["解决的挑战以及",{"2":{"8":1}}],["包含详细的技术文章",{"2":{"191":1}}],["包含数百gb到tb的文本数据",{"2":{"186":1}}],["包含数十亿词汇的大型数据集对于预训练",{"2":{"16":1}}],["包含在重新缩放的softmax概率张量scaled",{"2":{"180":1}}],["包含三个核心组件",{"2":{"171":1}}],["包含3个元素的3维向量仍然是一个阶为1的张量",{"2":{"147":1}}],["包含六个嵌入向量",{"2":{"110":1}}],["包含多个步骤",{"2":{"78":1}}],["包含多头注意力机制",{"2":{"59":1}}],["包含了gpt",{"2":{"185":1}}],["包含了关于",{"2":{"99":1}}],["包含了我们希望模型生成的目标",{"2":{"71":1}}],["包含了来自整个输入序列的信息",{"2":{"59":1}}],["包含潜在的下一个",{"2":{"63":1}}],["包含两个张量",{"2":{"44":1}}],["包含两部分",{"2":{"11":1}}],["包含相应的预测目标",{"2":{"42":1}}],["包含作为目标的词元",{"2":{"42":1}}],["包含输入词元",{"2":{"42":1}}],["包含",{"2":{"14":1,"59":1}}],["包括如何创建简单的张量及一些基本操作",{"2":{"148":1}}],["包括预训练模型",{"2":{"137":1}}],["包括短篇故事书",{"2":{"88":1}}],["包括五个步骤",{"2":{"71":1}}],["包括权重和偏置项",{"2":{"68":1}}],["包括多头注意力和前馈层",{"2":{"59":1}}],["包括第",{"2":{"59":1}}],["包括标点符号",{"2":{"20":1}}],["包括将文本拆分为单词",{"2":{"19":1}}],["包括",{"2":{"19":1,"27":1,"52":1,"121":1,"138":1}}],["包括实现训练代码",{"2":{"67":1}}],["包括实现",{"2":{"15":1,"18":1}}],["包括语言的语法",{"2":{"12":1}}],["包括自然语言和计算机语言",{"2":{"12":1}}],["包括回答问题",{"2":{"9":1}}],["包括浏览文档",{"2":{"9":1}}],["包括理解语言",{"2":{"8":1}}],["提高到用户指定的最大值",{"2":{"188":1}}],["提高llm生成文本的原创性",{"2":{"78":1}}],["提出任何问题",{"2":{"196":1}}],["提出最初transformer架构的论文",{"2":{"182":1}}],["提出的",{"2":{"58":1}}],["提出的深度神经网络架构",{"2":{"11":1}}],["提到的讲座以了解更多",{"2":{"72":1}}],["提示",{"2":{"64":1,"113":1,"128":1}}],["提取和编辑不在训练数据中的文本",{"2":{"13":1}}],["提供模块化",{"2":{"137":1}}],["提供的预训练权重",{"2":{"60":1}}],["提供了更高级的工具来自动化这一过程",{"2":{"154":1}}],["提供了更强大的工具来理解和处理人类语言",{"2":{"7":1}}],["提供了多种语法选项来实现相同的计算",{"2":{"151":1}}],["提供了自动微分引擎",{"2":{"147":1}}],["提供了在不同操作系统上安装支持",{"2":{"141":1}}],["提供了创建和管理模型层的必要功能",{"2":{"110":1}}],["提供了一个简洁易懂的实现",{"2":{"183":1}}],["提供了一个心智模型",{"2":{"104":1}}],["提供了一个层归一化如何工作的直观概览",{"2":{"54":1}}],["提供了便捷的",{"2":{"72":1}}],["提供了本章涵盖主题的概览",{"2":{"69":1}}],["提供了前馈神经网络层之间连接的视觉概览",{"2":{"57":1}}],["提供了数据在",{"2":{"53":1}}],["提供大致框架以展示各组件如何组合成完整的",{"2":{"53":1}}],["提供输入数据格式",{"2":{"45":1}}],["提供额外上下文的特殊词元",{"2":{"30":1}}],["提供目标示例的方式被称为少样本设置",{"2":{"11":1}}],["ns",{"2":{"175":1}}],["nccl",{"2":{"170":2}}],["nvidia",{"2":{"140":1,"143":1}}],["nvalidation",{"2":{"77":1}}],["nanogpt",{"2":{"185":2}}],["nanogpt是一个代码仓库",{"2":{"185":1}}],["naive",{"2":{"99":5}}],["named",{"2":{"58":2}}],["name",{"2":{"58":3,"89":2,"170":1,"180":2}}],["nprocs=world",{"2":{"170":1}}],["np",{"2":{"89":3}}],["numpy",{"0":{"148":1},"2":{"89":1,"137":1,"147":1,"148":3,"151":1}}],["numel",{"2":{"60":3,"78":1,"155":1,"174":1,"179":2,"190":1}}],["num",{"0":{"158":1},"2":{"54":1,"59":1,"61":1,"63":1,"77":8,"78":7,"80":2,"83":1,"123":3,"127":6,"128":3,"129":16,"130":3,"131":1,"155":5,"157":3,"158":9,"159":5,"165":5,"170":7,"174":2,"178":3}}],["number",{"2":{"21":2,"60":4,"155":2,"170":1,"174":1,"179":6}}],["ninputs",{"2":{"48":1}}],["nnn",{"2":{"110":1}}],["nn",{"2":{"46":1,"48":2,"53":10,"54":6,"57":6,"58":13,"59":2,"60":6,"68":1,"73":1,"77":1,"89":1,"105":3,"110":7,"111":4,"112":5,"113":3,"121":1,"123":5,"127":2,"129":6,"152":1,"154":1,"155":9,"159":1,"170":1,"178":3,"184":2,"190":2}}],["ntargets",{"2":{"45":1}}],["n",{"2":{"45":2,"48":2,"53":5,"54":7,"55":7,"59":1,"60":3,"70":3,"71":1,"78":1,"79":1,"83":1,"89":14,"179":2,"187":2,"188":2,"189":1,"190":5}}],["notebook",{"2":{"158":3,"170":1}}],["notebook文件中加载模型和优化器",{"2":{"87":1}}],["not",{"2":{"83":1,"187":1,"190":1}}],["none",{"2":{"77":1,"83":1,"190":3}}],["noutput",{"2":{"60":1}}],["norm=1",{"2":{"190":4}}],["normalization",{"2":{"160":1,"185":4}}],["normalized",{"2":{"53":1,"54":1}}],["normandy",{"2":{"63":1}}],["norm2",{"2":{"59":2,"89":4}}],["norm1",{"2":{"59":2,"89":4}}],["norm",{"2":{"53":2,"54":6,"60":2,"89":4,"119":2,"190":3}}],["no",{"2":{"21":1,"63":1,"71":1,"78":2,"83":1,"155":3,"161":2}}],["net",{"2":{"185":1}}],["networks",{"2":{"184":1}}],["neural",{"2":{"183":2,"184":2,"186":2}}],["neuralnetwork",{"2":{"155":5,"159":1,"160":1,"162":3,"165":1,"170":1,"174":1}}],["neg",{"2":{"72":1}}],["new",{"2":{"63":3,"68":2,"70":1,"78":1,"79":1,"82":2,"83":3,"89":5}}],["news",{"2":{"13":1}}],["next",{"2":{"44":2,"45":1,"48":1,"63":2,"80":6,"82":1,"83":3}}],["need",{"2":{"11":1,"182":1,"184":1}}],["nlp",{"2":{"7":5,"15":1}}],["和超过1的温度适用于头脑风暴或创意内容生成",{"2":{"180":1}}],["和高效的数据加载器",{"2":{"159":1}}],["和语法来进行张量操作",{"2":{"148":1}}],["和转置",{"2":{"129":1}}],["和当前词",{"2":{"115":1}}],["和键",{"2":{"110":1,"133":1}}],["和输出",{"2":{"105":1}}],["和值",{"2":{"104":1}}],["和权重参数",{"2":{"88":1}}],["和模型预测分布",{"2":{"73":1}}],["和通过",{"2":{"71":1}}],["和文本输出",{"2":{"60":1}}],["和位置嵌入层",{"2":{"60":1}}],["和层归一化",{"2":{"60":1}}],["和捷径连接",{"2":{"59":1}}],["和前馈网络",{"2":{"59":1}}],["和其他深度学习模型的上下文中",{"2":{"68":1}}],["和其他",{"2":{"59":1}}],["和现代",{"2":{"54":1}}],["和总计",{"2":{"14":1}}],["和各种",{"2":{"11":1}}],["和部分已翻译句子",{"2":{"11":1}}],["和",{"0":{"45":1,"113":1,"142":1},"2":{"10":1,"11":2,"12":1,"13":1,"14":1,"16":1,"19":1,"27":1,"31":1,"32":1,"35":1,"36":1,"41":2,"42":2,"43":1,"45":2,"46":2,"49":2,"52":1,"53":2,"54":1,"55":1,"57":3,"58":1,"60":2,"62":1,"70":1,"71":1,"72":2,"73":1,"77":1,"80":2,"88":1,"89":4,"104":2,"109":1,"110":3,"112":1,"113":1,"127":1,"129":1,"135":1,"137":1,"138":1,"139":1,"140":1,"141":1,"151":1,"152":1,"156":1,"157":3,"160":2,"162":1,"164":1,"167":1}}],["指定",{"2":{"127":1}}],["指定两个注意力头",{"2":{"127":1}}],["指定随机种子",{"2":{"58":1}}],["指定了计算统计量",{"2":{"54":1}}],["指示",{"2":{"36":1}}],["指令数据集",{"2":{"14":1}}],["指令微调",{"2":{"10":1}}],["指模型处理的文本单元",{"2":{"12":1}}],["指的是包含多个隐藏层的人工神经元或节点",{"2":{"138":1}}],["指的是机器能够执行通常需要人类智能的任务",{"2":{"138":1}}],["指的是该机制通过关联单个输入序列中的不同位置来计算注意力权重的能力",{"2":{"98":1}}],["指的是模型的可训练权重",{"2":{"52":1}}],["指的是模型的初始训练阶段",{"2":{"10":1}}],["指的是未经标注的常规文本",{"2":{"10":1}}],["后扫描模型权重的",{"2":{"190":1}}],["后归一化",{"2":{"185":1}}],["后收敛",{"2":{"159":1}}],["后来根据用户需求添加了更接近",{"2":{"151":1}}],["后者会内部应用",{"2":{"160":1}}],["后者允许同时访问整个输入序列",{"2":{"115":1}}],["后者关注的是两个不同序列的元素之间的关系",{"2":{"98":1}}],["后者是一种超出本书范围的",{"2":{"96":1}}],["后者是训练集中记忆的一段话",{"2":{"83":1}}],["后续在2",{"2":{"147":1}}],["后续章节将详细说明",{"2":{"53":1}}],["后续进行了更正",{"2":{"52":1}}],["后深入探讨指令微调和分类微调的具体内容",{"2":{"10":1}}],["后",{"2":{"10":1,"71":1,"108":1,"121":2,"137":1,"143":1,"155":1,"159":1,"165":1}}],["中无法正常运行",{"2":{"170":1}}],["中保存和加载模型的方法",{"2":{"162":1}}],["中创建高效的数据加载器",{"2":{"156":1}}],["中进行计算",{"2":{"153":1}}],["中进一步说明",{"2":{"10":1}}],["中所有的张量操作",{"2":{"151":1}}],["中所示",{"2":{"121":1}}],["中所示的神经网络",{"2":{"58":1}}],["中所示的映射",{"2":{"40":1}}],["中矩阵相乘的常用方法是",{"2":{"151":1}}],["中更常用的重塑张量命令是",{"2":{"151":1}}],["中可用的不同张量数据类型的更多信息",{"2":{"150":1}}],["中可以通过以下代码检查",{"2":{"141":1}}],["中选择",{"2":{"143":1}}],["中运行以下代码检查安装是否识别了内置的",{"2":{"143":1}}],["中添加了一个输出投影层",{"2":{"130":1}}],["中我们通过输入向量的加权和计算上下文向量一样",{"2":{"109":1}}],["中我们将使用另一种更复杂的激活函数",{"2":{"54":1}}],["中展示了第",{"2":{"100":1}}],["中显示了自注意力的目标",{"2":{"99":1}}],["中显示的连贯文本完全不同",{"2":{"63":1}}],["中显示的生成文本",{"2":{"63":1}}],["中后",{"2":{"89":1}}],["中加载这些权重",{"2":{"88":1}}],["中加载和保存模型权重",{"0":{"86":1},"1":{"87":1}}],["中加入额外的位置信息是很有帮助的",{"2":{"48":1}}],["中了解更高级的技术",{"2":{"78":1}}],["中不确定生成哪个作为下一个",{"2":{"74":1}}],["中各步骤的结果一致",{"2":{"73":1}}],["中使用的",{"2":{"175":1}}],["中使用的权重初始化方案与",{"2":{"113":1}}],["中使用的自注意力机制",{"2":{"104":1}}],["中使用的自注意力",{"2":{"98":1}}],["中使用",{"2":{"73":1,"124":2,"144":1}}],["中仅显示了一个文本示例",{"2":{"71":1}}],["中仅展示二维嵌入的原因",{"2":{"19":1}}],["中详述",{"2":{"63":1}}],["中详细描述的下一个",{"2":{"63":1}}],["中指定的层数相等的",{"2":{"60":1}}],["中为",{"2":{"59":1}}],["中找到这些概念的简要介绍",{"2":{"54":1}}],["中描述的那样",{"2":{"52":1}}],["中实现神经网络时",{"2":{"155":1}}],["中实现神经网络的更多信息",{"2":{"57":1}}],["中实现一个完整的词元化类",{"2":{"27":1}}],["中实例化数据加载器",{"2":{"48":1}}],["中实例化一个嵌入层",{"2":{"46":1}}],["中被广泛应用",{"2":{"18":1}}],["中",{"2":{"12":1,"14":1,"42":1,"52":1,"57":2,"73":1,"95":1,"99":1,"104":1,"115":1,"121":1,"127":1,"129":2,"130":1,"133":1,"157":1,"158":1,"163":1,"164":2,"180":1}}],["中的代码",{"2":{"165":1}}],["中的神经网络训练",{"2":{"159":1}}],["中的注意力模块包含多个因果注意力实例",{"2":{"133":1}}],["中的输入向量",{"2":{"110":1}}],["中的应用",{"2":{"96":1,"135":1}}],["中的分割和加载数据",{"2":{"77":1}}],["中的所有",{"2":{"72":1}}],["中的第",{"2":{"71":3,"72":2}}],["中的架构",{"2":{"60":1}}],["中的便捷方法",{"2":{"58":1}}],["中的示例",{"2":{"54":1,"57":1}}],["中的嵌入层作为查找操作",{"2":{"49":1}}],["中的嵌入大小为",{"2":{"46":1}}],["中的词元以字符串格式显示",{"2":{"42":1}}],["中的数据集中",{"2":{"13":1}}],["中的",{"0":{"164":1},"2":{"10":1,"42":1,"43":2,"44":2,"46":1,"54":1,"106":1,"113":1,"158":1}}],["它通过对隐藏层中的神经元输入求和并进行归一化来稳定网络的隐藏状态动态",{"2":{"185":1}}],["它通过允许序列中每个位置与序列中所有其他位置交互并衡量其重要性",{"2":{"96":1}}],["它允许我们封装层和操作并跟踪模型的参数",{"2":{"155":1}}],["它可以简化代码",{"2":{"155":1}}],["它可以看作是一个单层神经网络",{"2":{"152":1}}],["它可以用来表示和可视化数学表达式",{"2":{"152":1}}],["它提供了限时的",{"2":{"143":1}}],["它开始于一个多头层",{"2":{"129":1}}],["它限制模型在处理任何给定的标记时",{"2":{"115":1}}],["它评估并学习输入内部各个部分",{"2":{"98":1}}],["它只有一组注意力权重顺序处理输入",{"2":{"125":1}}],["它只能依赖当前的隐藏状态来包含所有相关信息",{"2":{"95":1}}],["它只是将负输入截断为",{"2":{"54":1}}],["它是公开可用的",{"2":{"182":1}}],["它是一种在多个gpu上训练深度学习模型的流行方法",{"2":{"172":1}}],["它是一个将每一层映射到其参数的字典",{"2":{"86":1}}],["它是",{"2":{"94":1}}],["它是逐个token地生成的",{"2":{"92":1}}],["它是第六行而不是第五行",{"2":{"47":1}}],["它从未接近训练集损失",{"2":{"78":1}}],["它已能生成语法正确的文本",{"2":{"78":1}}],["它在每次模型更新后打印训练集和验证集的损失",{"2":{"78":1}}],["它在该神经网络子模块中起着至关重要的作用",{"2":{"57":1}}],["它迭代生成指定数量的新",{"2":{"63":1}}],["它被用于原始",{"2":{"60":1}}],["它计算损失梯度",{"2":{"58":1}}],["它也被称为跳跃连接",{"2":{"58":1}}],["它接受输入批次中的",{"2":{"57":1}}],["它将模型中的每一层映射到其可训练参数",{"2":{"162":1}}],["它将在下一节中介绍",{"2":{"54":1}}],["它将频繁出现的字符组合",{"2":{"41":1}}],["它包含一个或多个",{"2":{"52":1}}],["它使用滑动窗口方法从训练数据集中获取图",{"2":{"42":1}}],["它与用于实现和训练神经网络的数学运算不兼容",{"2":{"19":1}}],["它仅包含解码器部分",{"2":{"14":1}}],["它用于将英文翻译为德语或法语",{"2":{"11":1}}],["它还具备一定的少样本学习能力",{"2":{"10":1}}],["它们",{"2":{"121":1}}],["它们的作用是通过包含序列中其他元素的信息",{"2":{"99":1}}],["它们的应用前景几乎无穷无尽",{"2":{"9":1}}],["它们逐步构建",{"2":{"94":1}}],["它们都根据提供的配置字典",{"2":{"59":1}}],["它们在训练过程中对保持梯度流动起到了关键作用",{"2":{"58":1}}],["它们为深度学习模型提供了更好的性能",{"2":{"57":1}}],["它们是输入层的一部分",{"2":{"19":1}}],["它们基于该架构进行了不同任务的适应",{"2":{"11":1}}],["它们不仅可以解答用户问题",{"2":{"9":1}}],["它们也被视为一种生成式人工智能",{"2":{"8":1}}],["通常需要继承",{"2":{"155":1}}],["通常需要大量计算资源",{"2":{"56":1}}],["通常最好指定所需的",{"2":{"141":1}}],["通常应用于两个特定位置",{"2":{"121":1}}],["通常使用一个包含编码器和解码器两个子模块的深度神经网络",{"2":{"95":1}}],["通常情况下",{"2":{"78":1}}],["通常为标签的真实分布",{"2":{"73":1}}],["通常的做法不是将平均对数概率提升到",{"2":{"72":1}}],["通常实现一种计算上更便宜的近似形式",{"2":{"57":1}}],["通常称为",{"2":{"53":1}}],["通常称为基础或底层模型",{"2":{"10":1}}],["通常训练",{"2":{"44":1}}],["通常会在每个后续文本源前插入一个特殊词元",{"2":{"31":1}}],["通常会处理上百万篇文章和数十万本书",{"2":{"22":1}}],["通常会生成其自身的嵌入",{"2":{"19":1}}],["通常指模型隐藏状态的维度",{"2":{"19":1}}],["通过在每一步仅保留得分最高的部分序列来生成输出序列",{"2":{"186":1}}],["通过在通用和特定领域文本语料库上进行训练",{"2":{"186":1}}],["通过在训练数据集上最小化预测误差",{"2":{"8":1}}],["通过优化内存访问模式加速计算过程",{"2":{"184":1}}],["通过跟踪在张量上执行的每一个操作",{"2":{"154":1}}],["通过反向传播算法计算梯度",{"2":{"153":1}}],["通过将训练过程分配到多个机器",{"2":{"168":1}}],["通过将模型输出",{"2":{"152":1}}],["通过将一层的输出直接传递到更深的层",{"2":{"65":1}}],["通过一种学习算法",{"2":{"138":1}}],["通过对查询",{"2":{"129":1}}],["通过用这些归一化后的注意力得分对值进行加权生成上下文向量",{"2":{"110":1}}],["通过注意力权重组合所有的值向量来计算上下文向量",{"2":{"109":1}}],["通过嵌入维度平方根的缩放可以缓解这种情况",{"2":{"108":1}}],["通过嵌入维度大小进行归一化",{"2":{"108":1}}],["通过矩阵乘法同时计算所有注意力头的输出",{"2":{"128":1}}],["通过矩阵乘法计算所有的注意力得分",{"2":{"107":1}}],["通过矩阵乘法计算所有上下文向量",{"2":{"103":1}}],["通过矩阵乘法",{"2":{"104":1,"106":1}}],["通过输入",{"2":{"104":1}}],["通过以下代码示例",{"2":{"99":1}}],["通过查询",{"2":{"99":1}}],["通过使用",{"2":{"94":1}}],["通过使用概率采样和温度缩放",{"2":{"92":1}}],["通过使用下一词预测任务训练拥有数百万到数十亿参数的",{"2":{"18":1}}],["通过pytorch中的multinomial采样函数",{"2":{"80":1}}],["通过确保每一层的输出具有一致的均值和方差来稳定训练过程",{"2":{"65":1}}],["通过这种逐步的过程",{"2":{"63":1}}],["通过展示生成",{"2":{"63":1}}],["通过计算我们",{"2":{"61":1}}],["通过第二层将",{"2":{"57":1}}],["通过位置嵌入实现",{"2":{"53":1}}],["通过迭代地将高频字符合并为子词",{"2":{"41":1}}],["通过",{"2":{"36":1,"43":1,"54":1,"57":1,"71":1,"121":1}}],["通过预测目标词的上下文或相反方向来训练神经网络架构生成词嵌入",{"2":{"19":1}}],["通过预测句子中的下一个词作为",{"2":{"16":1}}],["通过预测下一个词",{"2":{"8":1}}],["通过逐词迭代生成文本",{"2":{"14":1}}],["通过不同维度捕捉多种因素的数值表示",{"2":{"11":1}}],["通过编码一步步构建一个基于",{"2":{"7":1}}],["更高效的因果注意力遮罩方法是",{"2":{"121":1}}],["更高效的遮罩方法",{"0":{"121":1}}],["更高的维度可能捕捉到更多细微关系",{"2":{"19":1}}],["更新的下载说明",{"0":{"89":1}}],["更新权重",{"2":{"78":1}}],["更改",{"2":{"128":1}}],["更改了",{"2":{"89":1}}],["更改代码",{"2":{"64":1}}],["更改为",{"2":{"54":1}}],["更短的路径",{"2":{"58":1}}],["更有效地处理和理解",{"2":{"31":1}}],["更有帮助",{"2":{"24":1}}],["更多信息可见",{"2":{"13":1}}],["更多详情可参考附录",{"2":{"10":1}}],["更小规模的数据集上进一步训练",{"2":{"10":1}}],["广泛的数据集上训练",{"2":{"10":1}}],["预测准确率为100",{"2":{"161":1}}],["预测步骤",{"2":{"152":1}}],["预测下一个",{"2":{"63":2}}],["预测循环的六次迭代过程",{"2":{"63":1}}],["预",{"2":{"10":1}}],["预训练中的标签可以从文本本身推导出来",{"2":{"138":1}}],["预训练llm的过程涉及调整模型权重以最小化训练损失",{"2":{"92":1}}],["预训练llm计算成本极高",{"2":{"86":1}}],["预训练阶段并非如此",{"2":{"16":1}}],["预训练数据集的摘要",{"2":{"12":1}}],["预训练和微调的代码实现",{"2":{"10":1}}],["预训练后的",{"2":{"10":1}}],["预训练",{"0":{"76":1},"2":{"10":1,"13":1,"15":1,"18":1}}],["以在前20个训练步骤中将学习率从0",{"2":{"188":1}}],["以在训练期间将更多的注意力权重置零",{"2":{"121":1}}],["以尽量降低资源需求",{"2":{"180":1}}],["以销毁给定的进程组并释放其资源",{"2":{"170":1}}],["以初始化分布式设置中的每个进程的进程组",{"2":{"170":1}}],["以将梯度重置为零",{"2":{"161":1}}],["以最小化损失",{"2":{"160":1}}],["以最小化特定损失函数",{"2":{"52":1}}],["以避免偏差",{"2":{"160":1}}],["以避免关注填充词元",{"2":{"36":1}}],["以丢弃最后一个批次",{"2":{"157":1}}],["以节省内存和计算资源",{"2":{"155":1}}],["以更新模型的权重",{"2":{"169":1}}],["以更简洁的方式实现相同功能",{"2":{"151":1}}],["以更高效地实现多头注意力机制",{"2":{"129":1}}],["以保证与本书兼容",{"2":{"141":1}}],["以保持代码简洁",{"2":{"53":1}}],["以预测其未知标签",{"2":{"138":1}}],["以实现ddp训练",{"2":{"170":1}}],["以实现自定义和优化",{"2":{"136":1}}],["以实现文本生成模型",{"2":{"94":1}}],["以防止",{"2":{"133":1}}],["以理解注意力机制的基本原理",{"2":{"104":1}}],["以计算正确预测的数量和比例",{"2":{"161":1}}],["以计算损失相对于网络中每个参数",{"2":{"153":1}}],["以计算所有上下文向量",{"2":{"100":1}}],["以计算给定数据加载器上所有批次的损失",{"2":{"77":1}}],["以同时计算所有上下文向量",{"2":{"99":1}}],["以论文的第一作者命名",{"2":{"96":1}}],["以获取",{"2":{"141":1}}],["以获取更多安装说明",{"2":{"141":1}}],["以获取替代的更新说明",{"2":{"89":1}}],["以获得最佳效果",{"2":{"158":1}}],["以获得所有其他的注意力权重",{"2":{"100":1}}],["以获得注意力权重",{"2":{"99":1,"108":1}}],["以获得对应的嵌入向量",{"2":{"46":1}}],["以获得更简洁的输出",{"2":{"23":1}}],["以获得语言的基本理解",{"2":{"10":1}}],["以增加输出的多样性",{"2":{"82":1}}],["以增强模型的理解能力",{"2":{"49":1}}],["以控制生成文本的随机性和多样性",{"2":{"79":1}}],["以下两篇论文详细介绍了预训练llm所用的数据集",{"2":{"186":1}}],["以下论文和资源提供了公开的大规模预训练数据集",{"2":{"186":1}}],["以下论文发现类似的技术可成功应用于已预训练的llm继续预训练",{"2":{"186":1}}],["以下论文深入探讨了如何使用字节对编码",{"2":{"183":1}}],["以下论文提供了instructgpt的参考文献",{"2":{"182":1}}],["以下论文介绍了eleuther",{"2":{"182":1}}],["以下是代码",{"2":{"190":1}}],["以下是代码清单",{"2":{"165":1}}],["以下是5",{"2":{"180":1}}],["以下是推荐的在",{"2":{"162":1}}],["以下是",{"2":{"151":1}}],["以下与",{"2":{"88":1}}],["以下小节将介绍两个概念",{"2":{"79":1}}],["以下代码清单a",{"2":{"170":1}}],["以下代码应作为脚本执行",{"2":{"170":1}}],["以下代码会报错",{"2":{"164":1}}],["以下代码用于创建此数据集",{"2":{"156":1}}],["以下代码实现了一个具有两个隐藏层的经典多层感知机",{"2":{"155":1}}],["以下代码实现了简单逻辑回归分类器的前向传播",{"2":{"152":1}}],["以下代码演示了如何将",{"2":{"150":1}}],["以下代码加载了第",{"2":{"76":1}}],["以下代码将使用",{"2":{"43":1,"88":1}}],["以改进该函数",{"2":{"79":1}}],["以降低llm对训练数据的记忆效应",{"2":{"78":1}}],["以减少",{"2":{"133":1}}],["以减少输出干扰",{"2":{"105":1}}],["以减少记忆效应",{"2":{"78":1}}],["以减少计算资源需求",{"2":{"77":1}}],["以基于上一章的代码生成文本",{"2":{"69":1}}],["以继续训练",{"2":{"67":1}}],["以评估",{"2":{"67":1}}],["以禁用训练期间使用的随机组件",{"2":{"63":1}}],["以稳定学习过程",{"2":{"60":1}}],["以组装一个完整的",{"2":{"60":1}}],["以整合来自整个输入序列的上下文信息",{"2":{"59":1}}],["以缓解梯度消失的问题",{"2":{"58":1}}],["以提高神经网络训练的稳定性和效率",{"2":{"54":1}}],["以取代上面的",{"2":{"53":1}}],["以符合现代",{"2":{"53":1}}],["以此类推",{"2":{"48":1,"169":1}}],["以传达其确切位置",{"2":{"48":1}}],["以编码词元在文本中的位置信息",{"2":{"47":1}}],["以确定损失的梯度",{"2":{"78":1}}],["以确定词汇表的大小",{"2":{"26":1}}],["以确保模型在实际应用之前达到我们的性能标准",{"2":{"138":1}}],["以确保模型在训练时可以看到更长的文本",{"2":{"76":1}}],["以确保文件保存正确且包含有效的",{"2":{"88":1}}],["以确保结果可重复",{"2":{"46":1}}],["以充分利用数据集",{"2":{"45":1}}],["以帮助",{"2":{"31":1}}],["以帮助模型理解文本中的上下文或其他相关信息",{"2":{"31":1}}],["以支持两个新词元",{"2":{"31":1}}],["以空白字符",{"2":{"23":1}}],["以便代码在无",{"2":{"165":1}}],["以便日后重用",{"2":{"162":1}}],["以便为您的系统定制和选择安装命令",{"2":{"141":1}}],["以便为后续",{"2":{"99":1}}],["以便为后续章节中的",{"2":{"48":1}}],["以便",{"2":{"122":1}}],["以便可以在第",{"2":{"104":1}}],["以便清晰且易于应用",{"2":{"71":1}}],["以便在计算上下文向量时",{"2":{"115":1}}],["以便在本节后续部分计算生成文本质量的损失",{"2":{"71":1}}],["以便在模型架构的各个",{"2":{"64":1}}],["以便在后续章节中将其转化为嵌入",{"2":{"21":1}}],["以便模型并不总是选择最有可能的",{"2":{"63":1}}],["以便复现初始权重",{"2":{"58":1}}],["以便了解如何在",{"2":{"58":1}}],["以便更清晰地展示和理解流程",{"2":{"110":1}}],["以便更新这些矩阵",{"2":{"105":1}}],["以便更容易地比较层间的梯度",{"2":{"58":1}}],["以便更高效地训练模型",{"2":{"51":1}}],["以便更直观地理解代码示例",{"2":{"44":1}}],["以便它能够处理未知单词",{"2":{"31":1}}],["以便深度学习架构能够理解和处理",{"2":{"19":1}}],["以",{"2":{"19":1}}],["以生成下一个token",{"2":{"82":1}}],["以生成下一章训练",{"2":{"18":1}}],["以生成更具原创性的文本",{"2":{"79":1}}],["以生成每个词汇的",{"2":{"60":1}}],["以生成词元",{"2":{"27":1}}],["以生成文本时提取相关信息的技术",{"2":{"19":1}}],["以生成模型输入",{"2":{"11":1}}],["以供",{"2":{"18":1}}],["以遵循指令或执行特定任务",{"2":{"18":1}}],["以执行指令或分类任务",{"2":{"16":1}}],["以创建基础模型",{"2":{"15":1}}],["以及深度学习实用函数",{"2":{"137":1}}],["以及相应的上下文向量",{"2":{"99":1}}],["以及对标注数据集的微调",{"2":{"94":1}}],["以及对基础模型进行微调",{"2":{"18":1}}],["以及如何加载openai的预训练权重",{"2":{"85":1}}],["以及在带标签的数据集上微调",{"2":{"67":1}}],["以及掩码多头注意力模块",{"2":{"52":1}}],["以及将词元",{"2":{"46":1}}],["以及将词元转化为嵌入向量",{"2":{"19":1}}],["以及",{"2":{"45":1}}],["以及最初用于",{"2":{"37":1}}],["以及用于未知单词的两个",{"2":{"34":1}}],["以及用于医疗问答的",{"2":{"10":1}}],["以及微调基础模型",{"2":{"15":1}}],["以及是否会在实际应用中被广泛采用",{"2":{"11":1}}],["以适应特定任务或领域",{"2":{"10":1}}],["以适应特定领域的数据集或任务",{"2":{"10":1}}],["针对特定任务或领域量身打造",{"2":{"10":1}}],["为从本书中获得最大收益",{"2":{"196":1}}],["为训练循环添加增强功能",{"0":{"187":1},"1":{"188":1,"189":1,"190":1}}],["为训练循环添加功能",{"2":{"78":1}}],["为什么pytorch没有自动为我们调用optimizer",{"2":{"172":1}}],["为什么我们需要这样做呢",{"2":{"168":1}}],["为什么要构建自己的",{"2":{"10":1}}],["为模拟批次输入",{"2":{"122":1}}],["为补偿活跃元素的减少",{"2":{"121":1}}],["为便于演示",{"2":{"121":1}}],["为实现更高效的遮罩",{"2":{"121":1}}],["为实现这一点",{"2":{"115":1}}],["为检查",{"2":{"113":1}}],["为简化起见",{"2":{"122":1}}],["为简洁起见",{"2":{"110":1}}],["为简单起见",{"2":{"71":1}}],["为何使用查询",{"2":{"109":1}}],["为所有输入标记计算注意力权重",{"0":{"100":1},"1":{"101":1,"102":1,"103":1}}],["为保持一致性",{"2":{"89":1}}],["为文本分类任务微调模型",{"2":{"88":1}}],["为每个模型权重存储额外的参数",{"2":{"86":1}}],["为后续章节中的微调提供一个良好的起点",{"2":{"67":1}}],["为",{"2":{"44":1,"45":1,"46":3,"47":1,"48":1,"63":1,"128":1}}],["为此建议设置",{"2":{"157":1}}],["为此",{"2":{"23":1,"27":1,"48":1,"89":1,"170":1}}],["为了通用化准确率的计算",{"2":{"161":1}}],["为了通过具体示例说明概率采样过程",{"2":{"80":1}}],["为了获得类别标签",{"2":{"161":1}}],["为了简洁起见",{"2":{"170":1}}],["为了简便",{"2":{"155":1}}],["为了简单起见",{"2":{"48":1}}],["为了便于说明",{"2":{"155":1}}],["为了便于展示",{"2":{"99":1}}],["为了给出具体示例",{"2":{"155":1}}],["为了明确安装",{"2":{"141":1}}],["为了防止模型在序列中访问未来信息",{"2":{"114":1}}],["为了解决无法逐词翻译的问题",{"2":{"95":1}}],["为了解决这个问题",{"2":{"49":1}}],["为了教育目的",{"2":{"88":1}}],["为了生成更多样化的文本",{"2":{"80":1}}],["为了计算训练集和验证集上的损失",{"2":{"75":1}}],["为了使本附录的代码独立运行",{"2":{"187":1}}],["为了使代码简洁易懂",{"2":{"78":1}}],["为了使",{"2":{"73":1}}],["为了方便起见",{"2":{"68":1}}],["为了更易理解",{"2":{"63":1}}],["为了更好地了解此",{"2":{"57":1}}],["为了理解这意味着什么",{"2":{"60":1}}],["为了提高可读性",{"2":{"54":1}}],["为了实现与单头注意力类似的输出维度为2",{"2":{"178":1}}],["为了实现因果注意力遮罩的步骤",{"2":{"115":1}}],["为了实现概率采样过程",{"2":{"80":1}}],["为了实现图",{"2":{"53":1}}],["为了实现高效的数据加载器",{"2":{"42":1}}],["为了加深对数据加载器工作原理的理解",{"2":{"45":1}}],["为了说明这一概念",{"2":{"99":1}}],["为了说明这一点",{"2":{"80":1,"169":1}}],["为了说明",{"2":{"44":1}}],["为了直观展示效果",{"2":{"42":1}}],["为了将词元映射到词元",{"2":{"26":1}}],["为了演示梯度裁剪过程",{"2":{"190":1}}],["为了演示",{"2":{"20":1,"190":1}}],["为了可视化",{"2":{"19":1}}],["为例",{"2":{"19":1,"155":1}}],["为大型语言模型训练准备文本",{"2":{"18":1}}],["为多种任务奠定了基础",{"2":{"14":1}}],["为更直观地了解",{"2":{"12":1}}],["为自然语言处理",{"2":{"7":1}}],["3需要665年",{"2":{"185":1}}],["3官方论文以及lambda",{"2":{"185":1}}],["3在架构上与gpt",{"2":{"185":1}}],["3的微调",{"2":{"182":1}}],["3和chatgpt不同",{"2":{"182":1}}],["3模型的论文",{"2":{"182":1}}],["3x2",{"2":{"151":1}}],["3所示的工作流程相似",{"2":{"138":1}}],["3所示",{"2":{"138":1}}],["380",{"2":{"179":1}}],["3800",{"2":{"119":1,"121":1}}],["3897",{"2":{"131":1}}],["3860",{"2":{"128":2}}],["3kib",{"2":{"88":1}}],["3节开始时通过generate",{"2":{"83":1}}],["3方法的结果是3个非零的概率分数",{"2":{"82":1}}],["3f",{"2":{"78":2,"190":2}}],["3113",{"2":{"155":1}}],["3190",{"2":{"131":1}}],["3103",{"2":{"119":1,"121":1}}],["314",{"2":{"63":2}}],["3178",{"2":{"60":1}}],["3168",{"2":{"60":1}}],["3×3",{"2":{"58":3}}],["3d",{"2":{"54":1,"106":1,"122":1,"124":1,"149":1}}],["3775",{"2":{"122":1}}],["372",{"2":{"78":1}}],["3796",{"2":{"72":1}}],["3717",{"2":{"53":1}}],["3737",{"2":{"130":1}}],["373",{"2":{"45":2,"48":1}}],["3565",{"2":{"130":1}}],["3567",{"2":{"53":1}}],["3589",{"2":{"128":2}}],["355m",{"2":{"89":2,"185":1}}],["355",{"2":{"53":1}}],["3331",{"2":{"122":2}}],["3327",{"2":{"122":1}}],["33",{"2":{"99":1}}],["33901",{"2":{"177":2}}],["339",{"2":{"71":1,"78":1}}],["3388",{"2":{"53":1}}],["3374",{"2":{"46":1}}],["32",{"2":{"61":1,"150":5,"151":4}}],["3208",{"2":{"130":1}}],["320",{"2":{"76":1}}],["3205",{"2":{"60":1}}],["3201",{"2":{"53":1}}],["3257",{"2":{"128":2}}],["3258541822433472",{"2":{"58":1}}],["3255",{"2":{"46":1}}],["3288",{"2":{"60":1}}],["32896995544433594",{"2":{"58":1}}],["3285",{"2":{"45":2,"48":1}}],["3297",{"2":{"54":1}}],["326",{"2":{"45":2,"48":1}}],["360",{"2":{"179":1}}],["3653",{"2":{"60":1}}],["3610",{"2":{"82":1}}],["3613",{"2":{"60":1}}],["3619",{"2":{"44":1,"45":2,"48":1,"177":4}}],["36",{"2":{"60":1,"62":1,"89":1}}],["3694",{"2":{"53":1}}],["3677",{"2":{"72":1}}],["367",{"2":{"44":4,"45":2,"48":1,"177":2}}],["3626",{"2":{"53":1,"60":1,"71":2}}],["362",{"2":{"34":1}}],["3493",{"2":{"128":2}}],["3428",{"2":{"128":2}}],["3474",{"2":{"101":4}}],["3470",{"2":{"54":1}}],["343",{"2":{"80":1,"177":1}}],["3418",{"2":{"60":1}}],["3483",{"2":{"60":1}}],["3469",{"2":{"53":1}}],["34680",{"2":{"37":1}}],["3408",{"2":{"121":1}}],["340",{"2":{"45":2,"48":1}}],["345",{"2":{"37":1,"52":1,"53":1,"60":1,"71":1}}],["3966",{"2":{"122":1}}],["391",{"2":{"78":1}}],["3934",{"2":{"155":1}}],["393",{"2":{"78":1}}],["3926",{"2":{"60":1}}],["39",{"2":{"29":1}}],["300",{"2":{"175":1}}],["3000",{"2":{"13":2,"157":2}}],["30×20+20",{"2":{"174":1}}],["30个输入单元",{"2":{"174":1}}],["30个偏置单元",{"2":{"174":1}}],["30个隐藏单元",{"2":{"174":1}}],["3084",{"2":{"122":1}}],["3097",{"2":{"119":1,"121":1}}],["30961",{"2":{"63":1}}],["3058",{"2":{"110":1}}],["3061",{"2":{"109":1,"110":2}}],["3072",{"2":{"57":2}}],["304",{"2":{"52":1}}],["3010",{"2":{"46":1,"47":1}}],["30",{"2":{"12":1,"25":2,"37":1,"155":3}}],["3",{"0":{"10":1,"13":1,"26":1,"28":1,"53":1,"57":1,"64":1,"75":1,"79":1,"80":1,"82":1,"83":2,"85":1,"95":1,"96":1,"97":2,"99":2,"100":2,"103":1,"104":1,"110":1,"112":1,"113":1,"115":1,"123":2,"125":1,"126":1,"127":1,"129":1,"132":2,"133":1,"139":1,"151":1,"152":1,"160":1,"168":1,"174":1,"179":1,"184":1,"190":1},"1":{"27":1,"28":1,"29":1,"30":1,"55":1,"56":1,"76":1,"77":1,"80":1,"81":2,"82":1,"83":1,"84":3,"85":3,"98":2,"99":2,"100":2,"101":4,"102":4,"103":4,"105":1,"106":1,"107":1,"108":1,"109":1,"110":1,"111":1,"112":1,"113":1,"114":1,"116":1,"117":1,"118":1,"119":1,"120":1,"121":1,"122":1,"123":1,"124":1,"126":1,"127":1,"128":1,"129":1,"130":2,"131":2,"132":2,"140":1,"141":1,"142":1,"143":1,"144":1,"154":1,"157":1,"158":1,"160":1,"161":1,"169":1,"170":1},"2":{"10":4,"12":3,"13":5,"14":6,"15":1,"16":2,"19":5,"32":1,"37":1,"42":1,"46":9,"47":2,"48":3,"49":1,"53":9,"54":1,"57":8,"58":11,"59":3,"65":1,"68":1,"70":2,"71":9,"72":3,"73":2,"78":2,"80":2,"82":2,"88":1,"89":2,"94":5,"95":6,"96":5,"98":2,"99":13,"100":3,"101":1,"103":1,"104":11,"105":1,"107":1,"108":2,"109":4,"110":2,"115":6,"121":6,"122":4,"124":1,"126":2,"127":2,"128":2,"129":2,"130":1,"138":2,"140":3,"149":4,"150":2,"151":9,"154":1,"155":3,"156":2,"157":3,"159":1,"161":2,"164":1,"165":1,"170":1,"178":4,"180":6,"185":2}}],["换句话说",{"2":{"9":1,"46":1,"54":1,"63":1,"80":2,"147":1}}],["摘要文本",{"2":{"9":1}}],["并尽量避免依赖现有的库",{"2":{"196":1}}],["并输出如下内容",{"2":{"190":1}}],["并查看这对最大梯度值的影响",{"2":{"190":1}}],["并进行llm的预训练",{"2":{"187":1}}],["并进一步扩展为增强型自注意力机制",{"2":{"94":1}}],["并提供了额外的提示和见解",{"2":{"186":1}}],["并提出了最初的",{"2":{"96":1}}],["并处理其对应的数据子集",{"2":{"169":1}}],["并处理不同的上下文",{"2":{"49":1}}],["并传递给每个模型副本",{"2":{"169":1}}],["并传入一个包含",{"2":{"57":1}}],["并同时处理这些数据子集",{"2":{"169":1}}],["并重新构建包含模型参数的",{"2":{"162":1}}],["并不是必须的",{"2":{"155":1}}],["并不是特别有趣",{"2":{"88":1}}],["并随着数据和反馈的增加逐步提高性能",{"2":{"138":1}}],["并结合代码示例进行实际操作",{"2":{"137":1}}],["并增加了在",{"2":{"137":1}}],["并从零开始实现大语言模型",{"2":{"135":1}}],["并从词汇表中选择对应于最高概率分数的token",{"2":{"92":1}}],["并对结果矩阵进行归一化",{"2":{"115":1}}],["并对未遮罩的注意力权重进行归一化",{"2":{"115":1}}],["并对模型进行了多轮训练",{"2":{"78":1}}],["并对模型进行预训练",{"2":{"67":1}}],["并学习如何从头开始编写它",{"2":{"97":1}}],["并学习正确的文本生成格式",{"2":{"23":1}}],["并未精确展示",{"2":{"96":1}}],["并未特别针对翻译",{"2":{"14":1}}],["并微调它以用于文本分类和遵循指令",{"2":{"89":1}}],["并微调其功能",{"2":{"15":1}}],["并使其遵循类似",{"2":{"88":1}}],["并使用模型进行文本生成",{"2":{"88":1}}],["并使用train",{"2":{"87":1}}],["并使用这些梯度来更新模型权重",{"2":{"78":1}}],["并使用",{"2":{"63":1,"110":1}}],["并使用我们在本章开头创建的批次文本输入进行输入",{"2":{"60":1}}],["并使用第",{"2":{"53":1,"76":1}}],["并生成以下的下一个token的logits值",{"2":{"80":1}}],["并生成相应的",{"2":{"70":1}}],["并确保输入和输出的嵌入维度与",{"2":{"132":1}}],["并确保在计算损失时模型处于评估模式",{"2":{"78":1}}],["并确保训练过程的一致性和可靠性",{"2":{"54":1}}],["并应用",{"2":{"71":1}}],["并由分词器解码为人类可读的文本",{"2":{"70":1}}],["并引入两个便捷函数",{"2":{"70":1}}],["并加载预训练权重",{"2":{"70":1}}],["并简要回顾第",{"2":{"70":1}}],["并实现一些基本的模型评估技术",{"2":{"69":1}}],["并编写了",{"2":{"67":1}}],["并编写每个",{"2":{"15":1}}],["并初始化了一个具有随机权重的",{"2":{"63":1}}],["并以",{"2":{"63":1}}],["并基于最高概率的预测值选择下一个",{"2":{"63":1}}],["并通过",{"2":{"63":1}}],["并看到它输出形状为",{"2":{"61":1}}],["并看看它的表现如何",{"2":{"58":1}}],["并假设每个参数是",{"2":{"61":1}}],["并计算负平均对数概率",{"2":{"73":1}}],["并计算",{"2":{"60":1}}],["并添加到原始输入文本中",{"2":{"63":1}}],["并添加位置信息",{"2":{"60":1}}],["并添加相同大小的位置嵌入",{"2":{"48":1}}],["并改进深度模型的学习能力",{"2":{"59":1}}],["并没有缩小到接近零的极小值",{"2":{"58":1}}],["并返回",{"2":{"58":1}}],["并与",{"2":{"57":1}}],["并且训练数据量更多",{"2":{"185":1}}],["并且你对微积分的概念感到不太理解",{"2":{"154":1}}],["并且当前",{"2":{"140":1}}],["并且在the",{"2":{"180":1}}],["并且在大多数情况下会被multinomial选中",{"2":{"80":1}}],["并且在更多数据上进行训练",{"2":{"53":1}}],["并且还会添加位置嵌入",{"2":{"60":1}}],["并且如果",{"2":{"58":1}}],["并且它反映了实现原始",{"2":{"55":1}}],["并且均值为",{"2":{"54":1}}],["并且使得网络难以有效地调整其权重",{"2":{"54":1}}],["并且我们希望创建大小为",{"2":{"46":1}}],["并将此更复杂的训练函数的结果与第5章的train",{"2":{"190":1}}],["并将作为本书中从头实现llm的模板",{"2":{"182":1}}],["并将温度设置在1以下时",{"2":{"180":1}}],["并将大型层分配到不同的gpu上",{"2":{"172":1}}],["并将一个函数应用到多个输入上以并行执行",{"2":{"170":1}}],["并将命令中的",{"2":{"141":1}}],["并将生成的文本与本章加载的124m模型进行比较",{"2":{"91":1}}],["并将right张量返回为可训练的pytorch参数",{"2":{"89":1}}],["并将它们存储在每个模型权重",{"2":{"190":1}}],["并将它们的概率分数设为无穷小",{"2":{"82":1}}],["并将它们组装成一个类似",{"2":{"51":1}}],["并将输出张量解码为人类可读文本来实现",{"2":{"65":1}}],["并将该",{"2":{"63":1}}],["并将这些",{"2":{"63":1}}],["并将其存储在张量的",{"2":{"154":1}}],["并将其编码为向量表示",{"2":{"94":1}}],["并将其输入到llm中以生成一个文本样本",{"2":{"78":1}}],["并将其附加到输入中",{"2":{"63":1}}],["并将其附加到上下文中的过程如图",{"2":{"63":1}}],["并将其添加到输入上下文中",{"2":{"63":1}}],["并将其应用于示例输入",{"2":{"55":1}}],["并将其应用于两个输入示例",{"2":{"54":1}}],["并将随机种子设置为",{"2":{"46":1}}],["并用于填充",{"2":{"36":1}}],["并用于教育目的的预训练过程",{"2":{"13":1}}],["并讨论用于在训练中为",{"2":{"30":1}}],["并试用它来词元化",{"2":{"29":1}}],["并能在常规硬件上合理运行",{"2":{"22":1}}],["并创建词元嵌入",{"2":{"20":1}}],["并非所有",{"2":{"11":1}}],["并非指",{"2":{"7":1}}],["并在剩余训练中保持恒定",{"2":{"188":1}}],["并在某些情况下提高性能",{"2":{"185":1}}],["并在反向传播阶段计算不同的梯度",{"2":{"169":1}}],["并在",{"2":{"155":1}}],["并在加上偏置后通过激活函数",{"2":{"152":1}}],["并在每一步更新其隐藏状态",{"2":{"95":1}}],["并在有疑问时访问",{"2":{"89":1}}],["并在应用softmax函数之前将所有其他token的值设为负无穷",{"2":{"82":1}}],["并在第10轮后保持在6",{"2":{"78":1}}],["并在训练过程中逐渐下降",{"2":{"78":1}}],["并在训练过程中不断更新",{"2":{"19":1}}],["并在接下来的小节中计算训练和验证集的损失",{"2":{"69":1}}],["并在本节中讨论评估生成文本质量的基本方法",{"2":{"69":1}}],["并在本书中从零实现它",{"2":{"11":1}}],["并在它们之后应用了",{"2":{"59":1}}],["并在模型训练期间被优化",{"2":{"49":1}}],["并在特定任务上提高性能",{"2":{"13":1}}],["并在预训练出基础",{"2":{"10":1}}],["并逐步构建一个能够生成文本的",{"2":{"9":1}}],["有兴趣了解其他流行llm所使用的分词方案的读者",{"2":{"183":1}}],["有兴趣的读者可以尝试使用project",{"2":{"78":1}}],["有兴趣的读者可以在附录d",{"2":{"78":1}}],["有兴趣的读者可以在本章结尾的延伸阅读部分找到相关文献参考",{"2":{"11":1}}],["有以下几种方法",{"2":{"180":1}}],["有一个可用于加速",{"2":{"144":1}}],["有两个版本",{"2":{"140":1}}],["有效地恢复了保存时模型的学习状态",{"2":{"162":1}}],["有效地",{"2":{"121":1}}],["有助于更稳定和有效的模型训练",{"2":{"111":1}}],["有助于模型在不同类型输入上更好地泛化",{"2":{"77":1}}],["有趣的是",{"2":{"96":1}}],["有哪些不同的设置组合可以使generate函数表现出确定性行为",{"2":{"85":1}}],["有时会生成语法不正确或完全无意义的内容",{"2":{"82":1}}],["有时几乎涵盖了互联网上所有公开的文本内容",{"2":{"8":1}}],["有关在",{"2":{"57":1}}],["有偏方差",{"0":{"55":1}}],["有些单词仍附带标点符号",{"2":{"23":1}}],["有些数据集可能被多次纳入以达到总词元数",{"2":{"13":1}}],["有望重新定义我们与技术的关系",{"2":{"9":1}}],["简化了反向传播和模型优化",{"2":{"137":1}}],["简化了架构",{"2":{"16":1}}],["简介",{"0":{"134":1},"1":{"135":1,"136":1,"137":1,"138":1,"139":1,"140":1,"141":1,"142":1,"143":1,"144":1,"145":1,"146":1,"147":1,"148":1,"149":1,"150":1,"151":1,"152":1,"153":1,"154":1,"155":1,"156":1,"157":1,"158":1,"159":1,"160":1,"161":1,"162":1,"163":1,"164":1,"165":1,"166":1,"167":1,"168":1,"169":1,"170":1,"171":1,"172":1,"173":1,"174":1,"175":1}}],["简要介绍过编码器",{"2":{"95":1}}],["简而言之",{"2":{"9":1,"58":1,"106":1,"136":1}}],["简称生成式",{"2":{"8":1}}],["回答技术性问题等",{"2":{"9":1}}],["回到垃圾邮件分类的例子",{"2":{"8":1}}],["法律等专业领域",{"2":{"9":1}}],["galore",{"2":{"186":3}}],["galore是一个近期的研究项目",{"2":{"186":1}}],["gao等人",{"2":{"182":1,"186":1}}],["gaussian",{"2":{"57":1,"185":2}}],["gu和dao",{"2":{"182":1}}],["gutenberg的60",{"2":{"78":1,"186":1}}],["gutenberg",{"2":{"13":2,"75":1,"186":2}}],["gotorch",{"2":{"150":3}}],["gone",{"2":{"83":1}}],["good",{"2":{"21":1}}],["google",{"2":{"9":1,"143":3}}],["g",{"2":{"78":1,"89":4,"129":1,"152":1,"170":2,"190":1}}],["global",{"2":{"78":4,"188":4,"189":5,"190":8}}],["groeneveld等人",{"2":{"186":1}}],["groups",{"2":{"188":2,"189":2,"190":2}}],["group应在训练脚本结束时调用",{"2":{"170":1}}],["group函数应在训练脚本开始时调用",{"2":{"170":1}}],["group用于初始化和退出分布式训练模式",{"2":{"170":1}}],["group和destroy",{"2":{"170":1}}],["group",{"2":{"170":5,"188":2,"189":2,"190":2}}],["graphqltensor",{"2":{"161":1}}],["graphqltrue",{"2":{"143":1,"164":1}}],["graph=true",{"2":{"154":2}}],["grad属性",{"2":{"190":1}}],["grad属性中",{"2":{"190":1}}],["grad=false",{"2":{"105":4}}],["grad=true",{"2":{"46":1,"105":1,"154":2}}],["gradient函数",{"2":{"190":1}}],["gradient",{"2":{"58":11,"172":1,"186":1,"190":3}}],["gradients",{"2":{"58":5}}],["grad",{"2":{"46":1,"47":1,"53":1,"54":8,"55":2,"58":1,"60":1,"63":1,"71":1,"78":3,"83":1,"110":1,"112":1,"116":1,"118":1,"119":1,"121":2,"122":1,"128":1,"153":1,"154":12,"155":6,"159":1,"161":3,"165":1,"172":2,"174":1,"188":1,"189":1,"190":16}}],["grid",{"2":{"57":1}}],["gpu上",{"2":{"175":1}}],["gpu时也观察到显著的加速效果",{"2":{"175":1}}],["gpus",{"2":{"170":1}}],["gpu",{"0":{"141":1,"143":1,"163":1,"164":1,"165":1},"1":{"164":1,"165":1,"166":2,"167":2,"168":1,"169":1,"170":1},"2":{"53":3,"75":1,"76":1,"124":1,"135":2,"137":2,"139":1,"140":5,"141":1,"143":10,"150":1,"158":1,"163":4,"164":10,"165":5,"167":1,"170":3,"172":1}}],["gpts",{"2":{"185":1}}],["gpt应用于实践",{"2":{"89":1}}],["gpt函数花费了大量时间",{"2":{"89":1}}],["gpt函数中",{"2":{"89":1}}],["gpt函数",{"2":{"89":1}}],["gptmodel实例会初始化用于预训练的随机权重",{"2":{"89":1}}],["gptmodel的代码",{"2":{"78":1}}],["gptmodel",{"2":{"60":5,"61":3,"62":1,"63":1,"64":1,"65":1,"68":1,"70":4,"71":1,"76":1,"78":1,"86":2,"88":1,"89":2,"179":1,"180":1,"187":2,"190":2}}],["gptdatasetv1",{"2":{"43":3,"44":2}}],["gpt2",{"2":{"37":1,"44":1,"53":1,"60":2,"70":1,"79":1,"88":4,"89":7,"180":6}}],["gpt3",{"2":{"16":1}}],["gpt",{"0":{"13":1,"14":1,"50":1,"53":2,"60":1,"62":1,"70":1,"132":1},"1":{"51":1,"52":1,"53":1,"54":1,"55":3,"56":3,"57":1,"58":1,"59":1,"60":1,"61":2,"62":2,"63":1,"64":1,"65":1},"2":{"10":1,"11":8,"12":3,"13":4,"14":19,"15":3,"16":3,"18":2,"19":3,"31":1,"36":2,"37":2,"46":2,"48":3,"49":3,"51":4,"52":7,"53":28,"54":4,"55":5,"57":5,"58":2,"59":9,"60":20,"62":6,"63":11,"64":1,"65":6,"70":7,"71":2,"78":1,"86":2,"88":9,"89":57,"91":1,"96":3,"104":1,"105":1,"115":1,"121":2,"132":1,"179":6,"180":3,"183":1,"185":3,"186":1,"187":2,"190":2}}],["gis",{"2":{"78":1}}],["gisburn",{"2":{"21":1,"25":1,"29":2}}],["githubusercontent",{"2":{"88":1,"187":1}}],["github",{"2":{"37":1,"47":1,"89":1,"142":2,"170":1,"176":1,"183":2,"185":1,"186":3}}],["gt",{"2":{"31":7,"32":2,"33":1,"34":1,"36":5,"38":3,"39":1,"42":1,"49":2,"143":1}}],["gb",{"2":{"13":3,"22":1}}],["gelus",{"2":{"185":1}}],["gelu",{"0":{"57":1},"2":{"55":1,"57":24,"58":7,"59":2,"65":1,"185":1}}],["getitem",{"2":{"43":1,"157":4}}],["get",{"2":{"37":1,"44":1,"53":1,"70":1,"79":1}}],["genius",{"2":{"21":1,"25":1}}],["generation",{"2":{"186":1}}],["generative",{"2":{"14":2,"52":1}}],["generated",{"2":{"184":2}}],["generate",{"2":{"63":6,"70":3,"71":3,"78":5,"79":1,"83":2,"89":1,"190":2}}],["genesis",{"2":{"13":1}}],["genai",{"2":{"8":1}}],["gemini",{"2":{"9":1}}],["接收到的文本",{"2":{"42":1}}],["接下来余弦衰减生效",{"2":{"189":1}}],["接下来使用",{"2":{"157":1}}],["接下来两节将介绍如何设置高效的数据加载器并训练模型",{"2":{"155":1}}],["接下来是",{"2":{"152":1}}],["接下来是第",{"2":{"72":1}}],["接下来一节",{"2":{"77":1}}],["接下来的小节将介绍",{"2":{"148":1}}],["接下来的小节将介绍如何计算生成输出的损失指标",{"2":{"70":1}}],["接下来的章节将介绍准备",{"2":{"19":1}}],["接下来的章节将介绍",{"2":{"8":1}}],["接下来我们的问题是",{"2":{"63":1}}],["接下来我们将通过代码示例计算它",{"2":{"58":1}}],["接下来将介绍训练函数的增强功能",{"2":{"187":1}}],["接下来将把这一过程封装到一个",{"2":{"54":1}}],["接下来将从具体的",{"2":{"53":1}}],["接下来让我们对先前得到的层输出应用层归一化",{"2":{"54":1}}],["接下来",{"2":{"23":1,"26":1,"27":1,"32":1,"42":1,"53":1,"55":1,"57":1,"58":1,"60":1,"71":1,"72":1,"76":1,"77":1,"79":1,"89":1,"97":1,"105":1,"119":1,"122":1,"135":1,"151":1,"154":1,"155":1,"157":1,"161":1}}],["接着",{"2":{"15":1,"16":1,"49":1,"70":1,"104":1}}],["接口实现用户与",{"2":{"9":1}}],["文件名",{"2":{"162":1}}],["文件对象",{"2":{"157":1}}],["文件中导入",{"2":{"88":1}}],["文档分类",{"2":{"11":1}}],["文本到",{"2":{"70":1}}],["文本生成能力",{"2":{"65":1}}],["文本",{"2":{"60":2}}],["文本数据量高达数",{"2":{"22":1}}],["文本样本大小",{"0":{"22":1}}],["文本处理步骤",{"2":{"20":1}}],["文本词元化",{"0":{"20":1},"1":{"21":1,"22":1,"23":1,"24":1,"25":1}}],["文本摘要",{"2":{"11":1}}],["文本摘要等众多任务",{"2":{"9":1}}],["文本翻译等",{"2":{"9":1}}],["文章",{"2":{"9":1}}],["还需要初始化第5章中使用的数据加载器",{"2":{"187":1}}],["还需要一种方法将词元",{"2":{"27":1}}],["还有2个隐藏层",{"2":{"174":1}}],["还有一项任务要完成",{"2":{"42":1}}],["还要进行深度学习模型的处理",{"2":{"158":1}}],["还创建了一个包含两个样本的测试集",{"2":{"156":1}}],["还可以使用",{"2":{"151":1}}],["还可以生成上下文化的输出嵌入",{"2":{"19":1}}],["还分享了更大模型的权重",{"2":{"89":1}}],["还将介绍其他采样技术",{"2":{"63":1}}],["还将讨论并实现数据预处理步骤",{"2":{"11":1}}],["还提高了模型处理复杂数据模式的整体能力",{"2":{"59":1}}],["还使用了其他几种激活函数",{"2":{"57":1}}],["还能完成拼写纠正",{"2":{"14":1}}],["还能强化",{"2":{"9":1}}],["还能为智能聊天机器人和虚拟助手提供支持",{"2":{"9":1}}],["还被用于内容创作",{"2":{"9":1}}],["还指模型所训练的数据量",{"2":{"8":1}}],["由多个单头注意力模块堆叠而成",{"2":{"126":1}}],["由",{"2":{"14":1,"127":1}}],["由于数据集很小并多次迭代",{"2":{"190":1}}],["由于每个模型副本将看到不同的训练数据样本",{"2":{"169":1}}],["由于训练集较小",{"2":{"161":1}}],["由于我们使用了",{"2":{"157":1}}],["由于我们使用的是预训练权重",{"2":{"89":1}}],["由于有",{"2":{"127":1}}],["由于自注意力可能显得较为复杂",{"2":{"98":1}}],["由于仅将",{"2":{"77":1}}],["由于其简单性和在各种神经网络架构中的有效性",{"2":{"57":1}}],["由于实现",{"2":{"37":1}}],["由于本书目标是训练类似",{"2":{"19":1}}],["由于文本是类别型的",{"2":{"19":1}}],["由于下一词预测任务可以",{"2":{"14":1}}],["由于",{"2":{"9":1,"44":1,"47":1,"48":1,"63":1,"112":1,"139":1}}],["由专家或用户标注",{"2":{"8":1}}],["2最大的模型大100倍",{"2":{"185":1}}],["2基本相同",{"2":{"185":1}}],["2论文介绍了一系列不同规模的基于transformer的llm",{"2":{"185":1}}],["2及更先进的llm中被应用于这些组件之前",{"2":{"185":1}}],["2训练后生成的新数据集",{"2":{"180":1}}],["2的字节对编码分词器代码",{"2":{"183":1}}],["2的训练数据集",{"2":{"180":1}}],["2的预训练数据集",{"2":{"180":1}}],["2模型的简洁且高效的实现",{"2":{"185":1}}],["2模型的初始化如下",{"2":{"178":1}}],["2模型",{"2":{"180":1}}],["2模型是用1024",{"2":{"89":1}}],["2节",{"2":{"175":1}}],["2节所述",{"2":{"79":1}}],["2个偏置单元",{"2":{"174":1}}],["2个输出节点",{"2":{"174":1}}],["2个输入",{"2":{"174":1}}],["2所示",{"2":{"138":1,"189":1}}],["2×30+30",{"2":{"174":1}}],["2×2=42",{"2":{"127":1}}],["2×1",{"2":{"54":1}}],["2nd",{"2":{"103":2}}],["2306",{"2":{"186":1}}],["2304",{"2":{"185":1,"186":1}}],["2307",{"2":{"182":1,"184":1}}],["2302",{"2":{"182":1}}],["2305",{"2":{"182":2}}],["2303",{"2":{"182":1,"186":1}}],["2311",{"2":{"184":1}}],["2312",{"2":{"182":1}}],["2318",{"2":{"130":1}}],["2319",{"2":{"119":1,"121":1}}],["23",{"2":{"124":1}}],["2326",{"2":{"102":1}}],["2369",{"2":{"102":1}}],["2333",{"2":{"102":1}}],["2379",{"2":{"102":1}}],["2394",{"2":{"54":1}}],["2392",{"2":{"53":1}}],["2390",{"2":{"53":1}}],["2t",{"2":{"99":1}}],["2952",{"2":{"155":1}}],["2943",{"2":{"131":1}}],["2948",{"2":{"110":1}}],["2990",{"2":{"110":1}}],["2996",{"2":{"110":1}}],["2927",{"2":{"110":1}}],["2935",{"2":{"101":4}}],["29669",{"2":{"71":1}}],["290",{"2":{"42":5}}],["2f",{"2":{"61":1,"159":1,"165":1,"170":1}}],["2d对应阶数2",{"2":{"147":1}}],["2d",{"2":{"54":1,"106":1,"124":1}}],["26",{"2":{"129":1}}],["2642",{"2":{"121":1}}],["2630",{"2":{"60":1}}],["2659",{"2":{"60":1}}],["2665732502937317",{"2":{"58":1}}],["2698",{"2":{"53":1}}],["262",{"2":{"37":1}}],["2462",{"2":{"119":1,"121":1}}],["2460",{"2":{"119":1,"121":1}}],["24mib",{"2":{"88":1}}],["2403",{"2":{"186":2}}],["2402",{"2":{"186":2}}],["24086",{"2":{"63":1}}],["2407",{"2":{"53":1}}],["2439",{"2":{"53":1}}],["2430",{"2":{"53":1}}],["24",{"2":{"52":1,"53":1,"57":1,"59":1,"60":4,"62":2,"65":1,"89":1,"126":1}}],["2101",{"2":{"182":1,"186":1}}],["2150",{"2":{"130":1}}],["2183",{"2":{"121":1}}],["2184",{"2":{"102":1}}],["2175",{"2":{"119":1,"121":1}}],["2170",{"2":{"54":3}}],["2199",{"2":{"108":1}}],["2128",{"2":{"102":1}}],["21",{"2":{"99":1,"121":3}}],["21k",{"2":{"88":2}}],["2133",{"2":{"54":1}}],["2138",{"2":{"45":2,"48":1,"177":3}}],["2114",{"2":{"37":1}}],["2775",{"2":{"130":1}}],["2745",{"2":{"130":1}}],["2758",{"2":{"119":1,"121":1}}],["2753",{"2":{"46":1,"47":1}}],["2705",{"2":{"107":1}}],["27018",{"2":{"63":1}}],["2710",{"2":{"60":1}}],["271",{"2":{"45":2,"48":1,"177":5}}],["27271",{"2":{"37":1}}],["25",{"2":{"62":1,"89":1,"99":1,"127":2,"179":1}}],["2553",{"2":{"60":1}}],["2561",{"2":{"72":1}}],["2564",{"2":{"60":1}}],["256",{"2":{"44":1,"48":10,"70":2,"76":1,"77":7,"187":1}}],["257",{"2":{"38":1,"42":2,"45":4,"46":1,"48":3,"53":1,"60":5,"63":1,"177":2}}],["2899",{"2":{"121":1}}],["28",{"2":{"80":1}}],["2800",{"2":{"60":1,"82":2}}],["2838",{"2":{"60":1}}],["288",{"2":{"46":1,"48":1}}],["2885",{"2":{"44":4,"45":2,"48":1,"177":3}}],["284",{"2":{"45":4,"48":2}}],["2879",{"2":{"130":2}}],["287",{"2":{"42":4,"45":1}}],["286",{"2":{"37":1}}],["20步后",{"2":{"188":1}}],["20×2+2",{"2":{"174":1}}],["20个输入节点",{"2":{"174":1}}],["20个偏置单元",{"2":{"174":1}}],["20个节点",{"2":{"174":1}}],["2002",{"2":{"185":1}}],["2005",{"2":{"182":1,"185":1}}],["200",{"2":{"175":2}}],["2000",{"2":{"157":1}}],["2006",{"2":{"102":1}}],["2080",{"2":{"140":1}}],["2024",{"2":{"186":4}}],["2023",{"2":{"172":1,"182":6,"183":1,"184":2,"185":2,"186":3}}],["2021",{"2":{"172":1}}],["2022",{"2":{"136":2,"172":1,"182":1,"184":1}}],["2020",{"2":{"14":1,"182":3,"183":1,"185":1,"186":1}}],["2036",{"2":{"116":1,"118":1}}],["2034",{"2":{"53":1}}],["20中总结的步骤进行",{"2":{"115":1}}],["2098",{"2":{"102":1}}],["20mib",{"2":{"88":1}}],["20",{"2":{"62":1,"89":1,"115":1,"155":2,"188":1}}],["2074",{"2":{"60":1,"102":1}}],["20694105327129364",{"2":{"58":1}}],["2041",{"2":{"116":1,"118":1}}],["2046",{"2":{"102":1}}],["2048",{"2":{"52":4}}],["20479",{"2":{"21":2,"76":1}}],["2019",{"2":{"185":2}}],["2012",{"2":{"183":2}}],["2015",{"2":{"183":1}}],["2017",{"2":{"182":1,"184":1}}],["2014",{"2":{"96":1,"184":2}}],["2010",{"2":{"46":1,"47":1,"182":1}}],["2016",{"2":{"45":2,"48":1,"185":2,"186":1}}],["2018",{"2":{"14":1,"172":1,"182":1,"183":1,"186":1}}],["2213",{"2":{"155":1}}],["22169792652130127",{"2":{"58":1}}],["2216",{"2":{"54":1,"128":2}}],["22所示",{"2":{"121":1}}],["2264",{"2":{"108":1}}],["2260",{"2":{"54":1}}],["2278",{"2":{"99":1}}],["2249",{"2":{"99":1}}],["2246",{"2":{"60":1}}],["2241",{"2":{"42":5}}],["2205",{"2":{"184":1}}],["2203",{"2":{"182":1}}],["220",{"2":{"37":1,"177":1}}],["22",{"2":{"12":1,"99":1,"107":4,"121":1}}],["2",{"0":{"9":1,"19":1,"20":2,"21":1,"26":1,"27":2,"28":1,"31":1,"33":1,"37":1,"40":1,"42":1,"43":1,"44":1,"45":2,"46":1,"48":1,"49":1,"53":1,"54":1,"62":1,"71":1,"78":1,"82":1,"84":1,"96":1,"100":1,"102":1,"112":1,"129":1,"132":1,"138":1,"146":1,"147":1,"149":1,"150":2,"151":1,"165":1,"178":1,"183":1,"189":1},"1":{"21":2,"22":2,"23":2,"24":2,"25":2,"27":1,"28":1,"29":1,"30":1,"32":1,"33":1,"34":1,"35":1,"36":1,"38":1,"39":1,"40":1,"41":1,"42":1,"43":2,"44":2,"45":2,"47":2,"55":2,"56":2,"72":1,"73":1,"74":1,"101":1,"102":1,"103":1,"130":1,"131":1,"132":1,"148":2,"149":1,"150":1,"151":1,"154":1,"157":1,"158":1,"160":1,"161":1,"166":1,"167":1},"2":{"9":2,"13":1,"15":2,"16":1,"18":2,"19":12,"20":2,"24":2,"26":2,"27":3,"28":2,"29":1,"31":5,"32":2,"37":1,"39":2,"40":1,"42":7,"43":1,"44":4,"46":6,"47":5,"48":7,"49":1,"52":4,"53":12,"54":6,"55":2,"57":6,"58":4,"59":4,"60":14,"62":4,"69":3,"70":2,"71":14,"72":4,"73":2,"75":1,"76":4,"77":8,"78":2,"80":2,"88":7,"89":10,"91":1,"94":3,"95":1,"99":51,"100":6,"103":6,"104":17,"105":2,"106":17,"107":6,"108":3,"109":7,"110":1,"121":23,"122":3,"123":1,"124":2,"127":3,"128":6,"129":5,"130":3,"131":3,"132":1,"138":1,"141":2,"149":4,"150":2,"151":9,"152":3,"154":2,"155":1,"156":7,"157":11,"161":3,"162":6,"164":8,"165":1,"174":2,"175":1,"177":3,"178":1,"179":4,"180":3,"182":1,"183":1,"184":1,"186":1,"189":1,"190":2}}],["本附录特别涵盖学习率预热",{"2":{"187":1}}],["本示例的目的是为了说明我们如何将一系列计算视为计算图",{"2":{"152":1}}],["本公共领域书籍训练",{"2":{"75":1}}],["本质上",{"2":{"73":1}}],["本节的内容是为那些对pytorch中多gpu计算工作原理感兴趣的读者准备的",{"2":{"169":1}}],["本节的目标是展示自注意力中的一些关键概念",{"2":{"99":1}}],["本节将重点介绍",{"2":{"155":1}}],["本节将实现一个自定义",{"2":{"156":1}}],["本节将实现因果注意力遮罩的代码",{"2":{"115":1}}],["本节将实现原始",{"2":{"104":1}}],["本节将扩展该计算",{"2":{"100":1}}],["本节讲述如何实现这一点",{"2":{"86":1}}],["本节我们将介绍如何保存和加载一个预训练模型",{"2":{"86":1}}],["本节首先准备训练和验证数据集",{"2":{"75":1}}],["本节中",{"2":{"74":1,"83":1,"99":2,"115":1}}],["本节探讨通过计算文本生成损失来量化评估生成文本质量的技术",{"2":{"71":1}}],["本节以及本章剩余内容的结构如图",{"2":{"69":1}}],["本节和下一节将重点介绍这一转换步骤",{"2":{"46":1}}],["本节介绍温度缩放",{"2":{"80":1}}],["本节介绍了如何从词元",{"2":{"47":1}}],["本节介绍一种更复杂的词元化方案",{"2":{"37":1}}],["本节介绍如何将输入文本拆分为独立的词元",{"2":{"20":1}}],["本节所涵盖的",{"2":{"20":1}}],["本书以动手实践为核心",{"2":{"196":1}}],["本书的配套代码包含了用于准备project",{"2":{"186":1}}],["本书的主要关注点",{"2":{"138":1}}],["本书需要你了解的是链式法则用于计算损失函数相对于计算图中模型参数的梯度",{"2":{"154":1}}],["本书会在需要时引入其他操作",{"2":{"151":1}}],["本书无法全面覆盖",{"2":{"151":1}}],["本书基于",{"2":{"141":1}}],["本书中",{"2":{"16":1}}],["本书构建",{"2":{"15":1}}],["本书将专注于实现",{"2":{"14":1}}],["本书将实现",{"2":{"13":1}}],["本书将通过逐步构建一个",{"2":{"9":1}}],["本书将逐步实现这一架构",{"2":{"8":1}}],["本章介绍了ddp",{"2":{"172":1}}],["本章为",{"2":{"135":1}}],["本章为理解",{"2":{"15":1}}],["本章还将指导你如何设置一个配备了",{"2":{"135":1}}],["本章旨在让你具备将深度学习付诸实践的必要技能和知识",{"2":{"135":1}}],["本章聚焦于注意力机制",{"2":{"94":1}}],["本章将详细介绍",{"2":{"137":1}}],["本章将着重于编写并理解",{"2":{"96":1}}],["本章将深入探讨",{"2":{"94":1}}],["本章将专注于实现",{"2":{"52":1}}],["本章重点在于",{"2":{"67":1}}],["本章的其余代码也与这些更大的模型兼容",{"2":{"89":1}}],["本章的最后两节将讨论如何保存和加载训练好的llm",{"2":{"85":1}}],["本章的核心是实现一个训练函数",{"2":{"67":1}}],["本章的重点是实现",{"2":{"51":1}}],["本章中",{"2":{"51":1}}],["本章稍后将实现这种嵌入层",{"2":{"19":1}}],["本章稍后将详细讲解该概念",{"2":{"16":1}}],["本章内容足以帮助你掌握基础内容",{"2":{"172":1}}],["本章内容",{"0":{"7":1,"18":1,"51":1,"67":1,"94":1,"135":1},"1":{"68":1}}],["当上下文大小小于32",{"2":{"185":1}}],["当温度提高到5时",{"2":{"180":1}}],["当其终端节点之一的",{"2":{"153":1}}],["当然",{"2":{"138":1}}],["当大语言模型",{"2":{"92":1}}],["当",{"2":{"59":1,"158":1}}],["当调用",{"2":{"58":1}}],["当模型判断它们有助于提高任务的性能时",{"2":{"54":1}}],["当我们计算简化的注意力权重以获得上下文向量",{"2":{"104":1}}],["当我们将",{"2":{"71":1}}],["当我们实现",{"2":{"63":1}}],["当我们实现后处理代码时",{"2":{"53":1}}],["当我们在第",{"2":{"60":1}}],["当我们希望将",{"2":{"27":1}}],["当处理多个独立文本源时",{"2":{"31":1}}],["当在多个独立的文档或书籍上训练",{"2":{"31":1}}],["当遇到词汇表中没有的词时",{"2":{"31":1}}],["当将词嵌入投射到二维空间进行可视化时",{"2":{"19":1}}],["当前的词元化方案能够成功处理文本中的各种特殊字符",{"2":{"24":1}}],["当前大多数大型语言模型",{"2":{"11":1}}],["当前可以解决的一些问题",{"2":{"8":1}}],["当代",{"2":{"7":1}}],["即欧几里得范数",{"2":{"190":1}}],["即验证集",{"2":{"160":1}}],["即一个训练",{"2":{"157":1}}],["即一种自我标注的方式",{"2":{"14":1}}],["即对应一个训练示例或测试实例的特征和标签",{"2":{"157":1}}],["即对通用文本数据集的预训练",{"2":{"94":1}}],["即衡量函数相对于其一个变量变化率的量",{"2":{"154":1}}],["即沿其对角线进行翻转",{"2":{"151":1}}],["即遮罩掉一半的注意力权重",{"2":{"121":1}}],["即确定序列中元素之间相互关注的程度",{"2":{"99":1}}],["即输入序列中第",{"2":{"99":1}}],["即为每个输入元素计算一个上下文向量",{"2":{"99":1}}],["即中间层",{"2":{"95":1}}],["即隐藏层的内部值",{"2":{"95":1}}],["即嵌入",{"2":{"94":1}}],["即禁用随机采样",{"2":{"85":1}}],["即第1章介绍的下一个单词预测任务",{"2":{"138":1}}],["即第",{"2":{"75":1}}],["即交叉熵损失",{"2":{"72":1}}],["即目标",{"2":{"72":1}}],["即句子中的下一个单词",{"2":{"71":1}}],["即约",{"2":{"71":1}}],["即批次大小",{"2":{"71":1}}],["即表示词汇表中每个",{"2":{"70":1}}],["即",{"2":{"60":1,"62":1,"63":1,"68":1,"71":1,"89":1,"98":1}}],["即十进制形式的",{"2":{"54":1}}],["即标准差",{"2":{"54":1}}],["即便该操作沿指定的维度",{"2":{"54":1}}],["即占位符",{"2":{"53":1}}],["即包含",{"2":{"52":1}}],["即词元",{"2":{"49":1}}],["即将输入右移一位",{"2":{"42":1}}],["即所谓的词元",{"2":{"26":1}}],["即逐词生成文本",{"2":{"19":1}}],["即模型从输入数据中生成自身标签",{"2":{"16":1}}],["即使是小型矩阵乘法操作",{"2":{"175":1}}],["即使在单个gpu或机器上可以完成模型的训练",{"2":{"168":1}}],["即使在其更大但更简单的下一词预测架构下",{"2":{"14":1}}],["即使你不熟悉",{"2":{"95":1}}],["即使有微小的错误也会导致模型无法正常生成",{"2":{"89":1}}],["即使多次运行上述generate",{"2":{"79":1}}],["即使没有高端",{"2":{"75":1}}],["即使没有专门为此训练",{"2":{"14":1}}],["即使神经元接收到负输入",{"2":{"57":1}}],["即使模型在训练中没有见过这样的长度",{"2":{"48":1}}],["即使文本中包含训练数据中未出现的单词",{"2":{"39":1}}],["即刻",{"2":{"14":1}}],["即预测给定句子中被遮掩的词",{"2":{"11":1}}],["即微调",{"2":{"10":1}}],["即可以通过少量示例学习新任务",{"2":{"10":1}}],["即垃圾邮件和正常邮件",{"2":{"8":1}}],["即深度神经网络",{"2":{"8":1}}],["大于0可能会导致进程资源共享相关的问题",{"2":{"158":1}}],["大部分我们不会使用",{"2":{"151":1}}],["大写单词的使用或是否包含可疑链接",{"2":{"8":1}}],["大型",{"2":{"8":1}}],["大型语言模型的应用",{"0":{"9":1}}],["大型语言模型",{"2":{"7":2,"8":1,"182":1}}],["write",{"2":{"187":1}}],["wu等人",{"2":{"182":1,"186":1}}],["w1",{"2":{"152":4,"154":7}}],["wvw",{"2":{"104":2,"105":1,"110":2}}],["wkw",{"2":{"104":2,"105":1,"110":2}}],["wqw",{"2":{"104":2,"105":1,"110":2}}],["w",{"2":{"89":23,"105":3,"106":5,"110":9,"112":6,"116":2,"123":6,"129":6,"177":1,"178":6,"187":1}}],["wte",{"2":{"89":5}}],["wpe",{"2":{"89":2}}],["what",{"2":{"89":1}}],["wharton",{"2":{"20":1,"25":1,"26":2,"29":1,"75":1}}],["where",{"2":{"82":1,"83":1}}],["when",{"2":{"78":2,"190":2}}],["warning",{"2":{"194":6}}],["warmup",{"2":{"188":3,"189":4,"190":7}}],["watch",{"2":{"172":1,"186":1}}],["want",{"2":{"89":1}}],["wanted",{"2":{"78":1,"190":2}}],["way",{"2":{"89":1,"184":1}}],["wasnم",{"2":{"70":1}}],["was",{"2":{"21":1,"78":1,"79":1}}],["web",{"2":{"186":2}}],["webtext2",{"2":{"12":1,"13":1}}],["were",{"2":{"78":1,"190":2}}],["weights",{"2":{"89":6,"99":14,"102":3,"103":1,"108":2,"109":1,"110":2,"112":2,"116":2,"118":1,"121":3,"122":1,"123":4,"129":4}}],["weighted",{"2":{"57":1}}],["weight",{"2":{"46":1,"58":11,"60":5,"68":2,"78":2,"86":1,"88":1,"89":21,"155":3,"178":3,"179":1,"180":1,"188":1,"190":1}}],["worth",{"2":{"182":1}}],["workers",{"2":{"158":8}}],["workers=0",{"0":{"158":1},"2":{"157":3,"158":1}}],["work",{"2":{"83":1,"184":1}}],["works",{"2":{"0":1}}],["wordpiece",{"2":{"183":1}}],["words",{"2":{"26":2,"27":1,"182":1,"183":1}}],["word2vec",{"2":{"19":6}}],["world",{"2":{"23":4,"24":2,"170":7}}],["www",{"2":{"13":1,"16":1,"106":1,"136":1,"172":1,"185":1,"186":1}}],["width",{"2":{"80":3}}],["without",{"2":{"58":2}}],["with",{"2":{"29":2,"58":2,"63":1,"76":1,"78":2,"80":2,"83":2,"99":2,"104":1,"136":1,"161":2,"172":2,"182":3,"183":1,"184":2,"185":1,"186":1,"187":3,"193":1}}],["wikisource",{"2":{"20":1}}],["wiki",{"2":{"13":1}}],["wikipedia",{"2":{"12":1,"13":2}}],["window",{"2":{"78":1}}],["win",{"2":{"8":1}}],["模仿半余弦周期的轨迹",{"2":{"189":1}}],["模式",{"2":{"63":1}}],["模块来直观地构建多头注意力模块",{"2":{"125":1}}],["模块的顺序堆栈",{"2":{"60":1}}],["模块在增强模型从数据中学习和泛化的能力方面起到了关键作用",{"2":{"57":1}}],["模块是一个包含两个线性层和一个",{"2":{"57":1}}],["模块",{"2":{"55":1,"57":2,"59":1,"88":1,"126":1,"159":1}}],["模块中",{"2":{"54":1}}],["模块处理的嵌入输入示例",{"2":{"48":1}}],["模拟了滑动窗口方法",{"2":{"44":1}}],["模糊逻辑或符号推理",{"2":{"8":1}}],["模型输出变得不那么随机且更具确定性",{"2":{"180":1}}],["模型输出一个矩阵",{"2":{"63":1}}],["模型和数据传输包含两个关键步骤",{"2":{"169":1}}],["模型和大多数流行大型语言模型",{"2":{"104":1}}],["模型时",{"2":{"121":1}}],["模型时梯度能够在层间保持一致流动",{"2":{"58":1}}],["模型确定哪些键与查询最匹配后",{"2":{"109":1}}],["模型出现之前",{"2":{"95":1}}],["模型大小的差异",{"2":{"89":1}}],["模型权重到",{"2":{"89":1}}],["模型相关的",{"2":{"88":1}}],["模型只在更大的数据集上训练一轮",{"2":{"78":1}}],["模型只能在起始内容后添加逗号",{"2":{"78":1}}],["模型尚未生成连贯的文本",{"2":{"70":1}}],["模型配置为处理最多",{"2":{"70":1}}],["模型开始",{"2":{"70":1}}],["模型有不同的大小",{"2":{"65":1}}],["模型训练是一个庞大的主题",{"2":{"63":1}}],["模型实例",{"2":{"63":1}}],["模型无法生成连贯文本的原因是因为我们尚未对其进行训练",{"2":{"63":1}}],["模型生成以下文本",{"2":{"70":1}}],["模型生成文本的三步过程",{"2":{"70":1}}],["模型生成文本的函数",{"2":{"63":1}}],["模型生成的文本无意义",{"2":{"65":1}}],["模型生成了无意义的文字",{"2":{"63":1}}],["模型接收的数据块大小等于训练时的批次大小",{"2":{"161":1}}],["模型接收这些",{"2":{"70":1}}],["模型接收到",{"2":{"63":1}}],["模型接收一系列初始",{"2":{"63":1}}],["模型可以顺序生成文本",{"2":{"63":1}}],["模型在几个epoch后开始过拟合",{"2":{"190":1}}],["模型在包含示例及其对应标签的训练数据集上进行训练",{"2":{"138":1}}],["模型在训练数据集中带标签的示例上进行训练",{"2":{"138":1}}],["模型在给定输入的情况下生成下一个",{"2":{"63":1}}],["模型在大局上如何给定一个输入上下文",{"2":{"63":1}}],["模型从输出张量生成文本的过程涉及几个步骤",{"2":{"63":1}}],["模型从初始输入上下文",{"2":{"63":1}}],["模型如何将这些输出张量转换为如图",{"2":{"63":1}}],["模型已经构建出一个完整的句子",{"2":{"63":1}}],["模型现在仅包含",{"2":{"60":1}}],["模型参数数量中减去输出层的参数计数",{"2":{"60":1}}],["模型之前",{"2":{"60":1}}],["模型进行预训练",{"2":{"60":1}}],["模型代码",{"0":{"60":1},"1":{"61":1,"62":1}}],["模型所使用的",{"2":{"55":1}}],["模型中文本生成的机制",{"2":{"63":1}}],["模型中的流动过程",{"2":{"60":1}}],["模型中的位置编码那样固定或预定义",{"2":{"48":1}}],["模型中",{"2":{"57":1,"63":1}}],["模型中使用",{"2":{"54":1}}],["模型中添加层归一化时",{"2":{"54":1}}],["模型中进出流程的高级概览",{"2":{"53":1}}],["模型来展示其用法",{"2":{"53":1}}],["模型来生成文本",{"0":{"50":1},"1":{"51":1,"52":1,"53":1,"54":1,"55":1,"56":1,"57":1,"58":1,"59":1,"60":1,"61":1,"62":1,"63":1,"64":1,"65":1}}],["模型结构",{"2":{"53":1}}],["模型架构的实现",{"2":{"60":1}}],["模型架构的概览",{"2":{"60":1}}],["模型架构中重复多次",{"2":{"60":1}}],["模型架构由以下部分组成",{"2":{"53":1}}],["模型架构类",{"2":{"53":1}}],["模型架构",{"2":{"53":1}}],["模型采用了绝对位置嵌入",{"2":{"49":1}}],["模型使用了绝对位置嵌入",{"2":{"48":1}}],["模型展现了强大的能力",{"2":{"18":1}}],["模型能够",{"2":{"14":1}}],["模型执行未明确训练过的任务的能力被称为",{"2":{"14":1}}],["模型专为翻译而设计",{"2":{"14":1}}],["模型的保存与加载",{"0":{"162":1}}],["模型的基本构建块",{"2":{"110":1}}],["模型的总体架构保持不变",{"2":{"89":1}}],["模型的总参数数量",{"2":{"62":1}}],["模型的整体架构相同",{"2":{"89":1}}],["模型的共享方式",{"2":{"89":1}}],["模型的权重",{"2":{"88":1,"89":1}}],["模型的语言能力提高了很多",{"2":{"78":1}}],["模型的词汇量为",{"2":{"71":1}}],["模型的词元化器不使用",{"2":{"36":1}}],["模型的词元化器通常只使用",{"2":{"36":1}}],["模型的输入",{"2":{"71":1}}],["模型的输入和输出",{"2":{"60":1}}],["模型的输出被转换回文本",{"2":{"63":1}}],["模型的张量输出转换回文本",{"2":{"63":1}}],["模型的大小一致",{"2":{"60":1}}],["模型的归一化层兼容",{"2":{"55":1}}],["模型的规模",{"2":{"52":1,"65":1}}],["模型的核心结构组件",{"2":{"65":1}}],["模型的核心结构",{"2":{"52":1}}],["模型的结构",{"2":{"52":1}}],["模型的参数数量和存储需求",{"2":{"51":1}}],["模型的绝对嵌入方法",{"2":{"48":1}}],["模型的嵌入尺寸",{"2":{"48":1}}],["模型的下一词预测预训练任务中",{"2":{"14":1}}],["模型的一个显著区别在于",{"2":{"7":1}}],["模型主要通过相对简单的",{"2":{"14":1}}],["模型主要被设计和训练用于文本补全任务",{"2":{"11":1}}],["模型",{"0":{"62":1},"2":{"11":1,"14":2,"51":1,"52":1,"53":1,"59":1,"60":4,"62":1,"65":1,"76":1,"88":1,"91":1,"138":1}}],["模型会在更具体",{"2":{"10":1}}],["模型会在一个大规模",{"2":{"10":1}}],["模型学会识别代表垃圾邮件的模式和特征",{"2":{"8":1}}],["专为生成任务设计",{"2":{"11":1}}],["专为金融领域打造的",{"2":{"10":1}}],["专家系统",{"2":{"8":1}}],["专注于深度神经网络的训练和应用",{"2":{"138":1}}],["专注于深度神经网络的实现",{"2":{"138":1}}],["专注于开发和改进学习算法",{"2":{"138":1}}],["专注于开发能够从数据中学习并基于数据进行预测或决策的算法",{"2":{"8":1}}],["专注于生成文本的任务",{"2":{"11":1}}],["专注于生成任务",{"2":{"11":1}}],["专注于遮掩词预测",{"2":{"11":1}}],["专注于使用三层或更多层的神经网络",{"2":{"8":1}}],["专注于使用多层神经网络",{"2":{"8":1}}],["遗传算法",{"2":{"8":1}}],["例如20到40",{"2":{"180":1}}],["例如正式文件或报告",{"2":{"180":1}}],["例如小于10",{"2":{"180":1}}],["例如jupyter",{"2":{"170":1}}],["例如单个数字",{"2":{"149":1}}],["例如一个已经转化为词嵌入",{"2":{"99":1}}],["例如一个词或子词",{"2":{"59":1}}],["例如语言翻译",{"2":{"96":1}}],["例如德语",{"2":{"95":1}}],["例如最大的1558m参数模型",{"2":{"91":1}}],["例如嵌入层权重",{"2":{"89":1}}],["例如使用本章定义的train",{"2":{"86":1}}],["例如我们在第",{"2":{"68":1}}],["例如我们创建的",{"2":{"53":1}}],["例如未知单词或标记不相关文本之间的边界",{"2":{"49":1}}],["例如机器翻译",{"2":{"11":1}}],["例如带有垃圾邮件和非垃圾邮件标签的邮件",{"2":{"10":1}}],["例如",{"2":{"10":1,"11":1,"13":2,"19":2,"31":1,"41":1,"44":1,"47":1,"48":2,"52":1,"54":2,"63":1,"65":2,"68":1,"73":1,"76":1,"80":2,"83":1,"88":1,"96":1,"99":1,"115":1,"121":1,"124":1,"128":1,"138":1,"140":2,"147":3,"154":1,"155":1,"157":1,"160":1,"164":2,"180":1,"190":2}}],["例如某些触发词的频率",{"2":{"8":1}}],["例如基于规则的系统",{"2":{"8":1}}],["例如解析复杂指令",{"2":{"7":1}}],["但该仓库启发了将大型gpt",{"2":{"185":1}}],["但最大的版本有1750亿参数",{"2":{"185":1}}],["但不太流行",{"2":{"182":1}}],["但不是总是如此",{"2":{"80":1}}],["但不是精确的",{"2":{"54":1}}],["但养成使用",{"2":{"160":1}}],["但具体设置需视硬件和",{"2":{"158":1}}],["但应根据数据集大小和计算环境进行适配",{"2":{"158":1}}],["但若数据集较小",{"2":{"158":1}}],["但第3批仅包含一个样本",{"2":{"157":1}}],["但当有多个要按特定顺序执行的层时",{"2":{"155":1}}],["但增加了对深度学习非常重要的一些特性",{"2":{"147":1}}],["但适配了",{"2":{"142":1}}],["但并不是对",{"2":{"135":1}}],["但为便于理解计算",{"2":{"105":1}}],["但为便于说明",{"2":{"42":1}}],["但一旦掌握其基本原理",{"2":{"97":1}}],["但是",{"2":{"89":1}}],["但openai的原始gpt",{"2":{"89":1}}],["但此练习表明",{"2":{"86":1}}],["但也更容易导致无意义的文本",{"2":{"80":1}}],["但其张量格式的一个主要优势是对gpu的支持",{"2":{"171":1}}],["但其关键在于编码器部分将整个输入文本转换为隐藏状态",{"2":{"95":1}}],["但其他token如",{"2":{"80":1}}],["但其中的构建模块使用",{"2":{"60":1}}],["但正如前述",{"2":{"76":1}}],["但可参考附录",{"2":{"72":1}}],["但我们通常会多次使用它来调整模型设置",{"2":{"160":1}}],["但我们的",{"2":{"71":1}}],["但我们在",{"2":{"53":1}}],["但每个输出向量的内容被重新编码",{"2":{"59":1}}],["但随着我们逐层向前推进",{"2":{"58":1}}],["但对于",{"2":{"126":1}}],["但对于负值有一个非零的梯度",{"2":{"57":1}}],["但对于实验来说足够合理",{"2":{"48":1}}],["但它使我们可以通过替代嵌套的",{"2":{"133":1}}],["但它在许多大模型架构中常见",{"2":{"130":1}}],["但它在数学上实现的原理与之前的",{"2":{"129":1}}],["但它通过第一个线性层将嵌入维度扩展到一个更高维度的空间",{"2":{"57":1}}],["但它们缺乏词元在序列中的位置信息",{"2":{"49":1}}],["但它还包括其他方法",{"2":{"8":1}}],["但会导致模型更新更加不稳定",{"2":{"45":1}}],["但会牺牲计算效率",{"2":{"19":1}}],["但代码实现将直接处理词元",{"2":{"42":1}}],["但将词元",{"2":{"42":1}}],["但保留空白字符可能对对空白敏感的模型",{"2":{"24":1}}],["但还有句子",{"2":{"19":1}}],["但需要注意的是",{"2":{"19":1}}],["但在计算效率上进行了优化",{"2":{"184":1}}],["但在此之前",{"2":{"157":1}}],["但在此处有几个优点",{"2":{"124":1}}],["但在实际操作中",{"2":{"154":1}}],["但在深入讨论计算梯度之前",{"2":{"152":1}}],["但在灵活性上没有妥协",{"2":{"136":1}}],["但在模型训练中应设置",{"2":{"105":1}}],["但在处理长文本时表现较差",{"2":{"96":1}}],["但在第二轮之后对训练集的拟合过度",{"2":{"78":1}}],["但在第",{"2":{"53":1}}],["但在这一章",{"2":{"52":1}}],["但在我们能实现和训练",{"2":{"18":1}}],["但在垃圾邮件分类中",{"2":{"8":1}}],["但这些",{"2":{"16":1}}],["但像",{"2":{"14":1}}],["但类似的数据集",{"2":{"13":1}}],["但",{"2":{"13":1,"19":1,"143":1,"162":1}}],["但出色的模型能力往往令研究人员感到意外",{"2":{"8":1}}],["领域以机器学习和深度学习为主导",{"2":{"8":1}}],["领域带来了革命性的进展",{"2":{"7":1}}],["尽管本书中的代码与nanogpt有所不同",{"2":{"185":1}}],["尽管基于缩放点积的多头注意力是自注意力中最常见的变体",{"2":{"184":1}}],["尽管介绍了微积分的术语",{"2":{"154":1}}],["尽管取得了显著进展",{"2":{"138":1}}],["尽管它易于上手",{"2":{"136":1}}],["尽管当前目标是计算一个上下文向量",{"2":{"106":1}}],["尽管我们不需要深入了解编码器",{"2":{"95":1}}],["尽管最后一层",{"2":{"58":1}}],["尽管最初的",{"2":{"14":1}}],["尽管贡献小于正输入",{"2":{"57":1}}],["尽管代码已具备基本功能",{"2":{"53":1}}],["尽管其规模庞大",{"2":{"52":1}}],["尽管词嵌入是最常见的文本嵌入形式",{"2":{"19":1}}],["尽管",{"2":{"13":1}}],["尽管目前",{"2":{"8":1}}],["尽管这个任务听起来简单",{"2":{"8":1}}],["传统机器学习模型和深度神经网络的监督学习通常需要标注信息",{"2":{"16":1}}],["传统机器学习需要人工提取特征",{"2":{"8":1}}],["传统方法主要在分类任务上表现优异",{"2":{"7":1}}],["与本书中实现的模型类似",{"2":{"185":1}}],["与此前发布的方法相比显著缩短了训练时间",{"2":{"185":1}}],["与gpt",{"2":{"182":1}}],["与训练样本对应的是一个包含类标签的张量",{"2":{"156":1}}],["与训练集损失类似",{"2":{"78":1}}],["与给定标签",{"2":{"152":1}}],["与模型权重",{"2":{"152":1}}],["与擅长简单模式识别的传统机器学习技术不同",{"2":{"138":1}}],["与其维护两个单独的类",{"2":{"129":1}}],["与其他输入元素的点积来实现",{"2":{"99":1}}],["与上一节的",{"2":{"110":1}}],["与权重矩阵",{"2":{"104":1}}],["与之前在",{"2":{"165":1}}],["与之前的章节类似",{"2":{"104":1}}],["与之前一样",{"2":{"100":1}}],["与我们在3",{"2":{"104":1}}],["与对应的注意力权重相乘并求和",{"2":{"99":1}}],["与损失类似",{"2":{"74":1}}],["与交叉熵损失一起使用",{"2":{"74":1}}],["与实际期望输出的差异",{"2":{"72":1}}],["与目标",{"2":{"71":1,"73":1}}],["与图",{"2":{"63":1}}],["与原始",{"2":{"60":1}}],["与较为简单的",{"2":{"57":1}}],["与批量归一化不同",{"2":{"56":1}}],["与输入维度相同",{"2":{"54":1}}],["与使用",{"2":{"54":1}}],["与分词器的词汇大小一致",{"2":{"53":1}}],["与第",{"2":{"53":1}}],["与常规深度学习类似",{"2":{"45":1}}],["与检索",{"2":{"19":1}}],["与",{"0":{"53":1},"1":{"55":1,"56":1},"2":{"11":2,"14":1,"57":1,"129":1}}],["与深度学习相比",{"2":{"8":1}}],["与早期",{"2":{"7":1}}],["不稳定更新的风险",{"2":{"188":1}}],["不会被采样",{"2":{"180":1}}],["不要求具备多gpu的访问权限或使用条件",{"2":{"169":1}}],["不要与注意力权重混淆",{"2":{"106":1}}],["不可用",{"2":{"165":1}}],["不可能简单地逐词翻译",{"2":{"95":1}}],["不能被2整除",{"2":{"157":1}}],["不能访问未来标记",{"2":{"115":1}}],["不用担心",{"2":{"154":1}}],["不必担心",{"2":{"152":1}}],["不需要计算",{"2":{"161":1}}],["不需要",{"2":{"143":1}}],["不再局限于严格的基于规则的系统",{"2":{"138":1}}],["不再通过手动编写识别垃圾邮件的规则",{"2":{"8":1}}],["不包含任何可训练权重",{"2":{"99":1}}],["不论它在输入序列中的位置",{"2":{"48":1}}],["不相关的句子组成一个简单文本样本来测试新词元化器",{"2":{"34":1}}],["不含空白字符",{"2":{"25":1}}],["不同阶数的张量示例",{"2":{"147":1}}],["不同之处在于",{"2":{"108":1}}],["不同之处在于现在我们使用了通过权重矩阵变换后的查询和键",{"2":{"107":1}}],["不同大小的",{"2":{"89":1}}],["不同的是",{"2":{"57":1}}],["不同的数据格式需要不同的嵌入模型",{"2":{"19":1}}],["不同",{"0":{"45":1},"2":{"57":1,"113":1}}],["不同类型的鸟在嵌入空间中比国家和城市更接近",{"2":{"19":1}}],["不熟悉计算环境中的向量和张量的读者可以参考附录",{"2":{"19":1}}],["不过这里包括此行是为了说明我们需要一个模型实例来应用保存的参数",{"2":{"162":1}}],["不过我们会在书中逐步介绍相关操作",{"2":{"151":1}}],["不过我们实际上可以使用任何文件扩展名",{"2":{"86":1}}],["不过在下一节中",{"2":{"121":1}}],["不过",{"2":{"11":1,"13":1,"22":1,"23":1,"54":1,"60":1,"63":1,"78":1,"89":2,"157":1,"170":1}}],["不过由于复杂性",{"2":{"11":1}}],["不仅指模型参数的规模",{"2":{"8":1}}],["举例来说",{"2":{"8":1,"52":1}}],["举个例子",{"2":{"7":1}}],["图a",{"2":{"137":1,"138":3,"141":2,"143":1,"147":1,"152":1,"153":1,"154":1,"155":1,"156":2,"158":2,"169":2}}],["图5",{"2":{"78":3,"80":3,"82":2,"86":1,"89":1}}],["图的底部显示了已嵌入到",{"2":{"59":1}}],["图中总结了我们到目前为止实现的四种不同的注意力模块",{"2":{"124":1}}],["图中为",{"2":{"76":1}}],["图中展示了如何计算上下文向量",{"2":{"99":1}}],["图中展示了一个简化的",{"2":{"71":1}}],["图中展示了数据在",{"2":{"60":1}}],["图中展示的文本在",{"2":{"42":1}}],["图中的梯度表示每层的平均绝对梯度",{"2":{"58":1}}],["图中的编号标示了实现",{"2":{"53":1}}],["图中词元嵌入的值被设置为",{"2":{"48":1}}],["图中显示了翻译过程的最后一步",{"2":{"11":1}}],["图像甚至整个文档",{"2":{"19":1}}],["图",{"2":{"8":1,"9":1,"10":1,"11":4,"14":2,"15":1,"18":1,"19":2,"20":1,"24":1,"26":1,"27":1,"28":1,"31":2,"39":1,"42":2,"44":1,"46":1,"47":1,"48":3,"51":1,"52":1,"53":2,"54":3,"55":2,"57":5,"58":1,"59":2,"60":1,"63":4,"67":1,"69":1,"70":2,"71":5,"72":2,"75":1,"94":2,"95":2,"96":2,"99":2,"100":1,"104":3,"107":1,"108":1,"109":1,"110":1,"115":2,"121":2,"124":1,"126":2,"127":1}}],["所以5个样本全部预测正确",{"2":{"161":1}}],["所以需要一个连续的向量表示",{"2":{"46":1}}],["所有张量必须位于同一设备上",{"2":{"164":1}}],["所有偏导数的向量",{"2":{"154":1}}],["所有计算都将在普通硬件上运行",{"2":{"13":1}}],["所提供的主要功能",{"2":{"136":1}}],["所需的资源",{"2":{"76":1}}],["所需的",{"2":{"67":1}}],["所需的输入",{"2":{"18":1}}],["所总结的那样",{"2":{"48":1}}],["所示的三个步骤操作",{"2":{"100":1}}],["所示的例子",{"2":{"98":1}}],["所示的捷径连接",{"2":{"58":1}}],["所示的步骤",{"2":{"53":1}}],["所示的流程外",{"2":{"46":1}}],["所示的下一词预测任务",{"2":{"42":1}}],["所示的输入",{"2":{"42":1}}],["所示",{"2":{"8":2,"10":2,"11":3,"14":3,"15":1,"18":1,"19":4,"20":1,"24":1,"26":1,"27":1,"28":1,"31":3,"39":1,"42":2,"44":1,"46":1,"47":1,"48":2,"51":1,"52":1,"53":1,"54":2,"57":3,"58":2,"59":4,"60":2,"63":6,"67":2,"69":2,"71":2,"76":1,"77":1,"94":3,"95":2,"96":3,"99":5,"101":1,"104":1,"105":1,"108":1,"110":1,"115":2,"121":1,"127":1,"129":1,"138":1,"157":1}}],["所谓的",{"2":{"7":1}}],["或单词",{"2":{"180":1}}],["或称为批次",{"2":{"169":1}}],["或称计算图",{"2":{"152":1}}],["或损失本身",{"2":{"153":1}}],["或秩",{"2":{"147":1}}],["或者",{"2":{"143":1}}],["或者称为嵌入",{"2":{"46":1}}],["或更新的型号",{"2":{"166":1}}],["或更新型号",{"2":{"144":1}}],["或更高",{"2":{"140":1}}],["或更大很常见",{"2":{"77":1}}],["或重复",{"2":{"78":1}}],["或简而言之为参数",{"2":{"68":1}}],["或残差连接",{"2":{"58":1}}],["或标记",{"2":{"52":1}}],["或谷歌的",{"2":{"9":1}}],["或",{"2":{"8":1,"9":1,"54":1,"63":1,"70":1,"121":1,"124":1,"140":1}}],["因果注意力机制确保模型仅考虑当前标记及之前出现的标记",{"2":{"115":1}}],["因果注意力机制为自注意力添加了一个掩码",{"2":{"94":1}}],["因果注意力",{"2":{"115":1}}],["因果性修正注意力机制",{"2":{"114":1}}],["因果性",{"2":{"114":1}}],["因为其计算效率更高",{"2":{"185":1}}],["因为验证集也被用于训练",{"2":{"180":1}}],["因为可能需要多次训练迭代来微调模型参数和架构",{"2":{"168":1}}],["因为总训练时间可能仅需几秒钟",{"2":{"158":1}}],["因为有两个输入文本",{"2":{"128":1}}],["因为这种变体在实践中更为常见",{"2":{"121":1}}],["因为本讨论的重点在于编码器",{"2":{"95":1}}],["因为本章后面将使用预构建的词元化工具",{"2":{"23":1}}],["因为源语言和目标语言的语法结构不同",{"2":{"95":1}}],["因为模型会等待下一批数据的加载",{"2":{"158":1}}],["因为模型能够生成连贯的文本",{"2":{"89":1}}],["因为模型还没有经过训练",{"2":{"71":1}}],["因为openai使用的命名方式与我们的略有不同",{"2":{"89":1}}],["因为权重内容太多",{"2":{"89":1}}],["因为python使用0索引",{"2":{"80":1}}],["因为对于相对较小的模型来说",{"2":{"79":1}}],["因为我们将权重矩阵的列数",{"2":{"106":1}}],["因为我们不会在本书中使用它",{"2":{"95":1}}],["因为我们使用的训练数据集非常小",{"2":{"78":1}}],["因为我们使用了非常小的数据集",{"2":{"77":1}}],["因为我们传入了",{"2":{"60":1}}],["因为我们的感官认知和常见的图形表示通常仅限于三维或以下",{"2":{"19":1}}],["因为会出现梯度消失或梯度爆炸等问题",{"2":{"54":1}}],["因为它与独立的python脚本处理多进程的方式不同",{"2":{"170":1}}],["因为它是",{"2":{"142":1}}],["因为它无法直接访问输入中的前面部分",{"2":{"96":1}}],["因为它们对建模性能没有明显提升",{"2":{"89":1}}],["因为它们无法直接处理原始文本",{"2":{"49":1}}],["因为它表示模型在每一步中不确定的有效词汇大小",{"2":{"74":1}}],["因为它尚未经过训练",{"2":{"71":1}}],["因为它还没有经过训练",{"2":{"70":1}}],["因为它允许对模型参数进行更细微的调整",{"2":{"57":1}}],["因为它可以在单台笔记本电脑上运行",{"2":{"53":1}}],["因为它的许多组件是重复的",{"2":{"52":1}}],["因为它的训练目标主要是下一词预测任务",{"2":{"14":1}}],["因为嵌入层是一种等效于独热编码和矩阵乘法的更高效实现",{"2":{"47":1}}],["因为过多的重叠可能导致过拟合增加",{"2":{"45":1}}],["因为大写能帮助",{"2":{"23":1}}],["因为仍然存在基于循环网络和卷积网络的",{"2":{"11":1}}],["因为",{"2":{"8":1,"11":1,"42":1,"46":1,"53":1,"63":1,"157":1,"158":1,"164":1}}],["因此模型副本在前向传播阶段会产生不同的输出",{"2":{"169":1}}],["因此可以获得与上述相同的训练样本顺序",{"2":{"157":1}}],["因此可以省略",{"2":{"89":1}}],["因此称为",{"2":{"142":1}}],["因此建议使用以下命令来安装确切版本",{"2":{"141":1}}],["因此安装可能需要一些额外说明",{"2":{"139":1}}],["因此深度学习非常适合",{"2":{"138":1}}],["因此这两个上下文向量完全相同",{"2":{"128":1}}],["因此这种机制称为缩放点积注意力",{"2":{"108":1}}],["因此不会发生信息泄露",{"2":{"120":1}}],["因此不在词汇表中",{"2":{"30":1}}],["因此我们通常称",{"2":{"149":1}}],["因此我们在此添加它以便更全面",{"2":{"130":1}}],["因此我们将用整整一章来讨论",{"2":{"94":1}}],["因此我们还需相应更新new",{"2":{"89":1}}],["因此我们需要安装",{"2":{"88":1}}],["因此我们可以在几分钟内完成代码运行",{"2":{"76":1}}],["因此在计算softmax值时",{"2":{"82":1}}],["因此在实践中交叉熵和负平均对数概率常常可互换使用",{"2":{"73":1}}],["因此最可能的token",{"2":{"80":1}}],["因此经常用于训练llm",{"2":{"78":1}}],["因此仅有",{"2":{"77":1}}],["因此大多数初始概率会在",{"2":{"71":1}}],["因此起始随机值大约为",{"2":{"71":1}}],["因此代码中的",{"2":{"71":1}}],["因此实际上",{"2":{"63":1}}],["因此它可以被视为一种可以通过反向传播优化的神经网络层",{"2":{"47":1}}],["因此对应索引",{"2":{"46":1}}],["因此本章将重点放在词嵌入上",{"2":{"19":1}}],["因此被视为自回归模型",{"2":{"14":1}}],["因此",{"2":{"7":1,"11":1,"14":2,"15":1,"19":3,"39":1,"60":1,"86":1,"88":1,"92":1,"96":1,"115":1,"140":1,"141":1,"158":1,"170":1}}],["使网络可以学习输入与输出的复杂映射",{"2":{"155":1}}],["使输出上下文向量为",{"2":{"128":1}}],["使每行的注意力权重之和为",{"2":{"115":1}}],["使模型能够同时关注不同位置的信息",{"2":{"114":1}}],["使模型能够并行捕捉输入数据的不同方面",{"2":{"94":1}}],["使模型",{"2":{"104":1}}],["使我们能够评估训练是否使模型有所改进",{"2":{"78":1}}],["使我们能够在后续的",{"2":{"13":1}}],["使我们能够在现有的开源",{"2":{"10":1}}],["使得最终的范数正好等于1",{"2":{"190":1}}],["使得损失在一定数量的",{"2":{"159":1}}],["使得构建和训练模型更为便捷",{"2":{"155":1}}],["使得梯度计算更为简便",{"2":{"147":1}}],["使得实现和训练深度神经网络模型更加容易",{"2":{"137":1}}],["使得语言建模任务中",{"2":{"114":1}}],["使得它们生成相同的结果",{"2":{"113":1}}],["使得它们的均值为",{"2":{"54":1,"55":1}}],["使得各行的值总和为",{"2":{"102":1}}],["使得解码器可以在每个解码步骤中选择性地访问输入序列的不同部分",{"2":{"96":1}}],["使得",{"2":{"94":1,"103":1}}],["使得大型语言模型",{"2":{"94":1}}],["使得我们不必花费数十万甚至上百万美元在大规模语料库上重新训练模型",{"2":{"88":1}}],["使得在普通笔记本电脑上进行训练成为可能",{"2":{"70":1}}],["使得模型为期望的",{"2":{"72":1}}],["使得模型可以生成连贯且符合语境的文本",{"2":{"63":1}}],["使得模型能够捕捉各种语言的细微差别",{"2":{"7":1}}],["使得可以堆叠多个层",{"2":{"57":1}}],["使得神经网络的损失函数最小化",{"2":{"54":1}}],["使概念和代码示例更容易展示",{"2":{"52":1}}],["使",{"2":{"16":1,"138":1}}],["使用相对较小的top",{"2":{"180":1}}],["使用distributeddataparallel是加速训练的最简单方法",{"2":{"171":1}}],["使用distributeddataparallel策略进行模型训练",{"2":{"170":1}}],["使用ddp的好处在于",{"2":{"169":1}}],["使用多gpu进行训练",{"0":{"168":1},"1":{"169":1,"170":1}}],["使用多个自注意力机制虽然计算密集",{"2":{"126":1}}],["使用这种数据类型可以显著加快模型训练和推理",{"2":{"150":1}}],["使用这些注意力权重",{"2":{"103":1}}],["使用安装菜单",{"2":{"141":1}}],["使用因果注意力遮罩",{"2":{"121":1}}],["使用因果注意力隐藏未来词",{"0":{"115":1},"1":{"116":1,"117":1,"118":1,"119":1,"120":1,"121":1,"122":1,"123":1,"124":1}}],["使用了更复杂的权重初始化方案",{"2":{"112":1}}],["使用了昂贵的",{"2":{"76":1}}],["使用注意力权重和值",{"2":{"110":1}}],["使用注意力机制捕获数据依赖关系",{"0":{"96":1}}],["使用自注意力关注输入的不同部分",{"0":{"97":1},"1":{"98":1,"99":1,"100":1,"101":1,"102":1,"103":1}}],["使用传统的交叉熵损失和adamw优化器",{"2":{"92":1}}],["使用openai的模型权重的最后一步是用params字典中加载的权重覆盖这些随机权重",{"2":{"89":1}}],["使用torch",{"2":{"86":1}}],["使用top",{"2":{"82":1}}],["使用model",{"2":{"86":1}}],["使用print",{"2":{"81":1}}],["使用温度5时",{"2":{"80":1}}],["使用温度1相当于不进行任何温度缩放",{"2":{"80":1}}],["使用非常小的温度",{"2":{"80":1}}],["使用我们之前定义的generate",{"2":{"78":1}}],["使用我们先前定义的",{"2":{"59":1}}],["使用可变长度输入训练",{"2":{"77":1}}],["使用可变长度进行训练",{"0":{"77":1}}],["使用单独的",{"0":{"64":1}}],["使用独立的",{"2":{"60":1}}],["使用的一种激活函数",{"2":{"55":1}}],["使用的字节对编码",{"2":{"49":1}}],["使用层归一化来调整激活值",{"0":{"54":1},"1":{"55":1,"56":1}}],["使用滑动窗口的数据采样",{"2":{"48":1}}],["使用滑动窗口进行数据采样",{"0":{"42":1},"1":{"43":1,"44":1,"45":1,"47":1}}],["使用滑动窗口方法抽样训练示例",{"2":{"18":1}}],["使用一些简单的示例文本",{"2":{"23":1}}],["使用较小的文本样本",{"2":{"22":1}}],["使用类似设置可以确保我们的方法兼容将在第",{"2":{"55":1}}],["使用类似",{"2":{"19":1}}],["使用字节对编码进行更高级的文本词元化",{"2":{"18":1}}],["使用条款可能因使用目的和国家而异",{"2":{"13":1}}],["使用大型语言模型完成不同任务",{"0":{"11":1}}],["使用",{"0":{"57":1,"70":1,"111":1,"112":1,"131":1,"141":1,"163":1},"1":{"164":1,"165":1,"166":1,"167":1,"168":1,"169":1,"170":1},"2":{"10":1,"11":1,"14":1,"18":1,"54":3,"55":1,"60":1,"62":2,"63":1,"70":2,"77":1,"94":1,"95":1,"102":1,"111":1,"121":1,"132":1,"136":1,"154":2,"155":1}}],["使其在达到最大值后逐渐下降",{"2":{"188":1}}],["使其在预测时能够选择性地关注输入的不同部分",{"2":{"8":1}}],["使其重新归一化为",{"2":{"119":1}}],["使其和为",{"2":{"99":1}}],["使其对",{"2":{"99":1}}],["使其始终生成与generate",{"2":{"85":1}}],["使其生成更优质的文本",{"2":{"77":1}}],["使其接近",{"2":{"71":1}}],["使其可以与神经网络操作兼容",{"2":{"49":1}}],["使其能够处理",{"2":{"70":1}}],["使其能够处理词汇表外的单词",{"2":{"38":1}}],["使其能够处理其他标点符号",{"2":{"24":1}}],["使其能够执行个人助手或文本分类等任务",{"2":{"18":1}}],["使其能够执行回答问题或文本分类等指令任务",{"2":{"15":1}}],["使其能够生成新文本",{"2":{"15":1}}],["使其成为个人助手或文本分类器",{"2":{"15":1}}],["使其更加对话化",{"2":{"9":1}}],["使计算机能够从数据中学习并执行通常需要人类智能的任务",{"2":{"8":1}}],["4节中详细讨论",{"2":{"147":1}}],["4中显示的命令还会安装",{"2":{"141":1}}],["4所示",{"2":{"141":1}}],["4772",{"2":{"128":2}}],["4753",{"2":{"101":4}}],["4754",{"2":{"99":1,"101":4}}],["471k",{"2":{"88":2}}],["47678",{"2":{"74":2}}],["47843",{"2":{"63":1}}],["4545",{"2":{"130":1}}],["4541e",{"2":{"71":1}}],["4519",{"2":{"128":2}}],["4594",{"2":{"121":1}}],["4551",{"2":{"106":1}}],["4570",{"2":{"101":2}}],["4576",{"2":{"101":4}}],["456k",{"2":{"88":2}}],["452",{"2":{"78":1}}],["45",{"2":{"65":1}}],["4530",{"2":{"53":1}}],["4539",{"2":{"53":1}}],["4816",{"2":{"161":1}}],["4814",{"2":{"161":1}}],["4858",{"2":{"131":1}}],["48",{"2":{"62":1,"89":1,"179":1}}],["4835",{"2":{"53":1}}],["44",{"2":{"159":1,"165":1}}],["4483",{"2":{"119":1,"121":1}}],["4431",{"2":{"103":1}}],["4432",{"2":{"53":1}}],["4421",{"2":{"103":1}}],["4419",{"2":{"99":1,"103":2}}],["4454",{"2":{"60":1}}],["432",{"2":{"179":1}}],["4327",{"2":{"60":1}}],["4391",{"2":{"130":1}}],["4340",{"2":{"130":1}}],["4306",{"2":{"106":1}}],["4304",{"2":{"103":1}}],["43",{"2":{"99":1}}],["438",{"2":{"45":4,"48":2,"177":1}}],["425",{"2":{"175":1}}],["4252",{"2":{"37":1}}],["4220",{"2":{"130":1}}],["4222",{"2":{"60":1}}],["42×2=4",{"2":{"127":1}}],["42826",{"2":{"71":1}}],["428",{"2":{"71":1}}],["42348",{"2":{"63":1}}],["42",{"2":{"60":1,"65":1}}],["4177",{"2":{"103":1}}],["41751",{"2":{"71":1}}],["412",{"2":{"60":1}}],["4126",{"2":{"54":1}}],["4100",{"2":{"12":1,"13":1}}],["4x256",{"2":{"48":2}}],["4x3",{"2":{"47":1}}],["400",{"2":{"190":1}}],["4066",{"2":{"130":1}}],["4096",{"2":{"46":1,"47":1}}],["4015",{"2":{"46":2,"47":1}}],["4023",{"2":{"60":1}}],["402",{"2":{"45":2,"48":1,"177":5}}],["40",{"2":{"44":1,"45":1,"48":1,"71":1,"136":1,"177":2}}],["4925",{"2":{"122":1}}],["4921",{"2":{"122":1}}],["4920",{"2":{"42":6}}],["4937",{"2":{"101":2}}],["4950",{"2":{"99":1,"101":2}}],["498m",{"2":{"88":2}}],["49906",{"2":{"71":1}}],["49",{"2":{"27":1}}],["4656",{"2":{"121":1}}],["4671",{"2":{"103":1}}],["46mib",{"2":{"88":1}}],["4695",{"2":{"53":1}}],["4661",{"2":{"60":1}}],["4667",{"2":{"53":1}}],["466",{"2":{"37":1}}],["4649",{"2":{"25":1}}],["4606",{"2":{"130":1}}],["4600000",{"2":{"16":1}}],["460",{"2":{"13":1,"16":1}}],["4",{"0":{"11":1,"31":1,"33":1,"52":1,"54":1,"57":1,"58":2,"59":1,"60":1,"61":1,"62":1,"63":1,"64":1,"65":1,"86":1,"87":1,"104":1,"127":1,"153":1,"167":1,"175":1,"180":1,"185":1},"1":{"32":1,"33":1,"34":1,"35":1,"36":1,"53":1,"55":1,"56":1,"61":1,"62":1,"64":1,"87":1,"89":1,"90":1,"91":1,"105":1,"106":1,"107":1,"108":1,"109":1,"110":1,"111":1,"112":1,"113":1,"114":1,"154":1},"2":{"8":1,"11":3,"14":1,"20":2,"32":1,"42":1,"44":4,"45":1,"46":2,"48":8,"51":2,"52":3,"53":9,"54":8,"55":2,"57":15,"58":13,"59":14,"60":10,"61":2,"63":16,"68":1,"70":2,"71":10,"72":2,"80":2,"82":2,"83":2,"86":1,"95":3,"96":1,"98":1,"99":1,"104":2,"109":1,"121":1,"126":1,"127":1,"128":3,"141":2,"149":2,"151":5,"155":2,"157":1,"161":1,"164":1,"172":1,"179":3,"180":2,"190":2,"193":1}}],["生成遮罩矩阵",{"0":{"117":1}}],["生成下一个词更为重要",{"2":{"99":1}}],["生成过程在图",{"2":{"63":1}}],["生成过程",{"2":{"63":1}}],["生成过程展示了",{"2":{"63":1}}],["生成序列中的下一个",{"2":{"63":1}}],["生成文本时",{"2":{"92":1}}],["生成文本",{"0":{"63":1,"70":1},"1":{"64":1},"2":{"63":1}}],["生成的文本与我们在5",{"2":{"83":1}}],["生成的文本的整体流程",{"2":{"71":1}}],["生成的图如图5",{"2":{"80":1}}],["生成的单词是",{"2":{"80":1}}],["生成的",{"2":{"73":2}}],["生成的嵌入向量为词元提供了连续表示",{"2":{"49":1}}],["生成的基础模型可以更高效地进行多种下游任务的微调",{"2":{"16":1}}],["生成用于",{"2":{"49":1}}],["生成一个聚合每行数据的输出",{"2":{"54":1}}],["生成一个聚合每列数据的输出",{"2":{"54":1}}],["生成一个包含所有唯一词元的词汇表",{"2":{"26":1}}],["生成一个初始的预训练模型",{"2":{"10":1}}],["生成机器学习模型的嵌入",{"2":{"19":1}}],["生成和翻译方面的进步",{"2":{"16":1}}],["生成和解释能力",{"2":{"7":1}}],["生成标签",{"2":{"14":1}}],["生成式预训练",{"2":{"11":1,"14":1}}],["生成并回应类似人类文本的神经网络",{"2":{"8":1}}],["是设备的示例",{"2":{"164":1}}],["是最常用的约定",{"2":{"162":1}}],["是最受欢迎的编码器",{"2":{"95":1}}],["是任意的",{"2":{"162":1}}],["是阶为0的张量",{"2":{"147":1}}],["是指模型训练中优化的神经网络参数",{"2":{"106":1}}],["是计算上下文向量",{"2":{"99":1}}],["是计算张量均值时",{"2":{"54":1}}],["是对注意力得分进行归一化",{"2":{"99":1}}],["是机器翻译的流行选择",{"2":{"95":1}}],["是保存state",{"2":{"86":1}}],["是我们希望",{"2":{"73":1}}],["是单调的",{"2":{"63":1}}],["是显示本章中我们涵盖主题的心智模型",{"2":{"57":1}}],["是显示本章实现的不同构建模块的心智模型",{"2":{"55":1}}],["是标准高斯分布的累积分布函数",{"2":{"57":1}}],["是两个可训练参数",{"2":{"54":1}}],["是科学计数法表示的",{"2":{"54":1}}],["是相同的",{"2":{"54":1}}],["是更好的选择",{"2":{"53":1}}],["是可以由主",{"2":{"48":1}}],["是由我们之前实现的",{"2":{"48":1}}],["是在第",{"2":{"47":1}}],["是使用反向传播算法训练的深度神经网络",{"2":{"46":1}}],["是通过预测文本中的下一个词进行预训练的",{"2":{"42":1}}],["是否支持利用",{"2":{"144":1}}],["是否被识别",{"0":{"143":1}}],["是否一致",{"2":{"103":1}}],["是否具有相同维度或形状",{"2":{"89":1}}],["是否将空白字符编码为单独字符取决于应用需求",{"2":{"24":1}}],["是否移除空白字符",{"0":{"24":1}}],["是此模型的扩展版",{"2":{"14":1}}],["是因为它们在进一步微调时表现出极大的灵活性",{"2":{"13":1}}],["是公开可用的",{"2":{"13":1}}],["是",{"2":{"13":1}}],["是理解其工作机制和局限性的绝佳方式",{"2":{"10":1}}],["是深度学习技术的一个具体应用",{"2":{"8":1}}],["是一个开源库",{"2":{"171":1}}],["是一个开源的基于",{"2":{"136":1}}],["是一个有向图",{"2":{"152":1}}],["是一个支持",{"2":{"139":1}}],["是一个更广泛的概念",{"2":{"138":1}}],["是一个深度学习库",{"2":{"138":1}}],["是一个自动微分引擎",{"2":{"137":1}}],["是一个张量库",{"2":{"137":1}}],["是一个相对全面的库",{"2":{"137":1}}],["是一个继承自",{"2":{"110":1}}],["是一个嵌入",{"2":{"99":1}}],["是一个分段线性函数",{"2":{"57":1}}],["是一个小常数",{"2":{"54":1}}],["是一个",{"2":{"53":1,"162":1}}],["是一个广义的领域",{"2":{"8":1}}],["是一种替代解码算法",{"2":{"186":1}}],["是一种技术",{"2":{"121":1}}],["是一种特殊的自注意力形式",{"2":{"115":1}}],["是一种神经网络",{"2":{"95":1}}],["是一种大型深度神经网络架构",{"2":{"52":1}}],["是一种旨在理解",{"2":{"8":1}}],["是一项巨大的工程",{"2":{"15":1}}],["是近年来开发的深度神经网络模型",{"2":{"7":1}}],["1所示",{"2":{"188":1}}],["1时",{"2":{"180":1}}],["1节中定义的代码",{"2":{"180":1}}],["1节的因果注意力掩码模块中也使用过这种掩码技巧",{"2":{"82":1}}],["1示例批次如下",{"2":{"177":1}}],["1它会生成如下格式的批次",{"2":{"177":1}}],["1练习",{"2":{"177":1}}],["1返回结果为",{"2":{"177":1}}],["1返回的嵌入向量为",{"2":{"46":1}}],["1以利用芯片的计算能力",{"2":{"166":1}}],["1更改为",{"2":{"166":1}}],["1只需将张量移动到同一",{"2":{"164":1}}],["1模型的",{"2":{"162":1}}],["1模型训练的目标是使平均对数概率尽量接近",{"2":{"72":1}}],["1因为数据集中有5个训练样本",{"2":{"161":1}}],["1可以用",{"2":{"161":1}}],["1可以看到",{"2":{"34":1,"48":1}}],["1上面我们计算了训练集的预测标签",{"2":{"161":1}}],["1实际上",{"2":{"161":1}}],["1此输出包含了",{"2":{"155":1}}],["1此外",{"2":{"89":1}}],["1在下一节中",{"2":{"190":1}}],["1在深度学习中",{"2":{"155":1}}],["1在上述代码中",{"2":{"86":1}}],["1使用",{"2":{"155":1}}],["1使分布更加尖锐",{"2":{"80":1}}],["1有关",{"2":{"150":1}}],["1选择",{"2":{"150":1}}],["1d对应阶数1",{"2":{"147":1}}],["1不过",{"2":{"141":1}}],["1中进行了总结",{"2":{"137":1}}],["1结果应该为",{"2":{"164":1}}],["1结果如下",{"2":{"130":1}}],["1结果中",{"2":{"128":1}}],["1结果显示与之前计算的",{"2":{"103":1}}],["1到目前为止",{"2":{"109":1}}],["1缩放点积注意力的原理",{"2":{"108":1}}],["1至此",{"2":{"103":1}}],["1理解点积",{"2":{"99":1}}],["1现在我们定义了一个",{"2":{"157":1}}],["1现在我们可以应用温度缩放和前一节介绍的multinomial函数",{"2":{"82":1}}],["1现在",{"2":{"89":1}}],["1通过",{"2":{"89":1}}],["1通过将去词元化文本与原始输入文本进行比较",{"2":{"35":1}}],["1下载代码相对较长",{"2":{"88":1}}],["1最后",{"2":{"82":1}}],["1处理概率分数的对数在数学优化中更易操作",{"2":{"72":1}}],["1第一个数字",{"2":{"71":1}}],["1总结",{"2":{"61":1}}],["1好奇的读者可能会注意到一个不一致之处",{"2":{"60":1}}],["1我们多层神经网络模型的可训练参数包含在",{"2":{"155":1}}],["1我们在本节实现的",{"2":{"57":1}}],["1我们可以使用",{"2":{"37":1}}],["1e",{"2":{"54":1}}],["1910",{"2":{"185":1}}],["1971",{"2":{"119":1,"121":1}}],["1975",{"2":{"102":1}}],["1984",{"2":{"119":1,"121":1}}],["1983",{"2":{"119":1,"121":1}}],["1981",{"2":{"102":1}}],["1935",{"2":{"116":1,"118":1,"119":1,"121":1}}],["1921",{"2":{"116":1,"118":1}}],["1958",{"2":{"102":1}}],["1966",{"2":{"53":1}}],["194",{"2":{"52":1}}],["19",{"2":{"48":2,"115":3}}],["190",{"2":{"12":1}}],["1从上述输出可以看到",{"2":{"63":1}}],["1从",{"2":{"48":1}}],["1如上所示",{"2":{"151":1}}],["1如果机器有多个",{"2":{"164":1}}],["1如果我们从",{"2":{"150":1}}],["1如果我们将词元",{"2":{"46":1}}],["1如果返回",{"2":{"144":1}}],["1如果命令返回",{"2":{"143":1}}],["1如果你的计算机支持",{"2":{"140":1}}],["1如我们所见",{"2":{"60":1}}],["1如图",{"2":{"24":1,"48":1}}],["16x16",{"2":{"182":1}}],["1610",{"2":{"186":1}}],["1617295263",{"2":{"172":1}}],["1618",{"2":{"161":1}}],["1670",{"2":{"116":1,"118":1}}],["1666",{"2":{"116":2,"118":2,"119":2,"121":2}}],["1663",{"2":{"116":1,"118":1,"119":1,"121":1}}],["1669",{"2":{"116":1,"118":1}}],["1661",{"2":{"116":1}}],["1668",{"2":{"116":1,"118":1}}],["1667",{"2":{"116":1,"118":1}}],["1664",{"2":{"116":1}}],["1665",{"2":{"116":1}}],["16657",{"2":{"71":1}}],["1662",{"2":{"116":2,"118":1}}],["1658",{"2":{"116":1,"118":1}}],["1659",{"2":{"116":2,"118":2}}],["1652",{"2":{"116":1}}],["1656",{"2":{"99":1}}],["1646",{"2":{"116":1}}],["16mib",{"2":{"88":1}}],["16显示",{"2":{"86":1}}],["16中的章节概述所示",{"2":{"86":1}}],["16833",{"2":{"71":1}}],["1607",{"2":{"185":1}}],["1600",{"2":{"62":1,"89":1,"179":1}}],["160",{"2":{"60":1}}],["1606",{"2":{"46":1,"47":1,"185":1}}],["1631",{"2":{"130":2}}],["163",{"2":{"60":1}}],["16",{"2":{"47":2,"62":1,"63":5,"89":1,"108":2}}],["1690",{"2":{"46":1}}],["1706",{"2":{"182":1,"184":1}}],["1703",{"2":{"121":1}}],["1731",{"2":{"121":1}}],["1723",{"2":{"121":1}}],["1721",{"2":{"116":1}}],["1720",{"2":{"102":1}}],["17所述",{"2":{"89":1}}],["17所示",{"2":{"89":1}}],["17显示",{"2":{"89":1}}],["1792",{"2":{"60":1,"155":1}}],["1798",{"2":{"53":1}}],["17564",{"2":{"182":1,"186":1}}],["175",{"2":{"53":1}}],["1750",{"2":{"14":1}}],["17",{"2":{"48":2,"52":1,"63":4,"109":1}}],["1778",{"2":{"46":1}}],["1为了简化演示",{"2":{"46":1}}],["1508",{"2":{"183":1}}],["1500",{"2":{"108":1}}],["1529",{"2":{"116":1,"118":1,"119":1,"121":1}}],["1526",{"2":{"102":1}}],["1571",{"2":{"116":1,"118":1}}],["1576",{"2":{"45":2,"48":1}}],["1510",{"2":{"116":1}}],["15524",{"2":{"183":1}}],["1550",{"2":{"116":1}}],["1558m",{"2":{"89":2,"180":2}}],["15中的方法将所有未选择的logits值替换为负无穷值",{"2":{"82":1}}],["15中展示了当k=3时使用top",{"2":{"82":1}}],["15所示实现top",{"2":{"82":1}}],["15所示",{"2":{"82":1}}],["1564",{"2":{"116":1}}],["1563e",{"2":{"71":1}}],["15632",{"2":{"45":2,"48":1,"177":1}}],["1565",{"2":{"60":1,"102":1}}],["1585",{"2":{"116":1}}],["1588",{"2":{"116":1,"118":1}}],["1581",{"2":{"102":1}}],["1586",{"2":{"53":1}}],["1589",{"2":{"46":1}}],["1542",{"2":{"52":1,"116":1,"118":1,"119":1,"121":1}}],["15496",{"2":{"37":1,"63":2}}],["15",{"2":{"46":3,"53":1,"60":5,"65":1,"80":1,"88":1,"99":1,"107":1,"188":1,"190":2}}],["14135",{"2":{"184":1}}],["14165",{"2":{"182":1,"185":1}}],["1409",{"2":{"184":1}}],["1479",{"2":{"130":1}}],["1477",{"2":{"116":1}}],["14802",{"2":{"185":1}}],["1480",{"2":{"116":1}}],["1481",{"2":{"46":2,"47":1}}],["1498",{"2":{"116":1}}],["1496",{"2":{"116":1}}],["1420",{"2":{"102":1}}],["1462",{"2":{"102":1}}],["1464",{"2":{"44":4,"45":2,"48":1,"177":3}}],["1435",{"2":{"102":1}}],["1452",{"2":{"102":1}}],["1455",{"2":{"99":1}}],["14显示",{"2":{"80":2}}],["14中显示",{"2":{"80":1}}],["14所示",{"2":{"80":1}}],["14",{"2":{"44":2,"59":3,"71":1,"104":2,"105":1,"151":2}}],["1对比第一批次与第二批次可以发现",{"2":{"44":1}}],["1变量",{"2":{"44":1}}],["1810",{"2":{"182":1}}],["1811",{"2":{"172":1}}],["1814",{"2":{"53":1}}],["1805",{"2":{"186":1}}],["1801819312",{"2":{"172":1}}],["1807",{"2":{"44":3,"45":2,"48":1,"177":4}}],["1888",{"2":{"119":1,"121":1}}],["1830",{"2":{"116":1,"118":1}}],["1869",{"2":{"116":1,"118":1}}],["1820",{"2":{"108":1,"161":1}}],["18250",{"2":{"37":1}}],["1896",{"2":{"102":1}}],["1879",{"2":{"102":1}}],["184",{"2":{"76":1}}],["1858",{"2":{"60":1}}],["18",{"2":{"48":2,"63":5,"110":2,"126":1}}],["1创建下一词预测任务的输入",{"2":{"42":1}}],["13048",{"2":{"182":1}}],["1308",{"2":{"72":1}}],["13展示了如何使用这些新组件来实现我们之前实现的神经网络模型的ddp训练",{"2":{"170":1}}],["1311",{"2":{"108":1}}],["1367",{"2":{"102":1}}],["1390",{"2":{"102":1}}],["1385",{"2":{"102":2}}],["13所示的softmax概率缩放后的采样频率",{"2":{"81":1}}],["13所示",{"2":{"78":2,"169":1}}],["1324",{"2":{"54":3}}],["13",{"2":{"37":1,"42":3,"59":4,"104":3,"140":1,"159":1,"165":1,"169":1,"170":1,"175":1}}],["1这些值可以解释为类别概率",{"2":{"155":1}}],["1这将输出一个包含许多元素的大矩阵",{"2":{"155":1}}],["1这个词元化器的使用方式与我们之前实现的",{"2":{"37":1}}],["1这是因为单词",{"2":{"30":1}}],["1本章代码基于",{"2":{"37":1}}],["1输出为",{"2":{"151":6,"155":1,"157":1,"161":4}}],["1输出结果为",{"2":{"155":1}}],["1输出结果",{"2":{"102":1}}],["1输出如下",{"2":{"58":1}}],["1输出",{"2":{"35":1}}],["1输出显示结果为一个",{"2":{"47":1}}],["1输出显示",{"2":{"29":1}}],["1输出文本",{"2":{"29":1}}],["1输出表明词元化器在处理文本方面效果良好",{"2":{"25":1}}],["1接下来",{"2":{"29":1,"34":1,"99":1,"107":1,"164":1,"190":1}}],["11对此进行了说明",{"2":{"158":1}}],["1108",{"2":{"102":1}}],["1107",{"2":{"71":1}}],["111",{"2":{"78":1}}],["1110",{"2":{"53":1,"60":1}}],["11中的第7步",{"2":{"78":1}}],["11中显示的流程图描述了用于训练llm的典型pytorch神经网络训练流程",{"2":{"78":1}}],["11展示了一个典型的pytorch深度神经网络训练循环",{"2":{"78":1}}],["11所示",{"2":{"78":1}}],["11311",{"2":{"71":1}}],["1136",{"2":{"29":1}}],["1190",{"2":{"60":1}}],["11929",{"2":{"182":1}}],["1192",{"2":{"53":1}}],["1121",{"2":{"54":1}}],["11",{"0":{"172":1},"2":{"37":1,"39":2,"40":1,"45":2,"48":1,"57":2,"63":2,"72":1,"100":2,"101":1,"140":1,"158":1,"165":2}}],["116",{"2":{"190":1}}],["1160",{"2":{"32":1,"34":3}}],["1161",{"2":{"32":1}}],["1158",{"2":{"32":1}}],["1157",{"2":{"32":1}}],["1156",{"2":{"32":1}}],["1155",{"2":{"29":1,"34":1}}],["1159",{"2":{"26":1,"32":2,"34":2}}],["1文本中词元数量为",{"2":{"25":1}}],["10中的流程",{"2":{"156":1}}],["10概述了",{"2":{"156":1}}],["106",{"2":{"190":1}}],["1063",{"2":{"128":2}}],["1061e",{"2":{"71":1}}],["1036",{"2":{"121":1}}],["1077",{"2":{"99":1}}],["101",{"2":{"190":1}}],["1018",{"2":{"60":1}}],["1013",{"2":{"29":1,"34":2}}],["10866",{"2":{"182":1}}],["1080",{"2":{"155":1}}],["1081",{"2":{"128":2}}],["1082",{"2":{"102":1}}],["1085",{"2":{"60":1}}],["10899",{"2":{"45":2,"48":1,"177":3}}],["10⁻⁸",{"2":{"54":1}}],["1024",{"2":{"53":1,"61":2,"62":1,"70":2,"77":1,"89":3,"121":1,"178":1}}],["1049",{"2":{"45":2,"48":1}}],["10",{"0":{"171":1},"2":{"24":1,"31":2,"34":1,"53":1,"57":2,"63":1,"72":6,"73":1,"77":5,"78":2,"99":2,"140":1,"156":1,"161":1,"190":2}}],["1009",{"2":{"34":1}}],["1000次中有582次",{"2":{"80":1}}],["1000",{"2":{"34":1,"157":3,"180":1}}],["100",{"2":{"13":1,"57":1,"88":7,"175":1,"180":1}}],["1列表中仍包含空白字符",{"2":{"23":1}}],["1简单的词元化方案可以将示例文本拆分为单词和空白字符以及标点符号",{"2":{"23":1}}],["12裁剪后",{"2":{"190":1}}],["12更新后的代码如下",{"2":{"180":1}}],["12更常见的是使用",{"2":{"99":1}}],["12主要观察结果是训练集和验证集的表现相似",{"2":{"180":1}}],["12124m参数模型的结果为",{"2":{"180":1}}],["12练习",{"2":{"178":1,"179":1}}],["12我们也可以手动计算如下",{"2":{"174":1}}],["12我们的目标是将这篇",{"2":{"21":1}}],["12torch",{"2":{"162":1}}],["12toydataset",{"2":{"157":1}}],["12同样",{"2":{"161":1}}],["1262",{"2":{"155":1}}],["1263",{"2":{"102":1}}],["12在这种情况下",{"2":{"175":1}}],["12在gpu上执行的代码为",{"2":{"175":1}}],["12在本节中",{"2":{"161":1}}],["12在上例中",{"2":{"154":1}}],["12在自注意力机制中",{"2":{"99":1}}],["12此外",{"2":{"151":2}}],["12现在我们可以执行矩阵乘法",{"2":{"130":1}}],["12这将返回",{"2":{"150":1}}],["12这将输出",{"2":{"141":1}}],["12这将生成一个",{"2":{"122":1}}],["12这将结果",{"2":{"72":1}}],["1270",{"2":{"121":1}}],["12快速检查结果",{"2":{"107":1}}],["1290",{"2":{"121":2}}],["1295",{"2":{"102":1}}],["1296",{"2":{"53":1}}],["1220",{"2":{"102":1}}],["12归一化后的注意力权重如下",{"2":{"102":1}}],["12结果为",{"2":{"175":1}}],["12结果是",{"2":{"164":1}}],["12结果中的注意力权重矩阵有额外的元素被置零",{"2":{"122":1}}],["12结果向量内容如下",{"2":{"109":1}}],["12结果与之前一致",{"2":{"101":1}}],["12结果如下",{"2":{"60":1,"72":2}}],["1285",{"2":{"99":1}}],["12808",{"2":{"172":1}}],["1280",{"2":{"62":1,"89":1}}],["12默认情况下",{"2":{"89":1}}],["12settings",{"2":{"89":1}}],["12执行上述代码后",{"2":{"88":1}}],["12执行上述代码会产生以下错误",{"2":{"30":1}}],["12可以看到",{"2":{"82":1,"83":1}}],["12接着",{"2":{"82":1}}],["12接下来",{"2":{"53":1,"63":1,"79":1}}],["12如果这一节内容包含大量信息",{"2":{"154":1}}],["12如果模型加载成功",{"2":{"89":1}}],["12如前所述",{"2":{"151":1}}],["12如前文5",{"2":{"79":1}}],["12如输出所示",{"2":{"60":1}}],["12显示了在训练开始时",{"2":{"78":1}}],["12所示",{"2":{"78":1,"169":1,"170":1}}],["12虽然只有",{"2":{"76":1}}],["12生成的损失与之前手动应用图",{"2":{"73":1}}],["12记住",{"2":{"73":1}}],["12logits",{"2":{"73":1}}],["12llm",{"2":{"71":1}}],["12模型生成了与目标文本不同的随机文本",{"2":{"71":1}}],["12解码这些",{"2":{"71":1}}],["12由于模型尚未训练",{"2":{"77":1}}],["12由于我们有",{"2":{"71":1}}],["12由于分词器的词汇量为",{"2":{"60":1}}],["12注意",{"2":{"71":1}}],["12与这些输入相对应",{"2":{"71":1}}],["12从输出可以看出",{"2":{"70":1}}],["12从输出中可以看出",{"2":{"59":1}}],["12文本格式的模型输出如下",{"2":{"63":1}}],["12使用分词器的",{"2":{"63":1}}],["1240",{"2":{"102":1}}],["1242",{"2":{"102":2}}],["124",{"2":{"60":1}}],["124m",{"2":{"53":4,"57":3,"59":3,"60":5,"63":1,"64":1,"70":5,"77":4,"78":1,"79":1,"83":1,"86":2,"88":2,"89":7,"179":2,"180":3,"187":6,"190":2}}],["12上述",{"2":{"48":1}}],["12利用",{"2":{"46":1}}],["12第二批次的内容如下",{"2":{"44":1}}],["12通过处理输入以及作为目标的词元",{"2":{"42":1}}],["12",{"0":{"173":1},"1":{"174":1,"175":1},"2":{"42":4,"46":1,"48":1,"53":2,"58":5,"60":2,"70":2,"72":1,"89":4,"100":1,"106":1,"131":1,"132":1,"169":1,"170":1,"178":1,"180":1,"187":2}}],["123练习",{"2":{"178":1}}],["123然后可以使用以下代码重新组合原始字符串",{"2":{"177":1}}],["123接下来",{"2":{"155":1}}],["123最后",{"2":{"151":1,"155":1}}],["123类似于",{"2":{"151":1}}],["123不过",{"2":{"151":1}}],["123生成的遮罩结果如下",{"2":{"121":1}}],["123得到的归一化注意力权重矩阵中",{"2":{"119":1}}],["123得到的遮罩矩阵如下",{"2":{"117":1}}],["123由于",{"2":{"110":1}}],["123由于最大logit值位于第四个位置",{"2":{"80":1}}],["123结果的注意力权重如下",{"2":{"108":1}}],["123结果如下",{"2":{"61":1}}],["123在cpu上执行结果为",{"2":{"175":1}}],["123在类似",{"2":{"105":1}}],["123在之前的章节中讨论过",{"2":{"80":1}}],["123步骤",{"2":{"99":1}}],["123我们可以确认模型权重已正确加载",{"2":{"89":1}}],["123我们编写的这个神经网络层由一个线性层和一个非线性激活函数",{"2":{"54":1}}],["123细心的读者可能会记得我们之前使用了256",{"2":{"89":1}}],["123如第4章所述",{"2":{"86":1}}],["123温度大于1时",{"2":{"80":1}}],["123加载数据集后",{"2":{"76":1}}],["1230",{"2":{"60":1}}],["123使用上面的",{"2":{"48":1}}],["123上述代码中的",{"2":{"46":1}}],["123",{"2":{"46":2,"53":1,"54":1,"58":2,"59":1,"60":1,"70":1,"77":1,"78":1,"80":2,"83":1,"89":1,"105":1,"110":1,"121":1,"122":1,"124":1,"128":1,"131":1,"155":2,"157":2,"159":1,"165":1,"170":1,"187":2,"190":2}}],["123安装后",{"2":{"37":1}}],["123执行上述代码后得出词汇表大小为",{"2":{"26":1}}],["123输出结果为",{"2":{"174":1,"177":1}}],["123输出为",{"2":{"154":1,"155":1,"161":1}}],["123输出显示注意力权重的和为",{"2":{"99":1}}],["123输出依然是",{"2":{"80":1}}],["123输出如下",{"2":{"54":1,"107":1,"112":1}}],["123输出词元",{"2":{"37":1}}],["123输出",{"2":{"25":1}}],["1234图d",{"2":{"189":1}}],["1234第二个数据加载器的代码",{"2":{"177":1}}],["1234在深入了解使训练兼容ddp所需的更改之前",{"2":{"170":1}}],["1234在上述代码示例中",{"2":{"48":1}}],["1234",{"2":{"157":1,"177":1,"178":1,"179":1}}],["1234结果为",{"2":{"155":1}}],["1234注意",{"2":{"105":1}}],["1234接下来",{"2":{"89":1,"188":1}}],["1234然后",{"2":{"86":1}}],["1234前3个token的logits值和token",{"2":{"82":1}}],["1234使用",{"2":{"77":1}}],["1234生成的张量维度为",{"2":{"73":1}}],["1234生成的概率分数张量的维度为",{"2":{"71":1}}],["1234如我们所见",{"2":{"57":1}}],["1234现在",{"2":{"42":1}}],["1234以下输出展示了输入和输出的文本格式",{"2":{"42":1}}],["1234箭头左边的内容",{"2":{"42":1}}],["1234上述代码将打印以下内容",{"2":{"42":1}}],["1234执行上述代码后",{"2":{"42":1}}],["1234通过上面的",{"2":{"32":1}}],["12345然后",{"2":{"179":1,"180":1}}],["12345如我们所见",{"2":{"179":1}}],["12345如上输出所示",{"2":{"157":1}}],["12345例如",{"2":{"161":1}}],["12345要获得类别概率",{"2":{"161":1}}],["12345",{"2":{"149":1,"170":2}}],["12345生成的注意力得分如下",{"2":{"101":1}}],["12345结果的注意力权重如下",{"2":{"116":1}}],["12345结果为",{"2":{"99":1}}],["12345结果确认了点积的计算方式",{"2":{"99":1}}],["12345编码后的",{"2":{"63":1}}],["12345输出的上下文向量为一个",{"2":{"124":1}}],["12345输出的注意力得分为",{"2":{"99":1}}],["12345输出如下",{"2":{"58":1}}],["12345输出示例",{"2":{"27":1}}],["12345从输出中可以看到",{"2":{"58":1}}],["12345从",{"2":{"58":1}}],["12345这段代码会输出以下张量",{"2":{"54":1}}],["12345这将打印以下内容",{"2":{"45":1}}],["12345模型的输出",{"2":{"53":1}}],["12345运行上述代码将打印以下输出",{"2":{"42":1}}],["12345代码输出确认两个新添加的特殊词元已成功加入词汇表",{"2":{"32":1}}],["123456调用",{"2":{"190":1}}],["123456注解",{"2":{"170":1}}],["123456注意",{"2":{"112":1}}],["123456可以看到",{"2":{"159":1}}],["123456应用",{"2":{"121":1}}],["123456作为快速检查",{"2":{"110":1}}],["123456验证第",{"2":{"103":1}}],["123456验证所有行的总和确实为",{"2":{"102":1}}],["123456pytorch",{"2":{"99":1}}],["123456假设我们想加载最小的模型",{"2":{"89":1}}],["123456",{"2":{"86":1,"101":2,"117":1}}],["123456在上述代码中",{"2":{"165":1}}],["123456在",{"2":{"130":1}}],["123456在这个9个token的词汇表中",{"2":{"82":1}}],["123456在本节中",{"2":{"54":1,"55":1}}],["123456结果损失值如下",{"2":{"77":1}}],["123456前述代码打印了输入批次的内容和输出张量",{"2":{"60":1}}],["123456输出如下",{"2":{"59":1,"77":1,"164":1}}],["123456接下来",{"2":{"58":1}}],["123456我们可以根据结果看到",{"2":{"54":1,"55":1}}],["123456上面均值张量中的第一行包含了第一个输入行的均值",{"2":{"54":1}}],["1234567图d",{"2":{"188":1}}],["1234567此时",{"2":{"157":1}}],["1234567输出的维度由",{"2":{"131":1}}],["1234567输出的张量表示上下文向量",{"2":{"128":1}}],["1234567至此",{"2":{"121":1}}],["1234567接下来",{"2":{"121":1}}],["1234567",{"2":{"88":1,"116":1,"118":1,"119":1,"122":1,"193":1}}],["1234567采样结果如下",{"2":{"80":1}}],["12345678如果上面的代码中的所有内容都不太理解",{"2":{"152":1}}],["12345678如结果所示",{"2":{"80":1}}],["12345678下载此文件后",{"2":{"88":1}}],["12345678生成的文本如下",{"2":{"79":1}}],["123456789output",{"2":{"193":1}}],["123456789注解",{"2":{"170":1}}],["123456789这里我们使用了",{"2":{"155":1}}],["123456789步骤",{"2":{"99":1}}],["123456789生成的输出",{"2":{"63":1}}],["12345678910与第5章一样",{"2":{"190":1}}],["12345678910例如",{"2":{"127":1}}],["12345678910生成的文本如下",{"2":{"83":1,"89":1}}],["12345678910生成的两条文本的",{"2":{"53":1}}],["12345678910执行train",{"2":{"78":1}}],["12345678910接下来",{"2":{"57":1}}],["12345678910注意",{"2":{"54":1}}],["12345678910输出张量包含两行",{"2":{"53":1}}],["1234567891011在上面的代码中",{"2":{"190":1}}],["1234567891011注解",{"2":{"170":1}}],["1234567891011我们可以打印模型参数的损失梯度值",{"2":{"154":1}}],["1234567891011从训练过程中打印的结果可以看出",{"2":{"78":1}}],["1234567891011此代码片段演示了使用",{"2":{"63":1}}],["123456789101112在macbook",{"2":{"190":1}}],["123456789101112接下来",{"2":{"80":1}}],["123456789101112evaluate",{"2":{"78":1}}],["123456789101112如上代码所示",{"2":{"57":1}}],["12345678910111213输出张量",{"2":{"128":1}}],["12345678910111213默认情况下",{"2":{"77":1}}],["1234567891011121314接下来",{"2":{"187":1}}],["1234567891011121314代码清单中的函数对数据加载器进行迭代",{"2":{"161":1}}],["1234567891011121314pythontrain",{"2":{"157":1}}],["1234567891011121314",{"2":{"80":1,"156":1}}],["1234567891011121314可以看到",{"2":{"60":1}}],["123456789101112131415通过上面的",{"2":{"28":1}}],["12345678910111213141516运行上述代码后",{"2":{"188":1}}],["12345678910111213141516实例化训练数据加载器后",{"2":{"157":1}}],["1234567891011121314151617初始化模型后",{"2":{"187":1}}],["1234567891011121314151617我们可以如下实例化一个新的神经网络对象",{"2":{"155":1}}],["123456789101112131415161718以上代码中使用了相对较小的批次大小",{"2":{"77":1}}],["12345678910111213141516171819output",{"2":{"194":1}}],["1234567891011121314151617181920再次绘制学习率图以验证学习率的变化",{"2":{"189":1}}],["1234567891011121314151617181920注解",{"2":{"170":1}}],["1234567891011121314151617181920在这段",{"2":{"110":1}}],["1234567891011121314151617181920212223现在",{"2":{"187":1}}],["123456789101112131415161718192021222324运行上述代码将输出以下结果",{"2":{"165":1}}],["12345678910111213141516171819202122232425请注意",{"2":{"78":1}}],["1234567891011121314151617181920212223242526",{"2":{"123":1}}],["1234567891011121314151617181920212223242526得益于我们在第",{"2":{"60":1}}],["123456789101112131415161718192021222324252627282930313233343536373839虽然",{"2":{"129":1}}],["1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950定义好train",{"2":{"190":1}}],["123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354在load",{"2":{"89":1}}],["12345678910111213141516171819202122232425262728293031323334353637383940在这段代码中",{"2":{"53":1}}],["1234567891011121314151617181920212223242526272829303132这段代码定义了一个",{"2":{"59":1}}],["123456789101112131415161718192021222324该代码实现了一个由",{"2":{"58":1}}],["12345678910111213141516171819202122运行代码将得到以下输出",{"2":{"159":1}}],["12345678910111213141516171819202122现在让我们看看新的generate函数的效果",{"2":{"83":1}}],["12345678910111213141516171819202122使用以上代码",{"2":{"70":1}}],["1234567891011121314151617181920代码示例",{"2":{"43":1}}],["12345678910111213141516171819请注意",{"2":{"45":1}}],["12345678910111213141516可以像",{"2":{"112":1}}],["12345678910111213141516生成的训练和验证损失图如图5",{"2":{"78":1}}],["12345678910111213141516在",{"2":{"70":1}}],["12345678910111213141516从图",{"2":{"57":1}}],["12345678910111213141516与上一节中实现的",{"2":{"33":1}}],["12345678910111213141516",{"2":{"0":1}}],["12345678910111213在上面的代码中",{"2":{"58":1}}],["12345678910111213这种层归一化实现针对输入张量",{"2":{"54":1}}],["1234567891011可以看到",{"2":{"48":1}}],["123456789在",{"2":{"53":1}}],["1234567与evaluate",{"2":{"78":1}}],["1234567现在我们可以使用",{"2":{"77":1}}],["1234567如代码输出所示",{"2":{"77":1}}],["1234567每个批次的",{"2":{"71":1}}],["1234567最后",{"2":{"71":1}}],["1234567上述代码将打印以下输出",{"2":{"48":1}}],["1234567可以看到",{"2":{"46":1}}],["1234567执行上述代码将打印以下内容",{"2":{"44":1}}],["123456让我们使用批量大小为",{"2":{"44":1}}],["123456如上所示",{"2":{"27":1}}],["1234输出显示约一半的值被置零",{"2":{"121":1}}],["1234输出显示",{"2":{"106":1}}],["1234输出查询结果为",{"2":{"106":1}}],["1234输出如下",{"2":{"76":1,"161":1}}],["1234输出结果如下",{"2":{"54":1}}],["1234输出矩阵中的每一行是通过从嵌入权重矩阵中查找得到的",{"2":{"47":1}}],["1234输出词元",{"2":{"29":1}}],["1234输出",{"2":{"23":1,"24":1,"34":1}}],["1234输出示例",{"2":{"21":1}}],["12输出为",{"2":{"143":1,"150":2,"151":1,"154":1,"155":1,"157":2,"161":2}}],["12输出的每一行包含一个",{"2":{"103":1}}],["12输出形状如下",{"2":{"73":1}}],["12输出如下",{"2":{"48":1,"60":1,"89":2}}],["12输出文本",{"2":{"37":1}}],["12输出词元",{"2":{"34":1}}],["12输出",{"2":{"23":1,"32":1}}],["12输出结果显示每行的和为",{"2":{"121":1}}],["12输出结果显示",{"2":{"118":1}}],["12输出结果",{"2":{"23":1}}],["120",{"2":{"12":1}}],["1",{"0":{"8":2,"9":1,"10":1,"11":1,"12":1,"14":1,"15":1,"16":1,"19":1,"21":1,"40":1,"52":1,"61":1,"69":1,"70":2,"71":1,"75":1,"80":1,"81":1,"95":1,"99":1,"101":1,"102":1,"110":1,"113":1,"126":1,"136":1,"137":2,"138":1,"139":1,"145":1,"149":1,"164":1,"177":1,"182":1,"188":1},"1":{"13":1,"53":1,"70":1,"71":1,"72":2,"73":2,"74":2,"75":1,"76":2,"77":2,"81":1,"137":1,"138":1,"139":1,"140":2,"141":2,"142":2,"143":2,"144":2,"145":1,"146":1},"2":{"8":7,"9":2,"10":4,"11":7,"12":6,"13":2,"14":7,"15":2,"16":1,"18":2,"23":1,"25":1,"27":1,"28":1,"29":2,"33":1,"37":2,"42":4,"43":2,"44":3,"45":2,"46":9,"47":6,"48":2,"51":2,"52":2,"53":18,"54":21,"55":7,"57":4,"58":8,"59":1,"60":10,"61":2,"62":1,"63":5,"65":1,"67":3,"70":2,"71":19,"72":7,"73":2,"77":2,"78":5,"79":1,"80":10,"82":1,"83":4,"86":1,"88":4,"89":4,"94":2,"95":2,"96":1,"99":18,"101":16,"102":9,"104":5,"105":2,"106":2,"107":8,"108":2,"109":3,"110":2,"112":1,"115":2,"116":1,"117":21,"119":3,"121":6,"122":1,"123":3,"124":2,"126":1,"127":1,"128":1,"129":6,"130":10,"137":1,"138":2,"141":5,"149":6,"150":2,"151":5,"152":3,"154":3,"155":3,"156":9,"157":13,"161":10,"164":10,"165":1,"172":1,"175":1,"177":1,"178":2,"179":3,"180":2,"187":1,"188":4,"189":4,"190":6}}],["类用于实例化",{"2":{"157":1}}],["类用于实例化对象",{"2":{"156":1}}],["类标签编号",{"0":{"157":1}}],["类对象",{"2":{"149":1}}],["类采取了整合的方法",{"2":{"129":1}}],["类在一个类中集成了多头功能",{"2":{"129":1}}],["类独立执行注意力机制",{"2":{"129":1}}],["类时",{"2":{"124":1}}],["类支持第",{"2":{"122":1}}],["类中加载训练示例的代码而定",{"2":{"158":1}}],["类中没有这些组件",{"2":{"160":1}}],["类中没有",{"2":{"130":1}}],["类中的重塑",{"2":{"129":1}}],["类中",{"2":{"88":1,"109":1,"122":1,"129":1}}],["类中实例化一个新的词元化器对象",{"2":{"29":1}}],["类和函数",{"2":{"170":1}}],["类和",{"2":{"70":1}}],["类和代码示例",{"2":{"44":1}}],["类实现",{"2":{"65":1,"155":1}}],["类来采样数据集",{"2":{"157":1}}],["类来定义自定义的网络结构",{"2":{"155":1}}],["类来堆叠多个先前实现的",{"2":{"126":1}}],["类来实现",{"2":{"62":1}}],["类来替换这些占位符",{"2":{"60":1}}],["类的三个主要组成部分是",{"2":{"157":1}}],["类的典型用法",{"2":{"155":1}}],["类的代码",{"2":{"129":1}}],["类的",{"2":{"60":1}}],["类的一般结构和用法",{"2":{"43":1}}],["类显得相对简洁紧凑",{"2":{"60":1}}],["类结构不熟悉",{"2":{"43":1}}],["类基于",{"2":{"43":1}}],["类模型中使用的自注意力机制",{"2":{"96":1}}],["类模型的预训练任务通常是预测句子中的下一个词",{"2":{"16":1}}],["类模型的预训练成本可能高达数千甚至数百万美元",{"2":{"15":1}}],["类模型",{"2":{"14":1}}],["类似于键值对中的值",{"2":{"109":1}}],["类似于数据库中的键",{"2":{"109":1}}],["类似于数据库中的搜索查询",{"2":{"109":1}}],["类似于我们手动定义的",{"2":{"89":1}}],["类似地",{"2":{"104":2}}],["类似",{"2":{"11":1,"37":1}}],["类",{"0":{"131":1,"148":1},"2":{"7":1,"15":1,"18":1,"28":1,"31":1,"42":1,"43":1,"46":1,"59":1,"60":1,"65":1,"96":1,"104":1,"109":1,"124":1,"127":1,"128":2,"132":1,"155":1,"156":2,"157":1}}],["从数据加载到在自定义数据集上进行微调",{"2":{"196":1}}],["从头开始构建大型语言模型",{"2":{"196":1}}],["从头开始编写一个",{"2":{"10":1}}],["从结果可以看出训练函数有效地降低了训练集的损失",{"2":{"190":1}}],["从高层次来说",{"2":{"154":1}}],["从计算角度来看",{"2":{"147":1}}],["从历史数据中学习的算法",{"2":{"138":1}}],["从单头注意力扩展到多头注意力",{"0":{"125":1},"1":{"126":1,"127":1,"128":1,"129":1,"130":1,"131":1,"132":1}}],["从简化的注意力机制开始",{"2":{"124":1}}],["从注意力得分到注意力权重",{"0":{"108":1}}],["从一个简化的自注意力版本开始",{"2":{"94":1}}],["从网络获取文件",{"2":{"88":1}}],["从9",{"2":{"78":1}}],["从如何加载数据",{"2":{"71":1}}],["从初始输入上下文中构建连贯的短语和句子",{"2":{"63":1}}],["从底部开始",{"2":{"60":1}}],["从第",{"2":{"48":1}}],["从原则上讲",{"2":{"48":1}}],["从",{"0":{"88":1},"1":{"89":1,"90":1,"91":1},"2":{"47":1,"67":1,"157":1}}],["从嵌入层的权重矩阵中检索与词元",{"2":{"47":1}}],["从嵌入层权重矩阵中检索行的查找操作",{"2":{"46":1}}],["从输入数据集中创建多个批次时",{"2":{"44":1}}],["从中提取输入块作为",{"2":{"42":1}}],["从新文本示例开始",{"2":{"27":1}}],["从零开始实现一个",{"0":{"50":1},"1":{"51":1,"52":1,"53":1,"54":1,"55":1,"56":1,"57":1,"58":1,"59":1,"60":1,"61":1,"62":1,"63":1,"64":1,"65":1}}],["从零开始计数",{"2":{"46":1}}],["从零开始预训练一个大型",{"2":{"15":1}}],["从零构建大型语言模型",{"2":{"196":1}}],["从零构建",{"2":{"7":1}}],["从左至右的处理方式非常适合文本生成和下一词预测任务",{"2":{"14":1}}],["从而进一步提高模型训练效果",{"2":{"188":1}}],["从而在特定任务上超越通用llm",{"2":{"182":1}}],["从而实现并行计算",{"2":{"169":1}}],["从而实现了在",{"2":{"137":1}}],["从而引发错误或",{"2":{"158":1}}],["从而引入生成文本的多样性和创造性",{"2":{"63":1}}],["从而释放主进程专注于训练模型",{"2":{"158":1}}],["从而训练模型",{"2":{"152":1}}],["从而有效地将所有头的输出组合起来",{"2":{"130":1}}],["从而有助于更有效的训练",{"2":{"58":1}}],["从而减少过拟合",{"2":{"121":1}}],["从而减少模型重复选择最可能token的概率",{"2":{"82":1}}],["从而改善训练效果",{"2":{"108":1}}],["从而失去生成连贯文本的能力",{"2":{"86":1}}],["从而生成更具新意的文本",{"2":{"78":1}}],["从而最小化训练集损失",{"2":{"78":1}}],["从而确保模型始终选择目标",{"2":{"71":1}}],["从而确保更准确的上下文感知预测",{"2":{"48":1}}],["从而获得对应的",{"2":{"71":1}}],["从而更好地理解模型生成下一个最有可能",{"2":{"63":1}}],["从而更加擅长处理人类语言的细微差别和复杂性",{"2":{"8":1}}],["从而使multinomial函数的行为接近argmax",{"2":{"80":1}}],["从而使得深度神经网络的操作更加便捷",{"2":{"58":1}}],["从而使模型更具可扩展性",{"2":{"57":1}}],["从而难以有效地训练较早的层",{"2":{"58":1}}],["从而难以做出准确的预测或决策",{"2":{"54":1}}],["从而避免从",{"2":{"54":1}}],["从而为训练过程中参数的更新提供依据",{"2":{"153":1}}],["从而为",{"2":{"48":1}}],["从而提升生成文本的连贯性",{"2":{"14":1}}],["从而降低计算资源需求",{"2":{"13":1}}],["从而能够将新邮件分类为垃圾或正常",{"2":{"8":1}}],["从本章开始",{"2":{"7":1}}],["彻底改变了",{"2":{"7":1}}],["架构针对",{"2":{"150":1}}],["架构设置",{"2":{"88":1,"89":1}}],["架构在输出层重复使用了",{"2":{"60":1}}],["架构在整个网络中处理序列数据时不会改变它们的形状",{"2":{"59":1}}],["架构所需的所有构建模块",{"2":{"59":1}}],["架构所需的最后一个构建模块",{"2":{"58":1}}],["架构所需的一个构建模块",{"2":{"55":1}}],["架构时实现的不同构建模块的心智模型",{"2":{"55":1}}],["架构中导入使用",{"2":{"104":1}}],["架构中的核心组成部分",{"2":{"94":1}}],["架构中不可或缺的部分",{"2":{"94":1}}],["架构中",{"2":{"54":1,"60":1,"94":1}}],["架构及其输入输出",{"2":{"53":1}}],["架构和数据准备过程",{"2":{"18":1}}],["架构和数据预处理过程",{"2":{"15":1}}],["架构包含用于解析文本的编码器和用于生成文本的解码器",{"2":{"16":1}}],["架构仅使用原始",{"2":{"14":1}}],["架构相对简单",{"2":{"14":1}}],["架构相比",{"2":{"14":1}}],["架构由两个子模块组成",{"2":{"11":1}}],["架构",{"0":{"52":1},"1":{"53":1},"2":{"11":4,"16":1,"53":2,"55":1,"61":1,"63":1,"67":1,"96":2,"104":1,"121":1}}],["架构上进行预训练或微调",{"2":{"10":1}}],["架构并借助大规模数据训练的模型",{"2":{"7":1}}],["架构的每个",{"2":{"97":1}}],["架构的现代",{"2":{"96":1}}],["架构的各个位置设置",{"2":{"64":1}}],["架构的总体概览",{"2":{"60":1}}],["架构的主要组件",{"2":{"59":1}}],["架构的其他部分",{"2":{"59":1}}],["架构的基础构建模块",{"2":{"59":1}}],["架构的基本结构",{"2":{"14":1}}],["架构的步骤顺序",{"2":{"53":2}}],["架构的仅解码器",{"2":{"18":1}}],["架构的解码器部分",{"2":{"11":1}}],["架构的后续变体包括",{"2":{"11":1}}],["架构的简化示意图",{"2":{"11":1}}],["架构的",{"2":{"7":1,"126":1}}],["架构的支持",{"2":{"7":1}}],["架构的深入分析",{"0":{"14":1},"2":{"7":1}}],["二是庞大的训练数据",{"2":{"7":1}}],["这本书的构思已久",{"2":{"196":1}}],["这只需一行代码",{"2":{"190":1}}],["这适用于需要生成更可预测",{"2":{"180":1}}],["这有几种可能的解释",{"2":{"180":1}}],["这有助于防止神经网络在训练过程中陷入重复更新周期",{"2":{"157":1}}],["这有时也被称为前馈层或全连接层",{"2":{"155":1}}],["这有时会使优化变得更难",{"2":{"57":1}}],["这为通过梯度下降等方法更新每个参数以最小化损失函数提供了必要的信息",{"2":{"154":1}}],["这涉及开发能够识别模式",{"2":{"138":1}}],["这涉及将文本分割为单词或子词",{"2":{"94":1}}],["这听起来有些混乱",{"2":{"138":1}}],["这与我们之前在2",{"2":{"175":1}}],["这与标准自注意力机制不同",{"2":{"115":1}}],["这与传统的注意力机制不同",{"2":{"98":1}}],["这会导致两种机制生成不同的结果",{"2":{"113":1}}],["这会生成一个存储六个上下文向量的矩阵",{"2":{"110":1}}],["这会生成一个概率分布",{"2":{"82":1}}],["这三个矩阵用于将嵌入的输入标记",{"2":{"104":1}}],["这张图展示了注意力机制的概念",{"2":{"96":1}}],["这可以带来更稳定的训练动态",{"2":{"185":1}}],["这可以为生成的文本添加更多变化",{"2":{"80":1}}],["这可能导致上下文丢失",{"2":{"95":1}}],["这称为多头注意力",{"2":{"133":1}}],["这称为",{"2":{"92":1}}],["这使得生成过程可以探索那些概率较低但潜在更有趣和富有创造性的路径",{"2":{"82":1}}],["这使得模型能够学习适合数据处理的适当缩放和偏移值",{"2":{"54":1}}],["这主要用于教育目的",{"2":{"76":1}}],["这相当于模型在",{"2":{"74":1}}],["这不仅有助于衡量生成文本的质量",{"2":{"71":1}}],["这说明了模型训练在生成连贯文本中的重要性",{"2":{"65":1}}],["这两个函数便于在文本和",{"2":{"70":1}}],["这两个嵌入层和输出层非常大",{"2":{"60":1}}],["这两个层的权重张量形状相同",{"2":{"60":1}}],["这两种位置嵌入的目的是增强",{"2":{"48":1}}],["这由",{"2":{"60":1}}],["这就是为什么这些连接也称为跳跃连接",{"2":{"58":1}}],["这实现方式是将某一层的输出与后面某一层的输出相加",{"2":{"58":1}}],["这对于需要理解句子中词语之间关系的",{"2":{"99":1}}],["这对分布式训练或在资源受限的环境中部署模型尤其有利",{"2":{"56":1}}],["这对应于列的维度",{"2":{"54":1}}],["这已经可以作为",{"2":{"48":1}}],["这个定制的llm在金融任务上超越了chatgpt",{"2":{"182":1}}],["这个token",{"2":{"180":1}}],["这个网络有2个输入和2个输出",{"2":{"174":1}}],["这个输出投影层不是严格必须的",{"2":{"130":1}}],["这个单词被采样的频率是多少",{"2":{"81":1}}],["这个损失函数衡量模型预测偏离目标值的程度",{"2":{"72":1}}],["这个块重复了",{"2":{"60":1}}],["这个",{"2":{"59":1,"60":1,"155":1}}],["这个关键特性有助于在训练期间让梯度在网络中顺畅流动",{"2":{"59":1}}],["这个神经网络可以适应输入中不同的批次大小和",{"2":{"57":1}}],["这个值非常接近",{"2":{"54":1}}],["这个优化过程是模型训练的一部分",{"2":{"48":1}}],["这个权重矩阵有",{"2":{"46":1}}],["这个词汇表定义了如何将每个独特的单词和特殊字符映射到唯一的整数",{"2":{"26":1}}],["这将显著加速深度神经网络的训练过程",{"2":{"163":1}}],["这将是接下来几节的主题",{"2":{"152":1}}],["这将在下一节中探索",{"2":{"128":1}}],["这将在第三章中讨论",{"2":{"19":1}}],["这将为我们提供一个总体视图",{"2":{"53":1}}],["这将提供额外的直观性和清晰度",{"2":{"43":1}}],["这提醒我们",{"2":{"30":1}}],["这里",{"2":{"190":1}}],["这里是",{"2":{"152":1}}],["这里是数据集中的",{"2":{"73":1}}],["这里设置",{"2":{"105":1}}],["这里仅存在细微的差别",{"2":{"104":1}}],["这里所指的权重是指存储在",{"2":{"88":1}}],["这里简化为值",{"2":{"58":1}}],["这里每一层的初始化保证它接受一个包含",{"2":{"58":1}}],["这里我们将",{"2":{"45":1}}],["这里为简单化",{"2":{"24":1}}],["这里的关键是",{"2":{"95":1}}],["这里的",{"2":{"10":1,"162":1}}],["这也是本章将反复使用的技术",{"2":{"70":1}}],["这也是图",{"2":{"19":1}}],["这也解释了为什么输出结果中不包含任何负值",{"2":{"54":1}}],["这也为我们提供了所需的知识",{"2":{"10":1}}],["这包括将文本分解为单词和子词词元",{"2":{"18":1}}],["这在模型开发的实验阶段尤为关键",{"2":{"168":1}}],["这在模型训练过程中是非常重要的一步",{"2":{"75":1}}],["这在实验",{"2":{"154":1}}],["这在需要计算梯度时非常有用",{"2":{"153":1}}],["这在训练",{"2":{"124":1}}],["这在复杂任务中显著提升了模型性能",{"2":{"114":1}}],["这在改进深度神经网络架构的训练性能方面非常重要",{"2":{"57":1}}],["这在",{"2":{"16":1,"18":1,"130":1}}],["这表明模型仍在学习",{"2":{"78":1}}],["这表明",{"2":{"14":1,"59":1}}],["这类架构在规模上远超原始",{"2":{"14":1}}],["这类模型的设计初衷是适用于各种应用场景",{"2":{"10":1}}],["这类模型能够借助语言的顺序特性来学习文本的上下文",{"2":{"8":1}}],["这类模型通常包含数百亿甚至上千亿的参数",{"2":{"8":1}}],["这样",{"2":{"158":1,"169":1}}],["这样可以确保每个模型副本具有相同的更新权重",{"2":{"169":1}}],["这样可以在训练过程中打破对称性",{"2":{"155":1}}],["这样可以禁用模型中的dropout层",{"2":{"86":1}}],["这样可以得到稍微更有趣的文本片段",{"2":{"42":1}}],["这样我们在新会话中使用它时",{"2":{"86":1}}],["这样做简单且高效",{"2":{"77":1}}],["这样的生成模型是如何一次生成一个单词",{"2":{"63":1}}],["这样的解码器模型通过逐词预测生成文本",{"2":{"14":1}}],["这样复杂的智能助手的工作机制",{"2":{"9":1}}],["这种变体仅通过输入的均方根进行归一化",{"2":{"185":1}}],["这种时间效率会随着gpu数量的增加而线性扩展",{"2":{"169":1}}],["这种随机输出在未经训练的模型中是预期的",{"2":{"155":1}}],["这种放大至关重要",{"2":{"121":1}}],["这种自注意力机制也被称为缩放点积注意力",{"2":{"104":1}}],["这种记忆现象是意料之中的",{"2":{"78":1}}],["这种调整使adamw实现了更有效的正则化和更好的泛化",{"2":{"78":1}}],["这种调整加快了有效权重的收敛速度",{"2":{"54":1}}],["这种权重更新通过一种称为反向传播的过程实现",{"2":{"72":1}}],["这种方法结合了大量图示",{"2":{"196":1}}],["这种方法受到众多读者的认可",{"2":{"196":1}}],["这种方法可以防止模型过拟合",{"2":{"121":1}}],["这种方法的主要缺陷在于",{"2":{"96":1}}],["这种方法的一个缺点是",{"2":{"82":1}}],["这种方法将使我们能够在整个训练过程中监控并提升模型的性能",{"2":{"70":1}}],["这种方式确保在每次迭代中",{"2":{"161":1}}],["这种方式称为贪婪解码",{"2":{"80":1}}],["这种方式通常导致训练动态变差",{"2":{"59":1}}],["这种方式也称为",{"2":{"59":1}}],["这种方式被称为有偏方差估计",{"2":{"55":1}}],["这种方式使得模型可以更好地泛化到不同长度的序列",{"2":{"48":1}}],["这种设计使得它能够有效应用于各种序列到序列任务中",{"2":{"59":1}}],["这种设计允许模型探索更丰富的表示空间",{"2":{"57":1}}],["这种组合不仅可以更细致地理解和处理输入",{"2":{"59":1}}],["这种现象称为梯度消失问题",{"2":{"58":1}}],["这种扩展后接一个非线性的",{"2":{"57":1}}],["这种将未知单词拆解为子词或字符的能力确保了词元化器以及基于该词元化器训练的",{"2":{"39":1}}],["这种大型生成式语言模型的优势在于",{"2":{"14":1}}],["这种能力并非在训练中被显式教授",{"2":{"14":1}}],["这种抽样方法意味着模型并未训练所有数据",{"2":{"13":1}}],["这种规模和多样化的数据集使这些模型在多种任务上表现优异",{"2":{"12":1}}],["这种独特的训练方式使",{"2":{"11":1}}],["这种机制使模型能够捕捉长距离依赖和上下文关系",{"2":{"11":1}}],["这种由预训练和微调组成的双阶段训练流程如图",{"2":{"10":1}}],["这是如何工作的呢",{"2":{"169":1}}],["这是神经网络的主要训练算法",{"2":{"152":1}}],["这是神经网络训练的重要环节",{"2":{"135":1}}],["这是神经网络中的标准激活函数",{"2":{"54":1}}],["这是一本关于技术主题的书",{"2":{"191":1}}],["这是一个全连接神经网络",{"2":{"155":1}}],["这是一个追踪和分析研究论文的平台",{"2":{"136":1}}],["这是一款基于",{"2":{"135":1}}],["这是一种增强llm训练稳定性的关键技术",{"2":{"190":1}}],["这是一种支持多种机器学习算法的科学计算框架",{"2":{"142":1}}],["这是一种为下一个token生成任务添加概率选择过程的技术",{"2":{"80":1}}],["这是一种性能和效率的权衡",{"2":{"19":1}}],["这是一种结合生成",{"2":{"19":1}}],["这是通过实例化并组合多个",{"2":{"129":1}}],["这是下一章",{"2":{"124":1}}],["这是在后续章节中开发大型语言模型",{"2":{"115":1}}],["这是我们将在下一节中实现的最终注意力模块",{"2":{"122":1}}],["这是我们在第",{"2":{"95":1}}],["这是我们编写",{"2":{"58":1}}],["这是模型学习的标志",{"2":{"78":1}}],["这是优化",{"2":{"67":1}}],["这是为什么呢",{"2":{"80":1}}],["这是为什么",{"2":{"63":1}}],["这是训练深度神经网络的标准技术",{"2":{"72":1}}],["这是训练",{"2":{"49":1}}],["这是因为ddp需要生成多个进程",{"2":{"170":1}}],["这是因为上一节的玩具数据集有两个输入特征和两个类别标签",{"2":{"159":1}}],["这是因为共有5个训练示例",{"2":{"157":1}}],["这是因为计算机表示数值时的精度有限",{"2":{"54":1}}],["这是因为",{"2":{"38":1,"54":1,"151":1}}],["这是已进入公共领域的文本",{"2":{"20":1}}],["这是创建",{"2":{"20":1}}],["这是本章的核心内容",{"2":{"18":1}}],["这是",{"2":{"18":1,"51":1,"55":1,"59":1,"110":1}}],["这是开发高效",{"2":{"15":1}}],["这是用于语言翻译的深度学习模型",{"2":{"11":1}}],["这是2017年论文",{"2":{"11":1}}],["这是许多实际应用和研究中最常见的任务",{"2":{"15":1}}],["这是许多",{"2":{"7":1}}],["这一计算方法同样适用于矩阵",{"2":{"190":1}}],["这一过程确保模型参数在反向传播期间的更新幅度保持在可控范围内",{"2":{"190":1}}],["这一过程通过逐渐将学习率从非常低的初始值",{"2":{"188":1}}],["这一行在与保存模型相同的会话中执行此代码时并非严格必要",{"2":{"162":1}}],["这一节主要是为了说明",{"2":{"154":1}}],["这一名称承认了该库起源于",{"2":{"142":1}}],["这一特性使我们能够方便高效地使用反向传播",{"2":{"135":1}}],["这一特性意味着在训练过程中",{"2":{"57":1}}],["这一类将作为实现多头注意力的模板",{"2":{"122":1}}],["这一主题可能需要相当多的专注",{"2":{"97":1}}],["这一主题超出本书范围",{"2":{"72":1}}],["这一策略在第",{"2":{"71":1}}],["这一初始化是",{"2":{"46":1}}],["这一方法帮助模型理解词汇和短语如何在语言中自然地组合在一起",{"2":{"14":1}}],["这一将文本转化为词元的过程",{"2":{"12":1}}],["这一点将在下一节",{"2":{"10":1}}],["这一预训练模型成为后续微调的基础",{"2":{"10":1}}],["这意味着梯度乘以学习率后",{"2":{"160":1}}],["这意味着从输出层",{"2":{"153":1}}],["这意味着不需要手动确保这些张量与模型参数位于同一设备",{"2":{"124":1}}],["这意味着不需要专家去识别和选择最相关的特征供深度学习模型使用",{"2":{"8":1}}],["这意味着",{"2":{"79":1,"80":1,"96":1}}],["这意味着在输入转换为输出时保留顺序",{"2":{"63":1}}],["这意味着在计算方差时",{"2":{"55":1}}],["这意味着虽然序列的物理维度",{"2":{"59":1}}],["这意味着学习过程难以找到一组参数",{"2":{"54":1}}],["这意味着需要人类专家识别并选择最相关的特征供模型使用",{"2":{"8":1}}],["这些方法共同有助于稳定llm的训练",{"2":{"190":1}}],["这些方法在需要复杂理解和生成能力的语言任务上往往表现不佳",{"2":{"7":1}}],["这些函数处于测试阶段",{"2":{"184":1}}],["这些梯度会在训练期间被平均并同步",{"2":{"169":1}}],["这些副本在训练期间会保持同步",{"2":{"169":1}}],["这些设置用于将模型置于训练或评估模式下",{"2":{"160":1}}],["这些属性可能是文件路径",{"2":{"157":1}}],["这些是可选的",{"2":{"141":1}}],["这些隐藏层能够对数据中的复杂",{"2":{"138":1}}],["这些深度神经网络最初的灵感来源于人脑的工作方式",{"2":{"138":1}}],["这些任务包括理解自然语言",{"2":{"138":1}}],["这些组件在图a",{"2":{"137":1}}],["这些组件将在后续章节中详细实现",{"2":{"53":1}}],["这些向量会被重塑",{"2":{"130":1}}],["这些可训练权重会得到调整",{"2":{"110":1}}],["这些可训练的权重矩阵至关重要",{"2":{"104":1}}],["这些矩阵将输入数据转换为查询",{"2":{"110":1}}],["这些不同的注意力变体如图",{"2":{"94":1}}],["这些大小不同的",{"2":{"89":1}}],["这些概率在第",{"2":{"72":1}}],["这些权重通常存储在线性层中",{"2":{"68":1}}],["这些权重也称为权重参数",{"2":{"68":1}}],["这些权重在训练过程中进行调整",{"2":{"52":1}}],["这些步骤包括解码输出张量",{"2":{"63":1}}],["这些块堆叠在一起",{"2":{"60":1}}],["这些块内部包含上一章实现的掩码多头注意力模块",{"2":{"52":1}}],["这些我们已经在本章前面介绍了",{"2":{"59":1}}],["这些问题会导致训练动态不稳定",{"2":{"54":1}}],["这些嵌入层负责将输入",{"2":{"60":1}}],["这些嵌入被添加到词元嵌入向量中",{"2":{"49":1}}],["这些嵌入在训练过程中被优化",{"2":{"48":1}}],["这些词元被转换为整数表示",{"2":{"49":1}}],["这些词元随后使用词汇表转换为词元",{"2":{"48":1}}],["这些词元可以是单词或字符",{"2":{"49":1}}],["这些词元可以是单词或标点符号",{"2":{"20":1}}],["这些词元可以是单词或特殊字符",{"2":{"20":1}}],["这些值将在",{"2":{"46":1}}],["这些目标通过将输入向右移一位来创建",{"2":{"42":1}}],["这些输入向量被表示为",{"2":{"99":1}}],["这些输入",{"2":{"42":1}}],["这些",{"2":{"31":1,"57":1,"63":1,"70":1}}],["这些特殊词元可以包括未知单词的标记和文档边界等",{"2":{"31":1}}],["这些预训练模型之所以被称为基础或底层模型",{"2":{"13":1}}],["这些替代架构的主要动机是提高",{"2":{"11":1}}],["这些模型可以进一步微调",{"2":{"18":1}}],["这些模型不仅在文本补全上表现优异",{"2":{"14":1}}],["这些模型擅长执行零样本学习和少样本学习任务",{"2":{"11":1}}],["这些模型是通过对海量文本数据进行训练而成",{"2":{"8":1}}],["这些将在接下来的章节中逐步实现和讲解",{"2":{"11":1}}],["这些表示捕捉了输入的上下文信息",{"2":{"11":1}}],["这些数据通常被称为",{"2":{"10":1}}],["这些参数是神经网络中的可调权重",{"2":{"8":1}}],["这些都是人工编码难以实现的",{"2":{"7":1}}],["早期模型通常为特定任务设计",{"2":{"7":1}}],["早期的语言模型无法从关键词列表中生成一封完整的邮件",{"2":{"7":1}}],["情感分析",{"2":{"7":1,"9":1}}],["在调用",{"2":{"190":1}}],["在一种常见的变体中",{"2":{"189":1}}],["在一个通用文本数据集上预训练",{"2":{"51":1}}],["在gpt",{"2":{"185":1}}],["在generate",{"2":{"80":1}}],["在原始transformer模型中应用于自注意力和前馈网络之后",{"2":{"185":1}}],["在主章节中",{"2":{"180":1}}],["在温度为0或0",{"2":{"180":1}}],["在v100",{"2":{"175":1}}],["在我的实验中",{"2":{"175":1}}],["在我们的实现中",{"2":{"89":1}}],["在我们的训练循环中",{"2":{"78":1}}],["在我们的方差计算方法中",{"2":{"55":1}}],["在我们实现层归一化的代码之前",{"2":{"54":1}}],["在某些情况下",{"2":{"172":1}}],["在cpu或单gpu上训练模型是最简单的",{"2":{"171":1}}],["在pytorch中使用clip",{"2":{"190":1}}],["在pytorch中",{"2":{"171":1}}],["在ddp中",{"2":{"169":2}}],["在进行修改之前",{"2":{"164":1}}],["在进入下一节",{"2":{"151":1}}],["在进入本章最后两个部分之前",{"2":{"45":1}}],["在评估模型预测之前",{"2":{"159":1}}],["在未使用多个工作进程",{"2":{"158":1}}],["在预测或推理时",{"2":{"155":1}}],["在预训练阶段",{"2":{"18":1}}],["在反向传播计算梯度时会使用该信息",{"2":{"155":1}}],["在计算损失时将其与真实标签",{"2":{"152":1}}],["在计算注意力后再组合这些头的结果",{"2":{"129":1}}],["在计算注意力得分时",{"2":{"115":1}}],["在计算注意力得分",{"2":{"108":1}}],["在20步后达到最大值",{"2":{"189":1}}],["在2",{"2":{"147":1}}],["在你的计算机上安装并设置",{"2":{"145":1}}],["在撰写本文时",{"2":{"141":2,"143":1}}],["在推理阶段",{"2":{"138":1}}],["在推理期间",{"2":{"86":1}}],["在电子邮件垃圾邮件分类器的情况下",{"2":{"138":1}}],["在继续之前",{"2":{"138":1}}],["在继续下一节之前",{"2":{"60":1}}],["在易用性和功能之间实现了理想的平衡",{"2":{"136":1}}],["在简化的注意力机制中",{"2":{"133":1}}],["在3个",{"2":{"159":1}}],["在3",{"2":{"126":1}}],["在因果注意力中",{"2":{"115":1}}],["在类似",{"2":{"115":1}}],["在注意力机制中",{"2":{"109":1}}],["在自注意力计算的最后一步",{"2":{"109":1}}],["在自注意力中",{"2":{"98":1,"99":1,"110":1}}],["在具有可训练权重矩阵的自注意力机制的第一步中",{"2":{"104":1}}],["在解码阶段",{"2":{"95":1}}],["在解析和理解非结构化文本数据方面的强大能力",{"2":{"9":1}}],["在编码器",{"2":{"95":1}}],["在深入自注意力机制之前",{"2":{"95":1}}],["在深度学习模型中允许随机正则化和非线性",{"2":{"185":1}}],["在深度学习的背景下",{"2":{"152":1}}],["在深度学习中",{"2":{"72":1,"121":1}}],["在深度学习和",{"2":{"52":1}}],["在深度神经网络中添加捷径连接",{"2":{"51":1}}],["在之前的内容中",{"2":{"88":1}}],["在之前的generate",{"2":{"80":1}}],["在新的python会话或jupyter",{"2":{"87":1}}],["在保存权重后",{"2":{"87":1}}],["在通过state",{"2":{"86":1}}],["在通用文本数据集上预训练",{"2":{"67":1}}],["在top",{"2":{"82":1}}],["在更详细地讨论验证集损失之前",{"2":{"78":1}}],["在macbook",{"2":{"78":1}}],["在给定示例中",{"2":{"74":1}}],["在应用遮罩后重新归一化注意力权重",{"2":{"120":1}}],["在应用",{"2":{"73":1,"121":1}}],["在模型训练中",{"2":{"71":1}}],["在模型架构上与",{"2":{"53":1}}],["在从左到右读取和生成文本的",{"2":{"133":1}}],["在从",{"2":{"71":1}}],["在以下代码示例中",{"2":{"71":1}}],["在初始化一个层之后",{"2":{"68":1}}],["在无标签数据上进行预训练",{"0":{"66":1},"1":{"67":1,"68":1,"69":1,"70":1,"71":1,"72":1,"73":1,"74":1,"75":1,"76":1,"77":1,"78":1,"79":1,"80":1,"81":1,"82":1,"83":1,"84":1,"85":1,"86":1,"87":1,"88":1,"89":1,"90":1,"91":1,"92":1}}],["在上述",{"2":{"63":1}}],["在上一节的玩具数据集上训练一个神经网络",{"2":{"159":1}}],["在上一节中",{"2":{"26":2,"31":1,"63":1,"82":1,"100":1,"104":1,"129":1,"150":1,"152":1,"156":1,"162":1}}],["在上一章中",{"2":{"18":1,"51":1,"94":1}}],["在包含概率分数的向量中",{"2":{"63":1}}],["在每个gpu上",{"2":{"169":1}}],["在每个训练迭代中",{"2":{"169":1}}],["在每个输出向量与对应的输入向量之间保持一一对应的关系",{"2":{"59":1}}],["在每次循环中",{"2":{"78":1}}],["在每次迭代中",{"2":{"63":2}}],["在每次迭代中预测下一个",{"2":{"63":1}}],["在每一步中",{"2":{"63":1}}],["在开始之前",{"2":{"63":1}}],["在开发简单词元化器时",{"2":{"24":1}}],["在不进行代码修改的情况下",{"2":{"62":1}}],["在穿过",{"2":{"59":1}}],["在图",{"2":{"59":1,"63":1,"99":1}}],["在介绍了捷径连接之后",{"2":{"58":1}}],["在下面的代码示例中",{"2":{"58":1,"121":1}}],["在下一章",{"2":{"94":1}}],["在下一章中",{"2":{"51":1,"63":1,"96":1}}],["在下一阶段",{"2":{"41":1}}],["在下一节中",{"2":{"30":1,"55":1,"57":1,"59":1,"60":1,"61":1,"71":1,"74":1,"82":1,"114":1,"188":1}}],["在向后传播过程中逐渐减小",{"2":{"58":1}}],["在零点有一个陡峭的拐角",{"2":{"57":1}}],["在代码中组装",{"2":{"60":1}}],["在代码中",{"2":{"57":1,"59":1,"63":1,"82":1,"126":1}}],["在实践中",{"2":{"160":1}}],["在实践中试用",{"2":{"55":1}}],["在实际训练中",{"2":{"157":1}}],["在实际中",{"2":{"77":1,"158":1}}],["在实际的",{"2":{"76":1}}],["在实际操作中",{"2":{"63":1,"109":1}}],["在实现预训练代码后",{"2":{"13":1}}],["在执行均值或方差计算时",{"2":{"54":1}}],["在诸如均值或方差计算的操作中使用",{"2":{"54":1}}],["在对这些输出应用层归一化之前",{"2":{"54":1}}],["在对大型文本数据集进行下一词预测训练以获得预训练的",{"2":{"10":1}}],["在后续的微调",{"2":{"70":1}}],["在后续代码示例中将用到此配置",{"2":{"53":1}}],["在后续章节中",{"2":{"27":1}}],["在单个",{"2":{"53":1}}],["在输入处理管道的一部分中",{"2":{"48":1}}],["在这个子类中",{"2":{"155":1}}],["在这个上下文中",{"2":{"147":1}}],["在这一节中",{"2":{"124":1}}],["在这一过程中",{"2":{"89":1}}],["在这3个非零概率分数中选择下一个token",{"2":{"82":1}}],["在这种情况下",{"2":{"80":1,"89":1,"138":1,"150":1}}],["在这种情况下我们需要截断文本",{"2":{"48":1}}],["在这些平台上可以按小时成本运行",{"2":{"143":1}}],["在这些技术中",{"2":{"63":1}}],["在这些情况下提供了更大的灵活性和稳定性",{"2":{"56":1}}],["在这两个组件的前面各应用了层归一化",{"2":{"59":1}}],["在这里为",{"2":{"60":1}}],["在这里",{"2":{"48":1,"80":1,"121":1}}],["在高效数据加载器的实现中",{"2":{"42":1}}],["在将词元转换为嵌入之前",{"2":{"42":1}}],["在创建",{"2":{"42":1}}],["在前向传播中",{"2":{"110":1}}],["在前向传播过程中",{"2":{"58":1}}],["在前面的章节中",{"2":{"109":1}}],["在前几章中",{"2":{"52":1,"67":1}}],["在前几节中",{"2":{"37":1,"155":1}}],["在前一节中",{"2":{"42":1,"48":1,"153":1}}],["在批量输入的训练中通常会使用掩码",{"2":{"36":1}}],["在批量训练时确保所有文本长度相同",{"2":{"36":1}}],["在构建",{"2":{"30":1}}],["在此示例中",{"2":{"24":1}}],["在此阶段",{"2":{"16":1}}],["在使用google",{"2":{"175":1}}],["在使用小型数据集或交互式环境",{"2":{"158":1}}],["在使用",{"2":{"19":1,"166":1}}],["在定制数据集上微调的",{"2":{"16":1}}],["在逐词生成输出时能选择性访问整个输入序列",{"2":{"16":1}}],["在较小的标注目标数据集上进行微调",{"2":{"16":1}}],["在大型文本语料库上预训练llm需要大量时间和资源",{"2":{"92":1}}],["在大型语言模型出现之前",{"2":{"7":1}}],["在大多数情况下被采样",{"2":{"80":1}}],["在大量未标注文本上预训练",{"2":{"16":1}}],["在第",{"2":{"15":2,"46":1,"52":1,"60":1,"63":1,"96":1}}],["在接下来的两个小节中定义深度学习并安装",{"2":{"137":1}}],["在接下来的小节中",{"2":{"125":1,"136":1}}],["在接下来的部分中",{"2":{"20":1,"104":1}}],["在接下来的内容中",{"2":{"15":1}}],["在接下来的章节中",{"2":{"8":1,"59":1,"78":1,"86":1,"88":1,"89":1,"115":1}}],["在本附录的最后一节中",{"2":{"190":1}}],["在本附录中",{"2":{"187":1}}],["在本节的其余部分中",{"2":{"88":1}}],["在本节的其余部分",{"2":{"72":1}}],["在本节中",{"2":{"26":1,"31":1,"42":1,"46":1,"54":1,"57":1,"59":1,"60":1,"61":1,"70":1,"78":1,"79":1,"82":1,"104":1,"128":1,"168":1,"169":1,"190":1}}],["在本章结束时",{"2":{"76":1}}],["在本章末尾我们将更新上下文大小设置",{"2":{"70":1}}],["在本章开始时",{"2":{"64":1}}],["在本章的剩余部分",{"2":{"124":1}}],["在本章的最后部分",{"2":{"125":1}}],["在本章的最后一节中",{"2":{"63":1,"163":1}}],["在本章的最后两部分中",{"2":{"45":1}}],["在本章的结尾",{"2":{"53":1}}],["在本章中",{"2":{"18":1,"53":1,"62":1,"86":1,"94":2,"96":1}}],["在本章前面",{"2":{"14":1}}],["在本书中",{"2":{"9":1,"10":1,"196":1}}],["在表",{"2":{"12":1}}],["在训练数据集上遍历",{"2":{"157":1}}],["在训练数据集中",{"2":{"157":1}}],["在训练数据中所占比例",{"2":{"12":1}}],["在训练模型之前",{"2":{"156":1}}],["在训练模型时我们通过",{"2":{"88":1}}],["在训练",{"2":{"121":1}}],["在训练和检查模型后",{"2":{"86":1}}],["在训练完成后",{"2":{"79":1}}],["在训练中的梯度消失问题",{"2":{"65":1}}],["在训练中",{"2":{"42":1}}],["在训练过程中",{"2":{"92":1}}],["在训练过程中的文本生成质量",{"2":{"67":1}}],["在训练过程中预测该输入块之后的下一个词",{"2":{"42":1}}],["在训练过程中会被优化以预测下一个词",{"2":{"8":1}}],["在文本分类任务",{"2":{"11":1}}],["在分类微调中",{"2":{"10":1}}],["在指令微调中",{"2":{"10":1}}],["在微调阶段",{"2":{"10":1}}],["在性能上往往优于通用",{"2":{"10":1}}],["在几乎所有涉及解析和生成文本的自动化任务中都展现出极高的价值",{"2":{"9":1}}],["在医学",{"2":{"9":1}}],["在传统机器学习中",{"2":{"8":1}}],["在",{"0":{"59":1,"86":1,"144":1},"1":{"87":1},"2":{"8":1,"14":2,"22":1,"30":1,"53":1,"54":2,"57":1,"60":2,"68":2,"73":1,"95":2,"96":1,"98":1,"104":1,"113":1,"121":1,"124":3,"127":1,"128":1,"129":2,"135":1,"141":1,"143":2,"151":2,"155":2,"157":2,"158":1,"160":1,"164":3,"165":1}}],["在翻译任务中",{"2":{"11":1}}],["在翻译",{"2":{"7":1}}],["捕捉比以往方法更丰富的上下文信息和语言微妙之处",{"2":{"7":1}}],["而destroy",{"2":{"170":1}}],["而测试集通常只在最终评估时使用一次",{"2":{"160":1}}],["而如果我们希望训练",{"2":{"138":1}}],["而能够适应新的输入或变化的环境",{"2":{"138":1}}],["而根据",{"2":{"136":1}}],["而新的",{"2":{"129":1}}],["而注意力权重是动态的",{"2":{"106":1}}],["而解码器则生成翻译后的文本",{"2":{"95":1}}],["而剩余token的概率之和为1",{"2":{"82":1}}],["而验证集损失则停滞不前",{"2":{"78":1}}],["而generate",{"2":{"78":1}}],["而我们",{"2":{"71":1}}],["而我们希望这些标点符号能够作为独立的列表项",{"2":{"23":1}}],["而在第",{"2":{"60":1}}],["而在深度学习中则无需手动提取特征",{"2":{"8":1}}],["而前馈网络则在每个位置单独地对数据进行修改",{"2":{"59":1}}],["而层归一化在特征维度上进行归一化",{"2":{"56":1}}],["而第二行列出第二行的层输出",{"2":{"54":1}}],["而言",{"2":{"53":1,"55":1}}],["而这是第二批次输入的第一个",{"2":{"44":1}}],["而这是当代",{"2":{"7":1}}],["而不在平方之前减去均值",{"2":{"185":1}}],["而不必在",{"2":{"155":1}}],["而不会关注当前标记之后的未来标记",{"2":{"115":1}}],["而不仅仅是第",{"2":{"100":1}}],["而不仅是",{"2":{"80":1}}],["而不需数周",{"2":{"76":1}}],["而不需要在它们之间调整维度",{"2":{"57":1}}],["而不需要为每种任务开发特定模型",{"2":{"14":1}}],["而不考虑它在输入序列中的位置",{"2":{"48":1}}],["而不是注意力层",{"2":{"185":1}}],["而不是在jupyter界面中运行",{"2":{"170":1}}],["而不是在本章中讨论如何通过",{"2":{"88":1}}],["而不是预测标签",{"2":{"138":1}}],["而不是手动实现",{"2":{"111":1}}],["而不是多个输入的批量",{"2":{"110":1}}],["而不是常见的贝塞尔校正的",{"2":{"55":1}}],["而不是像原始",{"2":{"48":1}}],["而不是词元的绝对位置",{"2":{"48":1}}],["而不是",{"2":{"46":1,"54":1,"71":1}}],["而不是具备真正的理解力或意识",{"2":{"7":1}}],["而不提供具体示例的任务则称为零样本设置",{"2":{"11":1}}],["而少样本学习则是从用户提供的少量示例中学习",{"2":{"11":1}}],["而无需手动计算梯度",{"2":{"171":1}}],["而无需手动实现梯度计算的数学运算",{"2":{"58":1}}],["而无需消耗大量时间和计算资源",{"2":{"88":1}}],["而无需重新训练",{"2":{"11":1}}],["而无需大量训练数据",{"2":{"10":1}}],["而无需显式编程",{"2":{"8":1,"138":1}}],["而是在训练过程中用来计算损失函数相对于模型参数的梯度",{"2":{"155":1}}],["而是重点讲解我们将在本书中用于实现",{"2":{"135":1}}],["而是需要考虑上下文理解和语法匹配",{"2":{"95":1}}],["而是将负的平均对数概率降低到",{"2":{"72":1}}],["而是模型在大量多语言和多样化上下文数据的接触中自然产生的",{"2":{"14":1}}],["而是利用数据本身的结构",{"2":{"14":1}}],["而是从每个数据集中抽取一个总量为",{"2":{"13":1}}],["而是使用已标注为垃圾邮件和正常邮件的示例来训练算法",{"2":{"8":1}}],["而是指它能够生成符合逻辑",{"2":{"7":1}}],["而机器学习和深度学习则致力于开发算法",{"2":{"8":1}}],["而",{"2":{"7":1,"11":2,"13":2,"14":2,"53":1,"57":1,"73":1,"129":1,"138":1,"162":1,"164":1}}],["具体代码示例",{"0":{"128":1}}],["具体计算将在稍后讨论",{"2":{"99":1}}],["具体来说",{"2":{"78":1}}],["具体选择哪种方法通常取决于具体的应用和所处理数据的特性",{"2":{"48":1}}],["具体词元的选择便不再重要",{"2":{"36":1}}],["具体见代码示例",{"2":{"32":1}}],["具体如图",{"2":{"15":1}}],["具有两个隐藏层的多层感知机",{"2":{"155":1}}],["具有两个隐藏层的多层感知机示意图",{"2":{"155":1}}],["具有多个变量作为输入",{"2":{"154":1}}],["具有相同维度的嵌入层",{"2":{"48":1}}],["具有",{"2":{"48":1,"54":1}}],["具有机器学习背景的读者可能会注意到",{"2":{"16":1}}],["具有更多的参数和更大的训练数据集",{"2":{"14":1}}],["具有类似人类的意识",{"2":{"7":1}}],["具备惊人的语言理解",{"2":{"7":1}}],["如tensorflow",{"2":{"196":1}}],["如今能将它呈现给您",{"2":{"196":1}}],["如llm",{"2":{"188":1}}],["如线性预热和余弦退火",{"2":{"186":1}}],["如小说创作",{"2":{"180":1}}],["如技术分析",{"2":{"180":1}}],["如单个模型无法完全放入一个gpu",{"2":{"172":1}}],["如单词或图像",{"2":{"49":1}}],["如单词",{"2":{"19":1}}],["如代码清单a",{"2":{"170":1}}],["如您所见",{"2":{"164":1}}],["如创建多个工作进程的开销甚至可能超过实际的数据加载时间",{"2":{"158":1}}],["如右侧所示",{"2":{"158":1}}],["如左侧所示",{"2":{"158":1}}],["如上面代码清单",{"2":{"157":1}}],["如上一节图",{"2":{"96":1}}],["如句中的每个词",{"2":{"109":1}}],["如句子中的词或标记",{"2":{"109":1}}],["如句子中的词",{"2":{"99":1}}],["如句子中的单词或图像中的像素",{"2":{"98":1}}],["如同",{"2":{"109":1}}],["如英语",{"2":{"95":1}}],["如adamw",{"2":{"86":1}}],["如0",{"2":{"80":1}}],["如dropout",{"2":{"79":1}}],["如前所述",{"2":{"78":1,"141":1,"149":1}}],["如前述的图",{"2":{"63":1}}],["如打印损失和生成文本样本",{"2":{"78":1}}],["如学习率预热",{"2":{"78":1}}],["如我们在第",{"2":{"59":1}}],["如第",{"2":{"59":1,"60":1,"99":1}}],["如原始",{"2":{"59":1}}],["如矩阵",{"2":{"54":1}}],["如顶部所示",{"2":{"54":1}}],["如底部所示",{"2":{"54":1}}],["如下",{"2":{"53":1,"63":2}}],["如下所示",{"2":{"42":1,"53":1,"57":1,"82":1,"89":1,"109":1,"149":1,"150":1,"151":1,"154":1,"155":1,"157":3,"174":1,"179":1}}],["如果范数超过1",{"2":{"190":1}}],["如果the",{"2":{"180":2}}],["如果想了解更多关于机器学习模型评估的内容",{"2":{"172":1}}],["如果有多个gpu",{"2":{"171":1}}],["如果使用八个gpu",{"2":{"169":1}}],["如果再次遍历数据集",{"2":{"157":1}}],["如果类标签为",{"2":{"157":1}}],["如果没有",{"2":{"143":1}}],["如果返回",{"2":{"143":1}}],["如果最新的",{"2":{"140":1}}],["如果需要保持每次初始化的随机数一致",{"2":{"155":1}}],["如果需要可以通过打印整个字典查看或通过相应的字典键选择个别张量",{"2":{"89":1}}],["如果需要有关",{"2":{"42":1}}],["如果下载代码不起作用",{"2":{"89":1}}],["如果计划稍后继续预训练模型",{"2":{"86":1}}],["如果模型能够学习生成训练集和验证集中的下一个",{"2":{"77":1}}],["如果在generate",{"2":{"80":1}}],["如果在",{"2":{"77":1,"153":1}}],["如果好奇的话",{"2":{"55":1}}],["如果你想了解更多关于梯度累积的信息",{"2":{"172":1}}],["如果你想了解更全面的深度学习入门资料",{"2":{"172":1}}],["如果你想浏览",{"2":{"151":1}}],["如果你是",{"2":{"148":1}}],["如果你有一台配备",{"2":{"144":1}}],["如果你需要有关设置",{"2":{"142":1}}],["如果你的计算机有一个",{"2":{"140":1}}],["如果你已经熟悉深度学习",{"2":{"135":1}}],["如果你熟悉批量归一化",{"2":{"56":1}}],["如果你不熟悉或记不清偏导数",{"2":{"154":1}}],["如果你不熟悉梯度和神经网络训练的概念",{"2":{"58":1}}],["如果你不熟悉",{"2":{"54":1}}],["如果你对神经网络训练和梯度的概念还不熟悉",{"2":{"54":1}}],["如果一个神经网络层的权重矩阵为",{"2":{"52":1}}],["如果我们生成多个训练进程",{"2":{"170":1}}],["如果我们想要计算类别概率",{"2":{"155":1}}],["如果我们训练",{"2":{"138":1}}],["如果我们在此函数中出错",{"2":{"89":1}}],["如果我们尝试匹配两个形状不同的张量",{"2":{"89":1}}],["如果我们有一个维度为",{"2":{"54":1}}],["如果我们有",{"2":{"48":1}}],["如果我们将这些梯度裁剪到max",{"2":{"190":1}}],["如果我们将",{"2":{"44":1}}],["如果您不熟悉神经网络是如何通过反向传播进行训练的",{"2":{"46":1}}],["如果您有深度学习的经验",{"2":{"45":1}}],["如果您对使用pytorch训练深度神经网络不熟悉",{"2":{"78":1}}],["如果您对",{"2":{"43":1}}],["如果",{"2":{"44":1,"165":1}}],["如果词嵌入是二维的",{"2":{"19":1}}],["如问号",{"2":{"24":1}}],["如何在实际中发挥作用",{"2":{"154":1}}],["如何处理未知单词",{"0":{"39":1}}],["如何最有效地将文本拆分为词元列表",{"2":{"23":1}}],["如何有效地拆分文本",{"0":{"23":1}}],["如一本书",{"2":{"22":1}}],["如搜索外部知识库",{"2":{"19":1}}],["如生成文本",{"2":{"19":1}}],["如视频",{"2":{"19":1}}],["如分类",{"2":{"16":1}}],["如情感预测",{"2":{"11":1}}],["如图d",{"2":{"188":1,"189":1}}],["如图a",{"2":{"138":1,"141":1,"143":1,"147":1,"152":1,"155":1,"169":2}}],["如图5",{"2":{"78":3,"82":1,"86":1,"89":2}}],["如图所示",{"2":{"77":1}}],["如图",{"2":{"8":1,"10":1,"11":2,"14":3,"18":1,"19":2,"20":1,"26":1,"27":1,"28":1,"31":2,"39":1,"42":2,"44":1,"46":1,"47":1,"48":2,"51":1,"52":1,"53":1,"54":2,"57":2,"58":2,"59":4,"60":2,"63":4,"67":2,"71":3,"76":1,"77":1,"94":2,"95":2,"96":2,"98":1,"99":4,"100":1,"101":1,"104":1,"105":1,"108":1,"110":1,"115":2,"121":2,"127":1,"129":1}}],["如",{"2":{"7":1,"8":1,"13":1,"19":1,"24":1,"36":1,"37":1,"38":1,"41":1,"49":2,"52":1,"58":1,"59":1,"63":3,"78":1,"89":1,"99":2,"121":1,"140":1,"144":1,"147":1,"158":1,"166":1}}],["的熟悉会有所帮助",{"2":{"196":1}}],["的论文介绍了一种层归一化技术",{"2":{"185":1}}],["的llm架构示例",{"2":{"182":1}}],["的机器上也可运行",{"2":{"165":1}}],["的习惯有助于避免在更复杂模型中出现意外行为",{"2":{"160":1}}],["的随机梯度下降",{"2":{"159":1}}],["的情况下",{"2":{"158":1}}],["的一般结构",{"2":{"157":1}}],["的一个子类别",{"2":{"138":1}}],["的一个子领域",{"2":{"138":1}}],["的一个小缺陷是其自注意力机制",{"2":{"48":1}}],["的梯度会在所有gpu上同步",{"2":{"169":1}}],["的梯度",{"2":{"152":1,"153":1,"154":1}}],["的梯度仍然大于其他层",{"2":{"58":1}}],["的语法",{"2":{"151":1}}],["的语法惯例",{"2":{"151":1}}],["的所有张量操作和命令",{"2":{"151":1}}],["的所有上下文向量",{"2":{"104":1}}],["的新手",{"2":{"148":1}}],["的版本",{"2":{"141":1}}],["的安装推荐",{"2":{"141":1}}],["的安装方式类似于其他",{"2":{"139":1}}],["的命令",{"2":{"141":1}}],["的官网",{"2":{"141":1}}],["的默认命令如下",{"2":{"140":1}}],["的精简版本",{"2":{"140":1}}],["的综合库",{"2":{"139":1}}],["的工作流程与图a",{"2":{"138":1}}],["的工作原理",{"2":{"9":1}}],["的能力",{"2":{"138":1}}],["的发展中发挥了关键作用",{"2":{"138":1}}],["的发展方向",{"2":{"7":1}}],["的这三个核心组件",{"2":{"137":1}}],["的这种位置无关的确定性嵌入有助于可重复性",{"2":{"48":1}}],["的三大核心组件包括作为计算基础的张量库",{"2":{"137":1}}],["的三个主要组件之一",{"2":{"152":1}}],["的三个主要阶段的思维模型",{"2":{"94":1}}],["的三个主要阶段的心智模型",{"2":{"67":1}}],["的三个主要阶段",{"2":{"18":1,"51":1}}],["的三个核心组件",{"0":{"137":1}}],["的三个阶段",{"2":{"15":1}}],["的受访者约占",{"2":{"136":1}}],["的深度学习库",{"2":{"136":1}}],["的自动微分引擎",{"2":{"135":1,"152":1}}],["的自注意力机制称为缩放点积注意力",{"2":{"133":1}}],["的自注意力机制本身对位置不敏感",{"2":{"48":1}}],["的流行深度学习库",{"2":{"135":1}}],["的因子进行放大",{"2":{"121":1}}],["的全为",{"2":{"121":1}}],["的全称是",{"2":{"14":1}}],["的位置的注意力权重",{"2":{"115":1}}],["的权重矩阵转移到",{"2":{"113":1}}],["的权重尚未公开",{"2":{"53":1}}],["的优势在于",{"2":{"111":1}}],["的实现",{"2":{"111":1}}],["的实现中",{"2":{"8":1}}],["的类",{"2":{"110":1}}],["的矩阵乘法获得",{"2":{"104":1}}],["的信息",{"2":{"99":1}}],["的上下文向量",{"2":{"99":1,"104":1}}],["的句子",{"2":{"99":1}}],["的重要性或贡献由注意力权重",{"2":{"99":1}}],["的关键环节之一",{"2":{"97":1}}],["的关键组成部分",{"2":{"96":1}}],["的关键组成部分是自注意力机制",{"2":{"11":1}}],["的剩余部分",{"2":{"96":1}}],["的其余部分代码",{"2":{"96":1}}],["的其他构建模块",{"2":{"51":1}}],["的局限性促使了注意力机制的设计",{"2":{"95":1}}],["的读者",{"2":{"95":1}}],["的主要问题在于",{"2":{"95":1}}],["的内部运作",{"2":{"95":1}}],["的内容",{"2":{"89":1}}],["的指令",{"2":{"88":1}}],["的批次大小达到",{"2":{"77":1}}],["的粗略成本约为",{"2":{"76":1}}],["的成本",{"0":{"76":1}}],["的超过",{"2":{"75":1}}],["的初始",{"2":{"71":1}}],["的初版则是在",{"2":{"14":1}}],["的概率生成",{"2":{"80":1}}],["的概率得分会更高",{"2":{"80":1}}],["的概率",{"2":{"71":1}}],["的概率分布的向量",{"2":{"70":1}}],["的概念",{"2":{"58":1,"60":1,"164":1}}],["的线性层",{"2":{"68":1}}],["的文本生成功能通过顺序预测下一个",{"2":{"65":1}}],["的向量",{"2":{"63":1}}],["的向量表示",{"2":{"59":1}}],["的单步操作",{"2":{"63":1}}],["的非归一化概率",{"2":{"60":1}}],["的输出",{"2":{"63":1}}],["的输出投影到词汇空间中",{"2":{"60":1}}],["的输出映射到高维空间",{"2":{"60":1}}],["的输入参数",{"2":{"128":1}}],["的输入示例",{"2":{"71":1}}],["的输入批次",{"2":{"57":1}}],["的输入通常是一个占位符向量",{"2":{"48":1}}],["的输入",{"2":{"48":1}}],["的输入子样本",{"2":{"42":1}}],["的输入文本",{"2":{"18":1}}],["的后续层中",{"2":{"59":1}}],["的后续版本",{"2":{"13":1}}],["的推进",{"2":{"58":1}}],["的大部分构建模块",{"2":{"57":1}}],["的大型语言模型",{"2":{"51":1}}],["的值",{"2":{"57":1}}],["的平滑特性可以在训练过程中带来更好的优化特性",{"2":{"57":1}}],["的结果中可以看到",{"2":{"57":1}}],["的结构",{"2":{"18":1,"51":1}}],["的差异几乎可以忽略不计",{"2":{"55":1}}],["的最后一个维度操作",{"2":{"54":1}}],["的占位符架构",{"2":{"53":1}}],["的惯例",{"2":{"53":1}}],["的隐藏单元被丢弃",{"2":{"53":1}}],["的对比",{"0":{"53":1},"1":{"55":1,"56":1}}],["的维度",{"2":{"52":1}}],["的核心构建模块",{"2":{"58":1}}],["的核心组件之一",{"2":{"51":1}}],["的核心理念是出现在相似上下文中的词语往往具有相似的含义",{"2":{"19":1}}],["的数量也是一个需要选择的超参数",{"2":{"159":1}}],["的数值张量",{"2":{"61":1}}],["的数据用于验证",{"2":{"77":1}}],["的数据",{"2":{"53":1}}],["的数据加载器进行采样",{"2":{"45":1}}],["的数据加载器",{"0":{"45":1}}],["的数字序列",{"2":{"48":1}}],["的张量库类似于numpy等数组库",{"2":{"171":1}}],["的张量库",{"2":{"159":1}}],["的张量和自动微分组件",{"2":{"155":1}}],["的张量应用",{"2":{"121":1}}],["的张量输出可以看出",{"2":{"48":1}}],["的张量",{"2":{"48":1,"63":1}}],["的词汇表的",{"2":{"71":1}}],["的词汇表大小",{"2":{"48":1}}],["的词元",{"2":{"48":1}}],["的词嵌入技术后",{"2":{"19":1}}],["的人来说",{"2":{"47":1}}],["的嵌入大小在初始化权重时是固定的",{"2":{"57":1}}],["的嵌入大小为",{"2":{"57":2}}],["的嵌入向量",{"2":{"99":1}}],["的嵌入向量是嵌入层权重矩阵的第六行",{"2":{"47":1}}],["的嵌入向量与前面的嵌入矩阵对比",{"2":{"46":1}}],["的嵌入",{"2":{"46":1}}],["的嵌入之前的最后一步是生成用于训练的输入",{"2":{"42":1}}],["的示例适用于演示目的",{"2":{"45":1}}],["的含义",{"2":{"44":1}}],["的过拟合",{"2":{"133":1}}],["的过程",{"2":{"42":1,"63":1}}],["的过滤子集",{"2":{"13":1}}],["的开源",{"2":{"37":1}}],["的模型中",{"2":{"105":1}}],["的模型",{"2":{"37":1,"51":1}}],["的正则表达式库",{"2":{"23":1}}],["的标准文件读取工具加载它",{"2":{"20":1}}],["的短篇小说的词元赋值给一个名为",{"2":{"26":1}}],["的短篇小说分割为独立的词元",{"2":{"26":1}}],["的短篇小说",{"2":{"20":1,"25":1,"75":1}}],["的训练",{"2":{"138":1}}],["的训练代码时",{"2":{"63":1}}],["的训练分为两个主要步骤",{"2":{"16":1}}],["的训练方式不同",{"2":{"11":1}}],["的出现引入了新的深度学习驱动方法",{"2":{"16":1}}],["的特定调整参考",{"2":{"14":1}}],["的解码器部分",{"2":{"14":1}}],["的代码问答",{"2":{"13":1}}],["的预训练",{"2":{"67":1,"77":1}}],["的预训练权重时将重新探讨该设置",{"2":{"53":1}}],["的预训练权重",{"2":{"53":1}}],["的预训练代码",{"2":{"13":1}}],["的预训练成本估计约为",{"2":{"13":1}}],["的预训练数据集",{"2":{"12":1}}],["的预训练涉及在大规模文本数据集上进行下一词预测",{"2":{"10":1}}],["的计算效率",{"2":{"11":1}}],["的区别",{"2":{"11":1}}],["的编码器模块之上",{"2":{"11":1}}],["的缩写",{"2":{"11":2,"52":1}}],["的翻译",{"2":{"11":1}}],["的第一章",{"2":{"183":1}}],["的第一个训练阶段也称为预训练",{"2":{"10":1}}],["的第一步是将其训练在一个庞大的文本语料库上",{"2":{"10":1}}],["的延伸阅读和参考资料部分",{"2":{"10":1}}],["的总体架构",{"2":{"8":1}}],["的架构并不复杂",{"2":{"52":1}}],["的架构",{"2":{"8":1}}],["的基石",{"2":{"97":1}}],["的基本原理",{"0":{"41":1}}],["的基本思路为蓝图",{"2":{"15":1}}],["的基本流程包括预训练和微调",{"2":{"10":1}}],["的基本概念高级解释",{"2":{"7":1}}],["的基础模型",{"2":{"18":1}}],["的基础上",{"2":{"14":1}}],["的基础",{"2":{"7":1,"18":1}}],["的步骤规划",{"2":{"7":1}}],["的",{"0":{"111":1,"148":1},"2":{"7":2,"9":1,"11":3,"13":1,"14":3,"19":1,"26":1,"37":2,"42":1,"43":4,"44":1,"48":1,"49":1,"53":3,"54":2,"57":1,"58":1,"59":2,"60":2,"63":1,"72":1,"73":1,"88":1,"111":1,"115":1,"117":1,"121":2,"133":1,"135":1,"141":1,"144":1,"149":1,"152":1,"154":3,"155":1,"157":3,"161":1,"164":1}}],["等复杂模式识别非常重要",{"2":{"126":1}}],["等模型",{"2":{"121":1}}],["等模型的大型数据集包含了数十亿词汇的多样化文本语料库",{"2":{"12":1}}],["等不同的文本",{"2":{"80":1}}],["等框架中",{"2":{"68":1}}],["等人在论文",{"2":{"52":1}}],["等人撰写",{"2":{"14":1}}],["等深度学习模型的关键",{"2":{"49":1}}],["等英语单词中很常见",{"2":{"41":1}}],["等主流",{"2":{"18":1}}],["等流行",{"2":{"18":1}}],["等生成文本和执行指令的",{"2":{"16":1}}],["等更新的架构仍然基于相同的核心概念",{"2":{"14":1}}],["等传统搜索引擎的功能",{"2":{"9":1}}],["等",{"2":{"7":1,"41":1,"164":1}}],["lrs",{"2":{"188":3,"189":3,"190":4}}],["lr",{"2":{"188":15,"189":15,"190":16}}],["lr=1e",{"2":{"190":3}}],["lr=3e",{"2":{"190":1}}],["lr=5e",{"2":{"86":1,"180":1}}],["lr=0",{"2":{"78":1,"159":1,"165":1,"170":1}}],["l8",{"2":{"186":1}}],["l",{"2":{"154":4,"170":2}}],["luca",{"2":{"172":1}}],["luabatch",{"2":{"157":2}}],["lua",{"2":{"142":1,"151":1}}],["luatensor",{"2":{"131":1,"151":6,"155":2,"177":1}}],["lun",{"2":{"79":1}}],["luncheon",{"2":{"78":1}}],["look",{"2":{"190":2}}],["low",{"2":{"186":1}}],["long",{"2":{"185":1}}],["localhost",{"2":{"170":2}}],["loc=",{"2":{"78":1}}],["load加载保存的数据",{"2":{"86":1}}],["load",{"2":{"86":5,"88":3,"89":2,"162":4,"180":5}}],["loader",{"2":{"76":2,"77":20,"78":14,"157":6,"159":2,"161":2,"165":2,"170":8,"180":4,"187":2,"188":2,"189":1,"190":9}}],["logistic",{"2":{"186":1}}],["logit",{"2":{"71":3}}],["logits",{"2":{"53":7,"60":5,"63":8,"70":2,"71":2,"73":9,"77":2,"80":7,"82":11,"83":14,"155":2,"159":2,"160":1,"161":3,"165":2,"169":1}}],["logging",{"2":{"159":1,"165":1}}],["logger",{"2":{"63":1}}],["log",{"2":{"72":8}}],["losses",{"2":{"78":18,"190":8}}],["loss",{"2":{"58":5,"73":1,"74":1,"77":25,"78":32,"152":1,"154":4,"159":10,"160":1,"165":10,"170":2,"180":6,"186":1,"190":25}}],["ln",{"2":{"55":4,"89":4}}],["level",{"2":{"182":1}}],["lecture",{"2":{"172":1}}],["leanpub",{"2":{"183":1}}],["learn",{"2":{"172":1}}],["learning",{"2":{"172":4,"183":3,"184":1,"188":1,"189":1}}],["learners",{"2":{"52":1,"182":1,"185":3}}],["least",{"2":{"164":1}}],["left",{"2":{"89":4}}],["left和right",{"2":{"89":1}}],["legend",{"2":{"78":1,"80":1}}],["length=gpt",{"2":{"77":2,"187":2}}],["length=max",{"2":{"48":1}}],["length=8和stride=2",{"2":{"177":1}}],["length=8",{"2":{"45":1,"177":1}}],["length=2和stride=2的数据加载器代码",{"2":{"177":1}}],["length=2",{"2":{"45":1,"177":1}}],["length=256",{"2":{"44":1}}],["length=4",{"2":{"44":1,"45":1}}],["length",{"2":{"43":4,"44":2,"48":9,"53":3,"59":1,"60":1,"63":3,"70":3,"76":1,"77":4,"79":1,"83":1,"89":2,"117":3,"121":2,"123":3,"124":2,"127":2,"128":2,"129":3,"131":2,"185":1}}],["len",{"2":{"21":1,"25":1,"26":1,"32":1,"42":1,"43":3,"53":2,"60":2,"63":1,"76":2,"77":3,"78":1,"80":1,"89":1,"157":5,"159":1,"161":1,"165":1,"187":6,"188":1,"190":1}}],["lt",{"2":{"31":7,"32":2,"33":1,"34":1,"36":5,"38":3,"39":1,"49":2}}],["later",{"2":{"78":1}}],["laid",{"2":{"78":1,"79":1}}],["laughed",{"2":{"78":1,"190":2}}],["layout",{"2":{"57":1,"78":1,"80":1}}],["layernorm",{"2":{"54":1,"55":2,"59":5,"60":3,"185":2}}],["layers",{"2":{"53":3,"57":2,"58":13,"60":2,"70":1,"89":12,"155":9,"179":1,"187":1}}],["layer",{"2":{"46":3,"47":1,"48":6,"54":4,"58":19,"60":4,"68":2,"89":1,"185":4}}],["labels",{"2":{"157":3,"159":2,"161":2,"165":4,"170":4}}],["label=f",{"2":{"80":1}}],["label=",{"2":{"78":2}}],["label",{"2":{"57":3}}],["labs的技术概述",{"2":{"185":1}}],["labs",{"2":{"53":1}}],["lambdalabs",{"2":{"185":1}}],["lambda",{"2":{"53":1}}],["last=false",{"2":{"77":1,"187":1}}],["last=drop",{"2":{"44":1}}],["last=true",{"2":{"44":1,"77":1,"157":2,"170":1,"187":1}}],["last",{"2":{"29":2,"44":1}}],["lan",{"2":{"16":1}}],["language￾models",{"2":{"185":1}}],["language",{"2":{"14":3,"52":1,"172":1,"182":7,"183":1,"185":4,"186":5}}],["larger",{"2":{"182":1}}],["large",{"2":{"8":1,"62":1,"89":1,"172":1,"182":2,"186":3}}],["livebook讨论论坛",{"2":{"196":1}}],["liu",{"2":{"172":1}}],["line",{"2":{"193":1}}],["linestyle=",{"2":{"78":1}}],["linear",{"0":{"111":1,"112":1},"2":{"53":1,"54":1,"57":4,"58":5,"60":1,"68":1,"88":1,"111":3,"112":4,"113":2,"123":3,"129":4,"155":8,"182":1,"185":2}}],["linspace",{"2":{"57":1,"78":1}}],["like",{"2":{"30":1,"34":2,"35":1,"37":2,"71":3,"193":1}}],["library",{"2":{"13":1}}],["libgen",{"2":{"13":1}}],["listing",{"2":{"53":1}}],["list",{"2":{"5":1,"26":1,"32":2,"195":1}}],["llama",{"2":{"13":1,"14":1,"76":1,"182":1}}],["llm的大部分计算开销花费在前馈层",{"2":{"185":1}}],["llm的训练循环是深度学习中的标准流程",{"2":{"92":1}}],["llm有时会生成",{"2":{"80":1}}],["llm也将始终生成相同的输出",{"2":{"79":1}}],["llms",{"2":{"49":2,"88":1,"89":1,"94":1,"138":2,"142":1,"170":1,"176":1,"186":1,"187":1}}],["llm",{"0":{"8":1,"52":1,"76":1},"1":{"53":1},"2":{"7":14,"8":7,"9":12,"10":16,"11":15,"13":5,"14":3,"15":10,"16":14,"18":13,"19":7,"20":4,"22":1,"23":1,"27":1,"30":2,"31":3,"36":2,"37":1,"39":1,"42":10,"44":2,"45":2,"46":5,"48":10,"49":2,"51":6,"52":2,"53":2,"55":2,"56":1,"57":4,"58":1,"59":2,"60":1,"61":1,"63":2,"65":3,"67":10,"68":1,"69":1,"70":2,"71":1,"73":2,"75":1,"76":5,"77":4,"78":1,"89":1,"92":1,"94":8,"95":2,"96":4,"97":2,"98":1,"99":3,"103":1,"104":4,"109":1,"115":3,"121":1,"124":3,"126":1,"133":5,"135":2,"138":7,"143":1,"172":1,"186":4,"196":1}}],["认识大型语言模型",{"0":{"6":1},"1":{"7":1,"8":1,"9":1,"10":1,"11":1,"12":1,"13":1,"14":1,"15":1,"16":1}}],["or",{"2":{"190":1}}],["org",{"2":{"13":3,"136":1,"141":4,"150":1,"151":1,"172":2,"182":12,"183":3,"184":8,"185":6,"186":10}}],["olmo",{"2":{"186":1}}],["ouyang等人",{"2":{"182":1}}],["outperforming",{"2":{"186":1}}],["outputs=2",{"2":{"159":1,"165":1,"170":1}}],["outputs",{"2":{"54":2,"71":2,"155":2,"161":4}}],["output",{"2":{"46":3,"48":2,"53":1,"58":6,"59":4,"60":3,"63":3,"70":1,"79":1,"83":1,"89":1}}],["out设置为1",{"2":{"178":1}}],["out=4",{"2":{"127":1}}],["out=2",{"2":{"105":1,"127":1}}],["out=cfg",{"2":{"59":1}}],["out",{"2":{"5":1,"53":3,"54":8,"55":3,"57":2,"60":7,"63":4,"89":7,"105":4,"106":1,"110":8,"112":7,"113":1,"123":6,"124":1,"127":3,"128":2,"129":14,"130":2,"131":3,"155":9,"178":4,"195":1}}],["os",{"2":{"170":2,"187":2}}],["omega",{"2":{"107":1,"110":1}}],["omegaω",{"2":{"99":1,"108":1}}],["other=next",{"2":{"82":1}}],["overview",{"2":{"185":1}}],["overfitting",{"2":{"184":1}}],["over",{"2":{"78":1}}],["only",{"2":{"186":1}}],["on",{"2":{"78":1,"83":1,"89":1,"164":1,"172":1,"185":1,"186":2}}],["ones",{"2":{"54":1,"117":1,"121":2,"123":1,"129":1}}],["one",{"2":{"47":1,"78":1,"79":1,"83":1,"99":2,"104":1,"157":4}}],["optim",{"2":{"78":1,"86":1,"159":1,"165":1,"170":1,"180":1,"188":1,"190":1}}],["optimizer",{"2":{"78":5,"86":7,"159":3,"160":1,"161":1,"165":3,"170":1,"180":4,"188":3,"189":3,"190":7}}],["open",{"2":{"21":1,"42":1,"44":1,"76":1,"182":1,"186":1,"187":2}}],["openai的gpt",{"2":{"185":1}}],["openai还提供了一个交互式网页界面",{"2":{"183":1}}],["openai开源了用于训练gpt",{"2":{"183":1}}],["openai将第一个transformer块的输出投影层的权重张量存储为params",{"2":{"89":1}}],["openai在多头注意力模块的线性层中使用了偏置向量来实现查询",{"2":{"89":1}}],["openai",{"0":{"88":1},"1":{"89":1,"90":1,"91":1},"2":{"7":1,"9":1,"14":3,"48":1,"49":1,"53":1,"60":2,"67":1,"76":1,"88":2,"89":2,"183":2,"185":1}}],["of",{"2":{"0":2,"5":1,"21":2,"34":2,"35":1,"37":2,"58":11,"60":5,"61":2,"78":2,"79":2,"83":1,"88":1,"89":1,"155":2,"170":1,"174":1,"179":7,"182":2,"183":2,"186":3,"190":2,"192":1,"195":1}}],["rmsnorm的详细介绍见以下论文",{"2":{"185":1}}],["rwkv",{"2":{"182":1}}],["root",{"2":{"185":1}}],["rocm",{"2":{"141":1}}],["row",{"2":{"102":2,"119":1}}],["rotation=90",{"2":{"80":1}}],["rnn",{"2":{"95":9,"96":6}}],["rnns",{"2":{"95":1,"182":1}}],["right",{"2":{"78":1,"89":5}}],["rtx",{"2":{"53":1,"140":1}}],["run",{"2":{"78":1,"185":1}}],["runtime",{"0":{"0":1},"1":{"1":1,"2":1,"3":1,"4":1,"5":1},"2":{"0":1,"5":1,"143":3}}],["rust",{"2":{"37":1}}],["rare",{"2":{"183":1}}],["raschka",{"2":{"172":4,"183":1}}],["rasbt",{"2":{"88":1,"89":1,"142":1,"170":1,"176":1,"186":1,"187":1}}],["raise",{"2":{"89":1}}],["rank=rank",{"2":{"170":2}}],["rank",{"2":{"170":12,"186":1}}],["random",{"2":{"83":1}}],["rand",{"2":{"57":1,"59":1,"105":3,"110":3,"111":1,"113":1,"155":1,"175":2}}],["randn",{"2":{"54":1}}],["range",{"2":{"42":2,"43":1,"53":1,"60":1,"63":1,"78":1,"80":1,"83":1,"89":1,"127":1,"159":1,"165":1,"170":1,"188":2,"189":2,"190":1}}],["ratio",{"2":{"77":3,"187":2}}],["rate",{"2":{"53":3,"59":2,"60":1,"64":1,"70":1,"187":1,"188":1,"189":1}}],["rather",{"2":{"21":1,"25":1}}],["raw",{"2":{"21":3,"25":1,"42":2,"44":2,"45":1,"48":1,"88":1,"177":2,"187":1}}],["radford等人",{"2":{"185":1}}],["radford",{"2":{"14":1,"52":1}}],["r",{"2":{"16":1,"21":1,"23":2,"24":1,"25":1,"28":3,"33":3,"42":1,"44":1,"76":1,"187":1}}],["redpajama",{"2":{"186":2}}],["reddit",{"2":{"13":1,"16":1}}],["refinedweb",{"2":{"186":1}}],["refres",{"2":{"70":1}}],["regression",{"2":{"186":1}}],["register",{"2":{"123":1,"124":2,"129":1}}],["repository",{"2":{"185":1,"186":1}}],["replace",{"2":{"78":1}}],["reinventing",{"2":{"182":1}}],["recognition",{"2":{"182":1}}],["rects",{"2":{"80":1}}],["retain",{"2":{"154":2}}],["return",{"2":{"28":2,"33":2,"43":2,"44":1,"53":3,"54":1,"57":2,"58":1,"59":1,"60":1,"63":1,"70":2,"77":2,"78":2,"80":1,"83":1,"89":1,"99":1,"110":1,"112":1,"123":1,"127":1,"129":1,"155":1,"157":2,"161":1,"170":1,"190":2,"193":2}}],["request",{"2":{"88":2,"187":2}}],["requires",{"2":{"46":1,"105":5,"153":1,"154":2,"155":1,"174":1}}],["really",{"2":{"71":3}}],["ready",{"2":{"63":3}}],["read",{"2":{"21":1,"42":1,"44":1,"76":1,"187":2}}],["rexmechicular",{"2":{"70":1}}],["rentingetic",{"2":{"70":1}}],["relubackward0>",{"2":{"54":1}}],["relu",{"2":{"54":3,"55":1,"57":16,"155":4}}],["re",{"2":{"23":5,"24":1,"25":1,"28":2,"33":2}}],["response",{"2":{"187":2}}],["reshape",{"2":{"151":3}}],["reshaping",{"2":{"129":1}}],["res",{"2":{"99":2}}],["resid",{"2":{"59":3}}],["residual",{"2":{"58":1,"185":2}}],["result",{"2":{"23":5,"24":4}}],["results",{"0":{"1":1},"1":{"2":1,"3":1,"4":1},"2":{"0":1}}],["research",{"2":{"14":1,"143":1,"186":1}}],["==",{"2":{"58":1,"78":1,"129":1,"161":4,"170":1,"190":1}}],["=x⋅φ",{"2":{"57":2}}],["=",{"2":{"0":1,"21":1,"23":4,"24":3,"25":2,"26":2,"27":1,"28":7,"29":3,"30":1,"32":2,"33":8,"34":4,"37":4,"42":10,"43":6,"44":8,"45":3,"46":4,"48":12,"52":1,"53":22,"54":14,"55":4,"57":9,"58":13,"59":18,"60":18,"61":2,"63":11,"68":1,"70":8,"71":9,"72":3,"73":3,"76":4,"77":16,"78":17,"79":2,"80":16,"82":4,"83":12,"86":4,"88":3,"89":29,"99":10,"101":3,"102":1,"103":1,"105":6,"106":5,"107":3,"108":2,"109":1,"110":11,"112":11,"116":4,"117":2,"118":1,"119":2,"121":7,"122":1,"123":13,"124":3,"127":2,"128":4,"129":25,"130":1,"131":4,"149":4,"150":3,"151":1,"152":7,"154":9,"155":10,"156":4,"157":9,"159":5,"161":10,"162":2,"164":5,"165":9,"166":2,"170":12,"174":3,"175":3,"177":2,"178":9,"179":8,"180":10,"187":11,"188":12,"189":8,"190":23}}],["fan等人",{"2":{"186":1}}],["falcon",{"2":{"186":1}}],["false",{"2":{"53":2,"54":1,"70":1,"143":1,"187":1}}],["faster",{"2":{"184":1}}],["fast",{"2":{"183":1,"184":1}}],["fsdp可以执行分布式数据并行",{"2":{"172":1}}],["fsdp",{"2":{"172":2}}],["follow",{"2":{"182":1}}],["foundation",{"2":{"182":1}}],["found",{"2":{"164":1}}],["forward",{"2":{"53":4,"54":1,"57":2,"58":2,"59":1,"60":2,"80":10,"110":2,"112":1,"123":1,"127":1,"128":1,"129":1,"155":4,"179":2}}],["for",{"2":{"0":1,"5":1,"23":1,"24":1,"25":1,"27":2,"28":4,"32":1,"33":5,"43":1,"53":1,"57":1,"58":2,"60":3,"63":1,"77":3,"78":2,"80":5,"83":1,"89":1,"99":3,"101":5,"127":2,"128":1,"133":1,"155":1,"159":2,"161":1,"165":2,"170":2,"174":1,"179":2,"182":5,"183":1,"185":1,"186":5,"188":3,"189":3,"190":4,"195":1}}],["fn",{"2":{"155":1}}],["fn=",{"2":{"46":1,"47":1,"53":1,"54":8,"55":2,"60":1,"110":1,"112":1,"116":1,"118":1,"119":1,"121":2,"122":1,"128":1,"155":1}}],["fc",{"2":{"89":2}}],["flashattention",{"2":{"184":2}}],["flashattention在数学上与标准的自注意力机制相同",{"2":{"184":1}}],["flashattention是一个高效的自注意力机制实现",{"2":{"184":1}}],["flattened",{"2":{"73":3}}],["flatten",{"2":{"71":1,"73":2,"77":2,"190":1}}],["flat",{"2":{"70":2,"73":6}}],["float32",{"2":{"150":3}}],["floatvec",{"2":{"150":2}}],["float",{"2":{"82":1,"83":1}}],["ff",{"2":{"59":2,"89":8,"179":1}}],["ffn",{"2":{"57":1}}],["few",{"2":{"182":1,"185":1}}],["feedback",{"2":{"182":1}}],["feed",{"2":{"179":2}}],["feedforward",{"2":{"57":6,"59":3}}],["features",{"2":{"157":2,"159":2,"161":2,"165":4,"170":3,"193":1}}],["features=3",{"2":{"155":1}}],["features=30",{"2":{"155":2}}],["features=20",{"2":{"155":2}}],["features=50",{"2":{"155":1}}],["featureiman",{"2":{"63":1}}],["fellow",{"2":{"21":1}}],["function并设置num",{"2":{"180":1}}],["functional",{"2":{"73":1,"77":1,"152":1,"154":1,"159":1,"184":1}}],["function",{"2":{"57":1,"186":1}}],["fully",{"2":{"172":2}}],["full",{"2":{"5":1,"195":1}}],["fill",{"2":{"121":1,"123":1,"129":1}}],["filename",{"2":{"88":2}}],["file",{"2":{"76":3,"187":8}}],["files",{"2":{"0":1}}],["find",{"2":{"190":3}}],["finding",{"2":{"89":1}}],["fine",{"2":{"182":1}}],["finetuning",{"2":{"172":1}}],["finance",{"2":{"182":1,"186":1}}],["final",{"2":{"53":2,"60":2,"89":4}}],["fig",{"2":{"78":2,"80":1}}],["figsize=",{"2":{"57":1,"78":1,"80":1}}],["figure",{"2":{"57":1}}],["first",{"2":{"44":3}}],["f",{"2":{"21":2,"42":4,"44":2,"53":1,"57":2,"58":1,"60":2,"61":1,"63":1,"71":2,"78":3,"80":1,"89":1,"129":1,"152":3,"154":2,"157":2,"159":5,"165":4,"170":7,"179":2,"190":3}}],["freq=5",{"2":{"78":1,"190":1}}],["freq",{"2":{"78":2,"80":2,"190":2}}],["free",{"2":{"8":1}}],["frontmatter",{"0":{"4":1},"2":{"0":3,"4":1}}],["from",{"2":{"0":1,"43":1,"59":1,"70":2,"78":1,"88":1,"89":1,"142":1,"154":1,"170":4,"176":1,"184":1,"186":2,"187":2}}],["vn",{"2":{"190":1}}],["v=gxje0dzvydm",{"2":{"186":1}}],["v=jxfdlgrfoby",{"2":{"172":1}}],["vries",{"2":{"185":1}}],["v2",{"0":{"113":1},"2":{"112":6,"113":4,"116":2,"178":3,"190":1}}],["vvv",{"2":{"110":1}}],["vwv​",{"2":{"104":2,"105":1,"110":2}}],["vector",{"2":{"103":2}}],["vecs",{"2":{"103":2,"124":4,"128":7,"131":5}}],["vec",{"2":{"99":3,"103":2,"109":2,"110":3,"112":2,"121":1,"123":2,"129":6}}],["version",{"2":{"37":3,"141":1}}],["verdict属于gpt",{"2":{"180":1}}],["verdict的训练集和验证集部分表现相似",{"2":{"180":1}}],["verdict不属于gpt",{"2":{"180":1}}],["verdict",{"2":{"20":2,"21":1,"30":1,"42":2,"44":1,"75":1,"76":2,"90":1,"187":3}}],["vbnetruntimeerror",{"2":{"164":1}}],["vbnetoutput",{"2":{"83":1,"89":1}}],["vbnetep",{"2":{"78":1}}],["v",{"2":{"80":2,"89":4,"104":1}}],["vijayakumar等人",{"2":{"186":1}}],["viehmann",{"2":{"172":1}}],["view",{"2":{"129":5,"151":3}}],["vindicated",{"2":{"78":1,"190":2}}],["vitepress",{"2":{"0":2,"192":1,"193":1}}],["vaswani等人",{"2":{"182":1,"184":1}}],["vahid",{"2":{"172":1}}],["validation",{"2":{"77":3,"78":1,"180":1}}],["val",{"2":{"77":8,"78":24,"83":2,"165":7,"170":1,"180":2,"187":1,"190":16}}],["values",{"2":{"106":4,"109":1,"110":2,"112":2,"121":1,"123":2,"129":6,"133":1,"190":2}}],["valueerror",{"2":{"89":1}}],["value",{"2":{"53":2,"89":4,"105":1,"106":3,"109":1,"110":3,"112":2,"123":2,"129":2,"178":2}}],["varbackward0>",{"2":{"54":3,"55":1}}],["variance",{"2":{"54":6,"55":2}}],["var",{"2":{"54":11,"55":3}}],["v15",{"2":{"184":1}}],["v100",{"2":{"53":1}}],["v1",{"0":{"111":1,"113":1},"2":{"44":3,"45":1,"48":1,"77":4,"110":5,"111":1,"112":2,"113":4,"178":3,"187":3,"190":1}}],["vocab",{"2":{"26":2,"27":1,"28":3,"29":1,"32":3,"33":3,"34":1,"46":2,"48":2,"53":4,"60":2,"61":1,"63":1,"70":1,"80":7,"88":1,"89":1,"187":1}}],["vue",{"2":{"0":1}}],["msg",{"2":{"193":2}}],["mseloss",{"2":{"58":1}}],["m",{"2":{"170":2}}],["mp",{"2":{"170":3}}],["mps",{"2":{"144":1,"166":2}}],["m3",{"2":{"144":1,"166":1}}],["m2",{"2":{"144":1,"166":1}}],["m1",{"2":{"144":1,"166":1}}],["must",{"2":{"129":1}}],["mulbackward0>",{"2":{"118":1,"122":1}}],["multiprocessing",{"2":{"170":1}}],["multi",{"2":{"125":1}}],["multinomial函数根据每个token的概率得分进行采样",{"2":{"80":1}}],["multinomial",{"2":{"80":2,"83":1}}],["multiheadattentionwrapper",{"2":{"126":1,"127":3,"128":4,"129":6,"178":1}}],["multiheadattention",{"0":{"131":1},"2":{"59":3,"129":6,"130":2,"131":1,"132":1,"178":1,"184":2}}],["multitask",{"2":{"52":1,"185":2}}],["mha",{"2":{"128":2,"131":2,"178":2}}],["mmbackward0>",{"2":{"110":1,"112":1}}],["mlp",{"2":{"89":4,"155":1}}],["mirjalili",{"2":{"172":1}}],["mismatch",{"2":{"89":1}}],["minbpe",{"2":{"183":1}}],["minimal",{"2":{"183":1}}],["minibatch",{"2":{"169":1}}],["min",{"2":{"77":1,"83":2,"189":3,"190":4}}],["microsoft",{"2":{"9":1}}],["mb",{"2":{"61":5,"179":1}}],["meap版",{"2":{"196":1}}],["meanbackward1>",{"2":{"54":3,"55":1}}],["mean",{"2":{"54":14,"55":4,"58":12,"72":1,"185":1}}],["memory",{"2":{"184":1,"186":1}}],["memory=true",{"2":{"170":2}}],["medical",{"2":{"182":1}}],["medium",{"2":{"62":1,"89":1,"185":1}}],["me",{"2":{"78":1,"190":2}}],["metadata",{"2":{"37":1}}],["meta",{"2":{"13":1,"14":1,"88":1,"182":1}}],["monte",{"2":{"78":1}}],["moves",{"2":{"53":1,"70":2,"71":5,"78":6,"79":3,"80":8,"82":1,"83":3,"89":2,"190":4}}],["mode=false",{"2":{"54":1,"161":1}}],["mode",{"2":{"54":1}}],["modeling",{"2":{"182":2,"186":1}}],["model=gpt",{"2":{"89":1}}],["model=model",{"2":{"63":1,"70":1,"78":1,"79":1,"83":1}}],["model类似",{"2":{"78":1}}],["model函数后",{"2":{"190":1}}],["model函数为我们提供了模型训练进展的数值估计",{"2":{"78":1}}],["model函数计算训练集和验证集的损失",{"2":{"78":1}}],["model函数对应图5",{"2":{"78":1}}],["model和generate",{"2":{"78":1}}],["model",{"2":{"53":2,"58":8,"60":6,"61":2,"63":5,"68":1,"70":2,"71":1,"77":7,"78":26,"79":2,"83":2,"86":16,"87":1,"88":5,"89":4,"155":12,"159":5,"160":4,"161":7,"162":9,"165":7,"170":8,"172":2,"174":2,"179":2,"180":9,"182":1,"185":1,"186":1,"187":2,"188":1,"190":25}}],["models",{"2":{"52":1,"88":1,"172":1,"180":2,"182":5,"185":3,"186":4}}],["modulelist",{"2":{"58":1,"127":1}}],["module",{"2":{"53":3,"54":1,"57":2,"58":1,"59":1,"60":1,"110":2,"112":1,"123":1,"127":1,"129":1,"155":4,"179":4}}],["more",{"0":{"5":1,"195":1}}],["mrs",{"2":{"29":2,"78":1}}],["markdown",{"0":{"192":1},"1":{"193":1,"194":1,"195":1},"2":{"192":1,"195":1}}],["mamba",{"2":{"182":1}}],["master",{"2":{"170":2,"183":1}}],["maskedfillbackward0>",{"2":{"121":1}}],["masked",{"2":{"118":1,"119":4,"121":4,"123":1,"129":1}}],["mask",{"2":{"117":2,"118":1,"121":1,"123":2,"129":4}}],["math",{"2":{"189":3,"190":2}}],["matmul",{"2":{"151":2}}],["matplotlib",{"2":{"57":2,"78":1,"148":1,"188":1}}],["machine",{"2":{"172":2,"183":4,"184":1}}],["machinelearning",{"2":{"16":1}}],["mac",{"0":{"166":1},"2":{"144":3,"166":1}}],["makes",{"2":{"89":1}}],["manning",{"2":{"89":1}}],["manual",{"2":{"46":1,"53":1,"54":1,"58":2,"59":1,"60":1,"70":1,"77":1,"78":1,"80":2,"83":1,"89":1,"105":1,"110":1,"112":1,"121":1,"122":1,"124":1,"128":1,"131":1,"155":2,"157":2,"159":1,"165":1,"170":1,"187":2,"190":2}}],["max",{"2":{"43":4,"44":4,"45":3,"48":2,"63":3,"70":1,"76":1,"77":2,"78":1,"79":1,"83":3,"89":1,"177":2,"187":2,"190":11}}],["made",{"2":{"41":1}}],["main",{"2":{"0":1,"88":2,"170":3,"186":1,"187":2}}],["md```js",{"2":{"193":1}}],["md",{"2":{"0":2,"194":1}}],["ibrahim等人",{"2":{"186":1}}],["io",{"2":{"184":1}}],["image",{"2":{"182":2,"190":1}}],["images",{"2":{"182":2,"190":1}}],["implementation",{"2":{"183":1}}],["improving",{"2":{"14":1}}],["importlib",{"2":{"37":1}}],["import",{"2":{"0":1,"37":2,"43":1,"53":2,"59":1,"70":3,"77":1,"88":1,"154":1,"157":2,"170":3,"187":3,"190":2}}],["irony",{"2":{"78":1,"190":2}}],["ier",{"2":{"40":2,"177":1}}],["if",{"2":{"23":1,"24":1,"25":1,"27":1,"28":1,"33":2,"58":2,"77":3,"78":1,"83":2,"89":1,"155":1,"165":1,"166":2,"174":1,"187":2,"188":1,"189":1,"190":5}}],["i",{"2":{"21":1,"25":1,"27":2,"28":4,"32":1,"33":4,"42":6,"43":5,"57":2,"63":8,"71":2,"77":2,"78":3,"80":6,"99":17,"101":4,"104":3,"129":1,"170":2,"190":2}}],["ideal",{"2":{"89":1}}],["idx+1",{"2":{"157":2}}],["idx=text",{"2":{"70":1,"79":1,"83":1,"89":1}}],["idx=encoded",{"2":{"63":1,"78":1}}],["idx",{"2":{"43":3,"53":4,"60":4,"63":9,"71":6,"77":3,"83":10,"99":3,"157":2,"159":2,"161":1,"165":2,"187":3}}],["ids=",{"2":{"170":1}}],["ids",{"2":{"28":4,"29":3,"33":4,"43":11,"46":1,"47":1,"48":2,"60":2,"70":10,"71":7,"78":4,"79":4,"80":2,"83":4,"89":4}}],["id",{"0":{"26":1},"1":{"27":1,"28":1,"29":1,"30":1},"2":{"20":1,"26":3,"27":6,"28":2,"29":3,"34":2,"37":2,"38":2,"40":2,"42":6,"43":1,"44":6,"45":1,"46":9,"47":6,"48":11,"49":2,"53":2,"63":13,"70":4,"71":8,"72":2,"73":2,"80":5,"82":1,"164":1,"177":1}}],["id=25607809",{"2":{"13":1}}],["isbn",{"2":{"172":2}}],["ist",{"2":{"11":1}}],["is",{"2":{"11":2,"23":4,"24":2,"77":2,"83":1,"143":1,"144":1,"164":1,"165":1,"166":2,"182":2,"184":1,"187":1,"190":2,"194":10}}],["increment",{"2":{"188":2,"189":2,"190":2}}],["inches",{"2":{"80":4}}],["independent",{"2":{"183":1}}],["index",{"2":{"88":1,"157":3}}],["in=3",{"2":{"105":1}}],["in=cfg",{"2":{"59":1}}],["info",{"2":{"194":4}}],["inf",{"2":{"82":10,"83":1,"121":16,"123":1,"129":1}}],["inverse",{"2":{"80":4}}],["insensible",{"2":{"78":1,"190":1}}],["instructions",{"2":{"182":1}}],["instructgpt用于gpt",{"2":{"182":1}}],["instructgpt",{"2":{"14":1}}],["install",{"2":{"37":1,"88":1,"140":1,"141":1}}],["input=torch",{"2":{"82":1}}],["inputs=2",{"2":{"159":1,"165":1,"170":1}}],["inputs",{"2":{"45":3,"48":5,"71":2,"99":9,"101":4,"103":1,"105":2,"106":2,"110":2,"112":1,"116":2,"122":2,"155":2}}],["input",{"2":{"43":7,"47":1,"48":2,"58":3,"59":1,"60":1,"77":6,"78":3,"188":1,"189":1,"190":4,"193":1,"194":1}}],["introducing",{"2":{"172":2}}],["introduction",{"2":{"148":1,"172":1}}],["int64",{"2":{"150":1}}],["into",{"2":{"89":6}}],["int",{"2":{"28":4,"33":5,"77":1,"187":1}}],["integers",{"2":{"37":3}}],["integer",{"2":{"27":2,"32":2}}],["initial",{"2":{"188":4,"189":3,"190":4}}],["init",{"2":{"28":1,"33":1,"43":1,"53":6,"54":2,"57":4,"58":2,"59":2,"60":4,"110":3,"112":2,"123":2,"124":1,"127":2,"129":2,"155":4,"157":3,"170":4}}],["in",{"2":{"0":1,"23":1,"24":1,"25":1,"27":2,"28":4,"32":2,"33":6,"34":2,"35":1,"37":2,"42":4,"43":1,"53":4,"57":1,"58":3,"60":6,"63":1,"77":3,"78":2,"80":5,"83":1,"89":1,"99":3,"101":2,"105":4,"110":6,"112":5,"113":1,"123":5,"124":1,"127":4,"128":3,"129":5,"131":2,"148":1,"155":4,"157":2,"159":2,"161":1,"165":2,"170":2,"172":2,"174":1,"178":3,"179":6,"185":2,"188":3,"189":3,"190":4,"192":1}}],["iter=1",{"2":{"78":1,"190":1}}],["iter",{"2":{"44":4,"45":3,"48":3,"78":5,"190":8}}],["items",{"2":{"27":1,"28":1,"32":2,"33":1,"80":1}}],["item",{"2":{"13":1,"23":3,"24":3,"25":3,"27":2,"28":3,"32":2,"33":6,"58":1,"77":1,"80":3,"161":1}}],["it",{"2":{"0":1,"21":1,"29":2}}],["csharp",{"2":{"177":1}}],["css",{"2":{"154":2}}],["csscontext",{"2":{"124":1,"128":1}}],["csskeys",{"2":{"106":1}}],["cssall",{"2":{"102":1}}],["cssattention",{"2":{"99":1}}],["csstorch",{"2":{"122":1,"151":1,"155":1}}],["csstop",{"2":{"82":1}}],["csstensor",{"2":{"101":2,"102":1,"103":1,"110":1,"112":1,"116":1,"117":1,"118":1,"119":1,"121":3,"122":1,"128":1,"161":2}}],["csssettings",{"2":{"89":1}}],["ctx",{"2":{"89":1,"187":5}}],["ckpt",{"2":{"88":3}}],["clip",{"2":{"190":2}}],["cloudfront",{"2":{"185":1}}],["closer",{"2":{"80":4}}],["class",{"2":{"43":1,"53":3,"54":1,"57":2,"58":1,"59":1,"60":1,"110":1,"157":1}}],["cpu",{"2":{"77":1,"79":1,"124":1,"137":1,"139":1,"140":2,"158":1,"164":4,"165":3,"166":2,"167":1,"187":1}}],["custom",{"0":{"194":1}}],["curated",{"2":{"186":1}}],["curtains",{"2":{"78":1}}],["current",{"2":{"0":1}}],["cuda",{"2":{"77":2,"140":2,"141":3,"143":1,"164":7,"165":3,"166":2,"170":4,"175":2,"187":2}}],["cross",{"2":{"72":1,"73":5,"77":1,"152":1,"154":1,"159":1,"160":1,"165":1}}],["create",{"2":{"44":3,"45":1,"48":1,"77":4,"177":2,"187":3}}],["ca",{"2":{"124":2}}],["causalattention",{"2":{"122":1,"123":1,"124":3,"125":1,"126":1,"127":2,"129":5,"130":1}}],["carlo",{"2":{"78":1}}],["calculus",{"2":{"172":2}}],["calc",{"2":{"77":9,"78":3,"180":2,"190":3}}],["catbackward0>",{"2":{"128":1}}],["cat",{"2":{"63":1,"72":1,"83":1,"127":1}}],["can",{"2":{"0":1}}],["cdot",{"2":{"57":1}}],["cdn",{"2":{"14":1}}],["cfg",{"2":{"53":12,"57":5,"59":5,"60":13}}],["c",{"0":{"176":1,"177":1,"178":1,"179":1,"180":1},"1":{"177":1,"178":1,"179":1,"180":1},"2":{"28":1,"43":1,"44":1,"53":1,"59":1,"63":1,"77":1,"78":1,"82":1,"83":1,"89":10,"105":1,"123":1,"129":3,"136":1,"149":1,"152":1,"155":1,"157":1,"159":1,"161":1,"165":1,"170":2,"188":1,"190":1}}],["cos",{"2":{"189":1,"190":1}}],["corpora",{"2":{"186":1}}],["corpus",{"2":{"186":1}}],["correct",{"2":{"161":3}}],["count",{"2":{"170":2}}],["colab连接的v100",{"2":{"175":1}}],["colab",{"2":{"143":3}}],["copy",{"2":{"89":1,"179":1}}],["code",{"2":{"88":1,"136":1,"186":1,"187":1}}],["convolutional",{"2":{"182":1}}],["condition=next",{"2":{"82":1}}],["cond",{"2":{"63":2,"83":2}}],["considering",{"2":{"60":2,"179":1}}],["const",{"2":{"0":1}}],["config字典初始化一个新的gptmodel实例",{"2":{"89":1}}],["configs",{"2":{"89":3}}],["config",{"2":{"53":4,"57":3,"59":3,"60":3,"63":1,"64":1,"70":5,"77":4,"78":1,"79":1,"83":1,"86":2,"89":10,"179":7,"180":1,"187":6,"190":2}}],["connections",{"2":{"51":1,"58":4,"185":1}}],["containers",{"0":{"194":1}}],["containing",{"2":{"46":1}}],["continually",{"2":{"186":1}}],["contiguous",{"2":{"129":1}}],["context=",{"2":{"78":1,"190":1}}],["context",{"0":{"45":1},"2":{"42":8,"48":4,"53":3,"59":1,"60":1,"63":6,"70":6,"77":4,"78":7,"79":2,"83":4,"89":3,"99":3,"103":6,"109":1,"110":3,"112":2,"117":2,"121":3,"123":5,"124":5,"127":2,"128":8,"129":9,"131":7,"185":2,"190":2}}],["covers",{"2":{"14":1}}],["compare",{"2":{"161":3}}],["compute",{"2":{"161":5,"170":2}}],["computing",{"2":{"148":1}}],["compton",{"2":{"63":1}}],["comments",{"2":{"16":1}}],["commoncrawl",{"2":{"12":1,"13":4}}],["com",{"2":{"13":1,"14":1,"16":1,"88":1,"89":1,"136":2,"142":1,"143":1,"170":1,"172":3,"176":1,"183":4,"185":3,"186":4,"187":1}}],["ch02",{"2":{"187":1}}],["ch05",{"2":{"88":1,"186":1}}],["cho和bengio",{"2":{"184":1}}],["chocolate",{"2":{"71":1}}],["chunk",{"2":{"43":6}}],["cheap",{"2":{"21":1,"25":1}}],["checkpoint",{"2":{"86":2,"180":2}}],["check",{"2":{"5":1,"195":1}}],["chat",{"2":{"182":1}}],["chatgpt",{"2":{"7":3,"9":3,"10":2,"12":1,"14":2,"16":1,"18":1,"37":1,"88":1}}],["change",{"2":{"143":1}}],["chapter",{"2":{"88":1,"187":1}}],["chapter02",{"2":{"77":1}}],["chapter04",{"2":{"70":2}}],["chapters",{"2":{"59":1,"187":2,"190":2}}],["characters",{"2":{"21":2,"76":3}}],["d4mucfpksywv",{"2":{"185":1}}],["dual",{"2":{"185":1}}],["dummylayernorm",{"2":{"53":5,"54":1,"60":1}}],["dummytransformerblock",{"2":{"53":4,"60":1}}],["dummygptmodel",{"2":{"53":6,"60":2}}],["d18",{"2":{"183":1}}],["ddp在交互式python环境",{"2":{"170":1}}],["ddp通过在可用设备上分割输入数据",{"2":{"169":1}}],["ddp",{"2":{"169":1,"170":4}}],["ds",{"2":{"157":6,"170":3}}],["dtype",{"2":{"150":4}}],["distributeddataparallel",{"2":{"170":1}}],["distributedsampler",{"2":{"170":1}}],["distributed",{"2":{"170":2}}],["diverse",{"2":{"182":1,"186":3}}],["divisible",{"2":{"129":1}}],["divbackward0>",{"2":{"54":1,"119":1}}],["diagonal=1",{"2":{"121":1,"123":1,"129":1}}],["din​",{"2":{"110":1}}],["dind",{"2":{"110":1}}],["dir=",{"2":{"88":1,"180":2}}],["dictionary",{"2":{"89":2}}],["dict方法恢复模型和优化器状态",{"2":{"86":1}}],["dict内容",{"2":{"86":1}}],["dict保存模型权重后",{"2":{"86":1}}],["dict的文件名",{"2":{"86":1}}],["dict",{"2":{"86":11,"89":1,"162":4,"180":4}}],["dimensions",{"2":{"89":2}}],["dim=5",{"2":{"55":1}}],["dim=2",{"2":{"54":1}}],["dim=1",{"2":{"54":3,"63":1,"83":1,"102":2,"112":1,"116":1,"119":1,"121":1,"155":1,"161":4}}],["dim=",{"2":{"54":9,"55":2,"63":2,"71":2,"83":2,"108":1,"110":1,"123":1,"127":1,"129":1}}],["dim=0",{"2":{"53":1,"54":1,"80":2,"82":1,"99":2,"122":1}}],["dim",{"2":{"46":3,"48":3,"53":6,"54":7,"57":5,"59":4,"60":4,"70":1,"89":4,"129":4,"130":1,"179":1,"187":1}}],["dropout率",{"2":{"187":1}}],["dropout是一种正则化技术",{"2":{"184":1}}],["dropout通过在训练期间随机",{"2":{"86":1}}],["dropout=cfg",{"2":{"59":1}}],["dropout",{"0":{"64":1,"122":1},"2":{"53":5,"59":3,"60":2,"63":1,"64":4,"94":1,"121":13,"122":3,"123":5,"127":2,"129":5,"133":1,"160":1,"184":1}}],["drop",{"2":{"44":2,"53":5,"59":5,"60":3,"64":1,"70":1,"77":2,"157":2,"170":1,"187":3}}],["dolma",{"2":{"186":1}}],["dosovitskiy等人",{"2":{"182":1}}],["docs",{"2":{"150":1,"151":1,"184":2}}],["documentation",{"2":{"5":1,"184":2,"195":1}}],["dout​",{"2":{"110":1}}],["doutd",{"2":{"110":1}}],["dot",{"2":{"99":2,"101":1,"107":1,"184":3}}],["download",{"2":{"88":7,"89":1,"180":2}}],["down",{"2":{"78":1,"79":1}}],["donkey",{"2":{"78":1,"190":2}}],["do",{"2":{"30":1,"34":2,"35":1,"37":2}}],["details",{"2":{"194":4}}],["detokenizer",{"2":{"183":1}}],["demystifying",{"2":{"185":1}}],["demonstrates",{"2":{"0":1,"192":1}}],["devlin等人",{"2":{"182":1}}],["devices",{"2":{"164":1}}],["device=rank",{"2":{"170":2}}],["device=",{"2":{"164":1}}],["device=in",{"2":{"53":1,"60":1}}],["device",{"2":{"53":1,"60":1,"77":9,"78":11,"83":1,"89":1,"164":2,"165":6,"166":2,"170":5,"180":2,"187":2,"190":7}}],["deep",{"2":{"172":2,"182":1}}],["destroy",{"2":{"170":3}}],["desired",{"2":{"42":4}}],["decoding",{"2":{"186":1}}],["decoded",{"2":{"63":1,"78":2}}],["decode",{"2":{"27":1,"28":3,"29":3,"33":1,"35":1,"37":2,"40":2,"42":2,"63":2,"70":1,"177":1,"187":1}}],["decay=0",{"2":{"78":1,"86":1,"180":1,"188":1,"190":1}}],["depend",{"2":{"41":1}}],["de",{"2":{"41":1,"185":1}}],["default",{"2":{"193":2}}],["define",{"2":{"41":1}}],["def",{"2":{"28":3,"33":3,"43":3,"53":6,"54":2,"57":4,"58":2,"59":2,"60":2,"63":1,"70":2,"78":1,"89":1,"110":2,"112":2,"123":2,"127":2,"129":2,"155":2,"157":3,"190":1}}],["d",{"0":{"187":1,"188":1,"189":1,"190":1},"1":{"188":1,"189":1,"190":1},"2":{"16":1,"28":1,"41":1,"43":1,"53":1,"59":2,"63":1,"75":1,"77":1,"78":1,"83":1,"89":1,"99":1,"105":10,"106":1,"108":1,"110":12,"112":12,"113":2,"123":12,"124":2,"127":7,"128":4,"129":20,"130":1,"131":5,"149":1,"152":1,"155":1,"157":1,"159":1,"170":2,"178":6,"188":1,"190":2}}],["dangerous",{"2":{"194":2}}],["danger",{"2":{"194":2}}],["dao",{"2":{"184":1}}],["dao等人",{"2":{"184":1}}],["days",{"2":{"78":1,"190":2}}],["day",{"2":{"53":1,"78":1}}],["das",{"2":{"11":1}}],["data加载到数据加载器中",{"2":{"187":1}}],["dataloader",{"2":{"42":1,"43":4,"44":8,"45":2,"48":3,"77":4,"156":2,"157":6,"158":1,"161":2,"170":1,"177":2,"187":3}}],["dataset=test",{"2":{"157":1}}],["dataset=train",{"2":{"157":2,"170":1}}],["dataset",{"2":{"42":1,"43":5,"44":2,"78":1,"156":3,"157":5,"158":1,"170":2,"182":1,"186":3,"190":1}}],["data",{"0":{"2":1,"3":1},"2":{"0":3,"43":1,"44":3,"45":2,"48":2,"76":5,"77":13,"88":1,"157":2,"170":1,"172":2,"186":3,"187":6,"190":1,"193":2}}],["syntax",{"0":{"193":1},"2":{"193":1}}],["sketch",{"2":{"190":2}}],["skip",{"2":{"58":2}}],["srivastava14a",{"2":{"184":1}}],["srivastava等人",{"2":{"184":1}}],["src",{"2":{"183":1}}],["sgd",{"2":{"159":2,"160":1,"165":1,"170":1}}],["sqlep",{"2":{"190":1}}],["sql73",{"2":{"80":1}}],["square",{"2":{"185":1}}],["squeeze",{"2":{"63":1,"70":1}}],["sqrt",{"2":{"54":2,"57":1}}],["small",{"2":{"62":1,"89":3,"180":1}}],["swiglu",{"2":{"57":2}}],["scalable",{"2":{"186":1}}],["scaling",{"2":{"186":1}}],["scaled",{"2":{"80":4,"184":2}}],["scale",{"2":{"54":3,"89":6,"182":1}}],["scssneuralnetwork",{"2":{"155":1}}],["scsstensor",{"2":{"82":2,"99":3,"106":1,"107":2,"108":1,"109":1,"161":2,"164":1,"190":2}}],["score",{"2":{"107":3}}],["scores",{"2":{"99":8,"101":5,"102":1,"107":2,"108":1,"110":3,"112":2,"116":2,"117":1,"121":1,"123":3,"129":3}}],["scratch",{"2":{"88":1,"89":1,"142":1,"170":1,"176":1,"186":1,"187":1}}],["script>",{"2":{"0":1}}],["script",{"2":{"0":1}}],["science",{"2":{"186":1}}],["scientific",{"2":{"148":1}}],["scikit",{"2":{"172":1}}],["sci",{"2":{"54":2,"161":1}}],["shiki",{"2":{"193":1}}],["shift",{"2":{"54":3,"89":6}}],["sharded",{"2":{"172":2}}],["shape",{"2":{"48":6,"53":5,"57":1,"58":2,"59":6,"60":10,"63":3,"71":1,"73":8,"77":4,"78":1,"89":6,"99":2,"105":1,"106":6,"108":1,"110":1,"112":1,"116":1,"117":1,"121":1,"122":1,"123":2,"124":4,"128":5,"129":2,"131":4,"151":3,"155":2,"157":1,"170":1}}],["she",{"2":{"78":1,"190":2}}],["shot",{"2":{"182":1,"185":1}}],["show",{"2":{"57":1,"78":1,"80":1,"188":1,"189":1}}],["shortcut=true",{"2":{"58":1}}],["shortcut=false",{"2":{"58":1}}],["shortcut",{"2":{"51":1,"58":10,"59":4}}],["shuffle=false",{"2":{"44":1,"48":1,"77":1,"157":1,"170":2,"187":1}}],["shuffle=shuffle",{"2":{"44":1}}],["shuffle=true",{"2":{"44":1,"77":1,"157":2,"187":1}}],["same",{"2":{"164":1}}],["sampler=distributedsampler",{"2":{"170":2}}],["sampled",{"2":{"80":4,"81":1,"180":1}}],["samples=1",{"2":{"80":2,"83":1}}],["sample文本函数则生成了模型的具体文本示例",{"2":{"78":1}}],["sample函数中用multinomial函数替换argmax函数",{"2":{"80":1}}],["sample函数内部",{"2":{"79":1}}],["sample函数接收一个文本片段",{"2":{"78":1}}],["sample函数是一个辅助函数",{"2":{"78":1}}],["sample",{"2":{"42":7,"58":3,"78":3,"80":2,"190":2}}],["sa",{"2":{"110":2,"112":2,"116":2,"178":5}}],["save",{"2":{"86":3,"162":1}}],["save函数来实现",{"2":{"86":1}}],["said",{"2":{"29":2}}],["spaces",{"2":{"182":1}}],["spawn",{"2":{"170":2}}],["spawn这样的函数",{"2":{"170":1}}],["special=",{"2":{"37":1,"70":1}}],["split",{"2":{"23":3,"24":1,"25":1,"28":1,"33":1,"77":3,"88":1,"89":2,"187":3}}],["suite",{"2":{"186":1}}],["supplementary",{"2":{"172":1}}],["super",{"2":{"53":3,"54":1,"57":2,"58":1,"59":1,"60":1,"110":1,"112":1,"123":1,"127":1,"129":1,"155":1}}],["survey",{"2":{"136":1}}],["surprise",{"2":{"83":1}}],["sums",{"2":{"102":2,"119":2}}],["sum",{"2":{"60":2,"99":9,"102":1,"119":1,"155":1,"161":3,"174":1,"179":2}}],["sunlit",{"2":{"34":2,"35":1,"37":2}}],["subword",{"2":{"183":2}}],["subplots",{"2":{"78":1,"80":1}}],["subplot",{"2":{"57":1}}],["sub",{"2":{"28":1,"33":1}}],["s+",{"2":{"28":1,"33":1}}],["simplifying",{"2":{"184":1}}],["simple方法那样用它来训练模型",{"2":{"190":1}}],["simple的修改之处用注释标出",{"2":{"190":1}}],["simple训练函数",{"2":{"190":1}}],["simple中",{"2":{"80":1}}],["simple函数的结果进行比较",{"2":{"190":1}}],["simple函数继续预训练1个轮次",{"2":{"87":1}}],["simple函数类似的相同输出",{"2":{"85":1}}],["simple函数生成的文本非常不同",{"2":{"83":1}}],["simple函数并使用相同的起始内容",{"2":{"79":1}}],["simple函数中",{"2":{"79":1,"80":1}}],["simple函数将启动训练过程",{"2":{"78":1}}],["simple函数训练一个gpt模型实例10个轮次",{"2":{"78":1}}],["simple函数",{"2":{"78":1,"79":1,"83":1,"86":1}}],["simple函数使用了两个尚未定义的函数",{"2":{"78":1}}],["simple函数实现这一训练流程",{"2":{"78":1}}],["simple",{"2":{"63":6,"70":3,"71":3,"78":3,"79":1,"117":2,"118":3,"119":4,"180":1,"183":1,"184":1,"186":1}}],["simpletokenizerv2",{"2":{"31":1,"33":2,"34":2,"37":1}}],["simpletokenizerv1",{"2":{"28":2,"29":2,"33":1}}],["singhal等人",{"2":{"182":1}}],["single",{"2":{"172":1}}],["silicon",{"0":{"144":1},"2":{"144":4,"166":1}}],["silver",{"2":{"78":1,"79":1}}],["sigmoid",{"2":{"57":3,"152":1,"154":1}}],["sized",{"2":{"185":1}}],["sizes",{"2":{"58":14}}],["size=world",{"2":{"170":2}}],["size=new",{"2":{"89":1}}],["size=",{"2":{"88":1,"89":1,"180":2}}],["size=context",{"2":{"78":1}}],["size=cfg",{"2":{"59":1}}],["size=2",{"2":{"77":2,"157":3,"170":1,"187":2}}],["size=gpt",{"2":{"63":1,"70":1,"79":1,"83":1}}],["size=8",{"2":{"45":1,"48":1}}],["size=1",{"2":{"44":1}}],["size=batch",{"2":{"44":1}}],["size=4",{"2":{"44":1,"177":2}}],["size+1",{"2":{"42":3}}],["size",{"0":{"45":1},"2":{"26":2,"42":2,"44":1,"46":3,"48":6,"53":6,"54":2,"57":1,"59":2,"60":6,"61":8,"63":5,"70":1,"71":1,"73":4,"77":6,"78":2,"83":2,"106":2,"122":1,"124":1,"128":1,"131":2,"151":1,"155":1,"170":8,"178":3,"179":1,"187":1}}],["site",{"2":{"0":1}}],["story",{"2":{"186":1}}],["stevens",{"2":{"172":1}}],["steps=10",{"2":{"190":2}}],["steps",{"2":{"188":5,"189":6,"190":7}}],["step",{"2":{"78":12,"99":2,"104":1,"159":1,"160":1,"165":1,"188":5,"189":6,"190":9}}],["stable",{"2":{"150":1,"151":1,"184":2}}],["state",{"2":{"86":11,"162":4,"180":4,"182":1}}],["stand",{"2":{"83":1}}],["starts",{"2":{"99":2,"104":1}}],["start",{"2":{"63":1,"70":2,"78":6,"190":3}}],["stack",{"2":{"53":1,"122":1}}],["stackexchange",{"2":{"13":1}}],["strategies",{"2":{"186":1}}],["stren",{"2":{"70":1}}],["stride=gpt",{"2":{"77":2,"187":2}}],["stride=max",{"2":{"48":1}}],["stride=4",{"2":{"45":1}}],["stride=2",{"2":{"45":2,"177":2}}],["stride=1",{"2":{"44":2}}],["stride=128",{"2":{"44":1}}],["stride",{"0":{"45":1},"2":{"43":2,"44":4,"45":1}}],["strings",{"2":{"37":1}}],["strip",{"2":{"23":1,"24":2,"25":2,"28":2,"33":2}}],["str",{"2":{"28":4,"33":5}}],["s",{"2":{"23":3,"24":1,"25":1,"28":5,"29":2,"33":5,"88":7,"185":1}}],["solutions",{"2":{"186":1}}],["soldaini等人",{"2":{"186":1}}],["song等人",{"2":{"183":1}}],["softmaxbackward0>",{"2":{"116":1,"121":1}}],["softmax",{"2":{"63":7,"71":7,"72":1,"73":2,"80":4,"82":1,"83":1,"99":5,"102":2,"108":4,"110":2,"112":1,"115":1,"116":2,"120":1,"121":5,"123":1,"129":1,"155":2,"160":1,"161":3}}],["sorted",{"2":{"26":1,"32":1}}],["so",{"2":{"21":1}}],["something",{"2":{"89":1}}],["someunknownplace",{"2":{"37":2,"38":1}}],["some",{"2":{"0":1,"192":1}}],["sentencepiece",{"2":{"183":1}}],["sennrich等人",{"2":{"183":1}}],["selective",{"2":{"182":1}}],["selection",{"2":{"172":2}}],["selfattention",{"0":{"111":1,"113":2},"2":{"110":3,"111":1,"112":6,"113":8,"122":1}}],["self",{"2":{"28":7,"33":8,"43":11,"53":18,"54":8,"57":6,"58":7,"59":13,"60":14,"110":9,"112":9,"123":13,"124":1,"127":4,"128":1,"129":25,"130":1,"155":6,"157":8}}],["sebastianraschka",{"2":{"172":2}}],["sebastian",{"2":{"172":4,"183":1}}],["sevres",{"2":{"78":1,"79":1}}],["seen",{"2":{"78":14,"190":7}}],["seed",{"2":{"46":1,"53":1,"54":1,"58":2,"59":1,"60":1,"70":1,"77":1,"78":1,"80":2,"83":1,"89":1,"105":1,"110":1,"112":1,"121":1,"122":1,"124":1,"128":1,"131":1,"155":2,"157":2,"159":1,"165":1,"170":1,"187":2,"190":2}}],["sequence",{"2":{"182":1,"186":1}}],["sequential",{"2":{"53":1,"54":1,"57":1,"58":5,"60":1,"155":5}}],["seq",{"2":{"53":2,"60":2}}],["second",{"2":{"44":1}}],["setup",{"2":{"170":2}}],["setup>",{"2":{"0":1}}],["settings",{"2":{"88":2,"89":5}}],["set",{"2":{"26":1,"32":1,"54":1,"78":3,"80":3,"161":1,"170":2}}],["search",{"2":{"9":1,"186":1}}],["awareness",{"2":{"184":1}}],["aws",{"2":{"76":1}}],["ak",{"2":{"177":1}}],["akwirw",{"2":{"40":2,"177":1}}],["at",{"2":{"164":1,"182":1,"190":2}}],["attn",{"2":{"89":7,"99":15,"101":3,"102":3,"103":1,"107":4,"108":3,"109":1,"110":5,"112":4,"116":4,"117":1,"118":1,"121":3,"122":1,"123":7,"129":7}}],["att",{"2":{"59":2,"89":17,"179":1}}],["attention",{"2":{"11":1,"99":4,"125":1,"179":2,"182":1,"184":5}}],["additional",{"2":{"193":1}}],["addr",{"2":{"170":1}}],["addmmbackward0>",{"2":{"155":1}}],["adamw使用历史数据动态调整每个模型参数的学习率",{"2":{"86":1}}],["adamw",{"2":{"78":1,"86":1,"180":1,"188":1,"190":1}}],["adamw是adam的一个变体",{"2":{"78":1}}],["adamw优化器",{"2":{"78":1}}],["adam优化器是深度神经网络训练的常见选择",{"2":{"78":1}}],["autograd",{"2":{"137":1,"152":2,"154":5,"171":1}}],["axis=",{"2":{"89":2}}],["axioms",{"2":{"78":1,"79":1}}],["ax",{"2":{"80":6}}],["ax2",{"2":{"78":3}}],["ax1",{"2":{"78":7}}],["again",{"2":{"78":2,"190":2}}],["available",{"2":{"77":1,"143":1,"144":1,"164":1,"165":1,"166":2,"170":1,"187":1}}],["avg",{"2":{"72":4}}],["a100",{"2":{"76":1}}],["amd",{"0":{"141":1},"2":{"141":1}}],["am",{"2":{"63":8}}],["abs",{"2":{"58":1,"172":1,"182":12,"183":2,"184":5,"185":6,"186":10}}],["aclanthology",{"2":{"183":1}}],["accelerating",{"2":{"186":1}}],["access",{"2":{"0":1}}],["accumulation",{"2":{"172":2}}],["accuracy",{"2":{"161":5,"170":4}}],["acc",{"2":{"170":4}}],["across",{"2":{"78":1,"79":1,"186":1}}],["activation",{"2":{"57":1}}],["architecture",{"2":{"185":1}}],["arduinototal",{"2":{"179":1}}],["arduino",{"2":{"141":1,"177":1}}],["arduinoprevious",{"2":{"103":1}}],["arduinooutput",{"2":{"79":1}}],["armed",{"2":{"71":1}}],["args=",{"2":{"170":1}}],["argue",{"2":{"63":1}}],["argmax选择概率最高的token作为下一个token",{"2":{"80":1}}],["argmax",{"2":{"63":3,"71":3,"80":1,"83":1,"161":5}}],["are",{"2":{"52":1,"182":1,"185":3}}],["arange",{"2":{"48":2,"53":1,"60":1,"80":1}}],["arxiv",{"2":{"13":1,"172":1,"182":12,"183":2,"184":5,"185":6,"186":10}}],["apple",{"0":{"144":1},"2":{"144":5,"166":1}}],["appointed",{"2":{"78":1,"79":1}}],["append",{"2":{"43":2,"53":2,"78":3,"188":1,"189":1,"190":4}}],["apis",{"2":{"0":1,"5":1}}],["api",{"0":{"0":1,"148":1},"1":{"1":1,"2":1,"3":1,"4":1,"5":1},"2":{"0":1,"148":1,"159":1,"172":2}}],["alt",{"2":{"190":1}}],["align",{"2":{"184":1}}],["algorithm",{"2":{"172":1}}],["alphaα",{"2":{"99":1,"108":1}}],["alpha",{"2":{"99":2}}],["alpha=0",{"2":{"78":1}}],["always",{"2":{"21":1,"25":1}}],["allowed",{"2":{"37":1,"70":1}}],["all",{"2":{"11":1,"26":1,"27":1,"32":2,"102":1,"103":1,"164":1,"182":1,"184":1}}],["assert",{"2":{"129":1}}],["assign函数会发出警告",{"2":{"89":1}}],["assign",{"2":{"89":22}}],["as",{"2":{"21":1,"42":1,"44":1,"53":1,"57":1,"76":1,"78":1,"89":1,"110":1,"152":1,"154":1,"159":1,"170":2,"187":3,"188":1}}],["a2",{"2":{"19":1}}],["a",{"0":{"134":1,"136":1,"137":1,"138":1,"139":1,"145":1,"146":1,"147":1,"149":1,"150":1,"151":1,"152":1,"153":1,"155":1,"156":1,"159":1,"160":1,"162":1,"163":1,"164":1,"165":1,"167":1,"168":1,"171":1,"172":1,"173":1,"174":1,"175":1},"1":{"135":1,"136":1,"137":2,"138":2,"139":2,"140":3,"141":3,"142":3,"143":3,"144":3,"145":2,"146":2,"147":1,"148":3,"149":2,"150":2,"151":2,"152":1,"153":1,"154":3,"155":1,"156":1,"157":3,"158":3,"159":1,"160":3,"161":3,"162":1,"163":1,"164":2,"165":2,"166":3,"167":3,"168":2,"169":3,"170":3,"171":1,"172":1,"173":1,"174":2,"175":2},"2":{"19":1,"21":2,"23":4,"24":2,"25":1,"28":1,"33":1,"41":1,"42":4,"43":3,"44":2,"46":2,"53":3,"54":3,"57":4,"58":3,"59":2,"60":1,"61":1,"63":7,"70":1,"71":1,"72":3,"77":3,"78":5,"82":1,"83":2,"89":1,"99":1,"105":1,"107":1,"116":1,"121":1,"122":1,"123":1,"129":1,"130":2,"149":2,"152":5,"154":4,"155":2,"156":1,"157":9,"159":3,"161":2,"165":3,"170":2,"172":1,"175":3,"182":1,"183":3,"184":1,"185":2,"186":2,"188":1,"190":1,"194":8}}],["analyzing",{"2":{"186":1}}],["analogous",{"2":{"63":1}}],["answering",{"2":{"182":1}}],["antiga",{"2":{"172":1}}],["an",{"2":{"11":1,"78":1,"79":1,"89":1,"182":2,"186":2,"194":2}}],["and",{"2":{"0":2,"42":3,"58":1,"78":35,"79":2,"80":1,"86":2,"88":3,"89":1,"148":1,"164":1,"172":2,"180":3,"182":1,"183":5,"184":3,"186":3,"190":6}}],["air或类似的笔记本电脑上",{"2":{"190":1}}],["air或类似笔记本电脑上大约需要5分钟",{"2":{"78":1}}],["ai创建的公开可用的数据集the",{"2":{"182":1}}],["ai的模型是一种类似gpt的开源模型",{"2":{"182":1}}],["ai",{"2":{"8":4,"9":1,"13":1,"138":9,"183":3,"186":1}}],["tuned",{"2":{"182":1}}],["two",{"2":{"164":1}}],["twiny",{"2":{"78":1}}],["twitter",{"2":{"11":1}}],["type",{"2":{"143":1}}],["tying",{"2":{"60":3,"179":1}}],["t4",{"2":{"140":1}}],["tmp",{"2":{"99":3}}],["tqdm>=4",{"2":{"88":1}}],["tqdm",{"2":{"88":1}}],["t",{"2":{"80":4,"89":6,"99":7,"101":1,"104":3,"107":1,"109":3,"110":1,"112":1,"116":1,"151":4,"178":3}}],["tip",{"2":{"194":4}}],["tie等人",{"2":{"185":1}}],["time",{"2":{"182":1}}],["timeit",{"2":{"175":2}}],["times",{"2":{"127":1}}],["ti",{"2":{"140":1}}],["tight",{"2":{"57":1,"78":1,"80":1}}],["title",{"2":{"57":1}}],["tiktoken",{"2":{"37":9,"40":1,"44":1,"53":3,"70":2,"79":1}}],["table",{"2":{"78":1}}],["tanh",{"2":{"57":1}}],["targets",{"2":{"45":3,"48":1,"71":5,"73":12}}],["target",{"2":{"43":6,"58":2,"71":4,"72":6,"77":6,"78":2,"188":1,"189":1,"190":3}}],["tree",{"2":{"186":1}}],["trends",{"2":{"136":1}}],["triu",{"2":{"121":1,"123":1,"129":1}}],["trillion",{"2":{"186":1}}],["tril",{"2":{"117":2}}],["true",{"2":{"57":1,"58":1,"89":1,"143":1,"144":1,"153":1,"161":5}}],["trf",{"2":{"53":2,"60":2,"89":33}}],["translate",{"2":{"184":1}}],["translation",{"2":{"183":1,"184":1}}],["transpose",{"2":{"123":1,"129":6,"130":1}}],["transformers",{"2":{"182":2}}],["transformerblock",{"2":{"59":4,"60":4,"179":1}}],["transformer",{"0":{"59":1},"2":{"7":4,"8":1,"10":1,"11":20,"14":8,"16":2,"18":1,"48":1,"51":1,"52":3,"53":5,"54":1,"57":2,"58":1,"59":17,"60":9,"62":3,"65":2,"95":2,"96":4,"97":1,"104":1,"121":1,"126":1,"133":1,"182":1,"184":1,"185":2}}],["track",{"2":{"78":3,"188":3,"189":3,"190":6}}],["train",{"2":{"77":11,"78":33,"86":1,"156":2,"157":7,"159":10,"160":2,"161":4,"165":10,"170":10,"180":1,"186":1,"187":3,"188":2,"189":1,"190":22}}],["trainable",{"2":{"60":2,"155":2,"174":1,"179":1}}],["training",{"2":{"14":1,"77":1,"78":1,"170":1,"182":2,"185":1,"186":2,"188":2,"189":2,"190":2}}],["technical",{"2":{"185":1}}],["temperature=1",{"2":{"83":1,"89":1}}],["temperatures",{"2":{"80":2}}],["temperature",{"2":{"80":5,"83":3}}],["tensors",{"2":{"150":1,"151":1,"164":1,"172":1}}],["tensor3d",{"2":{"149":1}}],["tensor2d",{"2":{"149":1,"151":9}}],["tensor1d",{"2":{"149":1,"150":2}}],["tensor0d",{"2":{"149":1}}],["tensorflow>=2",{"2":{"88":1}}],["tensorflow",{"2":{"55":1,"88":2}}],["tensor",{"2":{"43":2,"44":4,"45":2,"46":3,"47":1,"48":1,"53":3,"54":7,"55":2,"57":1,"58":2,"60":2,"63":7,"70":3,"71":5,"72":1,"74":1,"78":2,"80":2,"82":3,"83":1,"89":3,"99":4,"102":1,"103":1,"130":1,"149":6,"150":2,"151":2,"152":4,"154":8,"156":4,"157":10,"164":14}}],["terraces",{"2":{"34":2,"35":1,"37":2}}],["tea",{"2":{"30":1,"34":2,"35":1,"37":2}}],["test",{"2":{"23":4,"24":2,"156":2,"157":4,"161":1,"170":6}}],["text1",{"2":{"34":1}}],["text2",{"2":{"34":2}}],["text",{"2":{"21":3,"23":3,"24":1,"25":1,"28":6,"29":2,"30":1,"33":6,"34":3,"35":1,"37":1,"42":5,"44":2,"45":1,"48":1,"57":1,"63":8,"70":12,"71":13,"76":3,"77":3,"78":6,"79":7,"80":2,"83":3,"89":3,"177":2,"182":1,"183":1,"186":1,"187":6,"190":1}}],["txt2",{"2":{"53":2}}],["txt1",{"2":{"53":2}}],["txt",{"2":{"20":1,"21":1,"42":1,"43":2,"44":3,"76":1,"187":2}}],["togethercomputer",{"2":{"186":1}}],["together",{"2":{"186":1}}],["touvron等人",{"2":{"182":1}}],["toydataset",{"2":{"157":5}}],["topk",{"2":{"82":2,"83":1}}],["top",{"0":{"82":1},"2":{"82":9,"83":6,"89":2,"180":1}}],["towards",{"2":{"182":2}}],["toward",{"2":{"80":4,"89":1}}],["tolist",{"2":{"63":1,"70":1}}],["tok",{"2":{"53":4,"60":5,"89":2}}],["tokenization",{"2":{"183":1}}],["tokenizer",{"2":{"29":2,"30":1,"34":1,"35":2,"37":2,"42":3,"43":4,"44":2,"53":3,"63":2,"70":7,"71":2,"76":1,"78":4,"79":2,"83":2,"89":2,"177":3,"183":3,"190":1}}],["token长度训练的",{"2":{"89":1}}],["token的长度",{"2":{"89":1}}],["token的概率为0",{"2":{"82":1}}],["token的概率值为0",{"2":{"82":1}}],["token的选择概率等于原始softmax概率分数",{"2":{"80":1}}],["token概率分布会更接近于均匀分布",{"2":{"80":1}}],["tokens函数来打印",{"2":{"180":1}}],["tokens函数打印温度图5",{"2":{"81":1}}],["tokens=15",{"2":{"83":1}}],["tokens=10",{"2":{"70":1}}],["tokens=25",{"2":{"79":1,"89":1}}],["tokens=50",{"2":{"78":1}}],["tokens=6",{"2":{"63":1}}],["tokens",{"2":{"32":3,"54":1,"60":2,"61":1,"63":2,"76":6,"78":11,"80":2,"83":2,"123":3,"129":7,"130":2,"133":1,"186":1,"190":7}}],["token",{"2":{"27":2,"32":2,"43":4,"48":7,"53":5,"57":6,"59":3,"60":13,"63":34,"65":1,"70":19,"71":26,"72":4,"73":7,"74":3,"76":3,"77":2,"78":4,"79":4,"80":7,"82":3,"83":4,"89":5,"99":1}}],["torch==2",{"2":{"141":2}}],["torchaudio",{"2":{"141":1}}],["torchvision",{"2":{"141":1}}],["torch",{"0":{"142":1},"2":{"43":4,"46":3,"47":1,"48":5,"53":7,"54":5,"57":7,"58":3,"59":3,"60":4,"63":8,"68":1,"70":3,"71":5,"72":3,"73":5,"74":1,"77":10,"78":4,"80":10,"82":3,"83":8,"86":3,"89":2,"99":9,"101":2,"102":1,"105":6,"106":2,"108":1,"110":5,"111":1,"112":1,"113":1,"116":1,"117":2,"121":6,"122":1,"123":4,"124":1,"127":1,"128":1,"129":4,"130":1,"131":1,"140":1,"141":3,"142":4,"143":2,"144":1,"149":6,"150":3,"151":3,"152":6,"154":7,"155":14,"156":4,"157":4,"159":3,"161":8,"162":1,"164":3,"165":4,"166":4,"170":10,"175":2,"178":3,"180":2,"184":2,"186":1,"187":5,"188":1,"190":3}}],["total",{"2":{"21":1,"60":4,"61":5,"76":3,"77":4,"155":1,"161":3,"174":1,"179":9,"188":2,"189":2,"190":2}}],["to",{"2":{"0":1,"28":4,"33":5,"63":2,"70":6,"71":2,"77":3,"78":5,"79":3,"83":4,"89":5,"148":1,"150":2,"164":7,"165":3,"170":3,"172":1,"175":3,"182":1,"184":2,"186":1,"190":4}}],["three",{"2":{"186":1}}],["threw",{"2":{"78":1,"190":2}}],["thomas",{"2":{"172":1}}],["though",{"2":{"21":1}}],["thought",{"2":{"21":1,"25":1}}],["that",{"2":{"89":1}}],["there",{"2":{"78":1,"190":2}}],["theme",{"0":{"2":1},"2":{"0":4,"2":1}}],["the",{"2":{"0":3,"5":2,"13":1,"16":1,"20":2,"21":1,"29":2,"30":1,"34":4,"35":2,"37":2,"42":2,"44":1,"61":2,"75":1,"76":2,"78":5,"79":2,"90":1,"164":1,"179":1,"182":2,"185":2,"186":4,"187":3,"190":6,"192":1,"195":2}}],["this",{"2":{"0":1,"11":1,"23":4,"24":2,"192":1,"194":10}}],["brown等人",{"2":{"182":1,"185":1}}],["break",{"2":{"27":1,"77":1}}],["built",{"2":{"192":1}}],["but",{"2":{"164":1}}],["buffer",{"2":{"123":1,"124":2,"129":1}}],["blob",{"2":{"183":1}}],["bloomberg团队通过在金融数据上从头预训练的gpt版本展示了这一点",{"2":{"182":1}}],["bloomberggpt是一个在金融领域专门设计的大型语言模型",{"2":{"186":1}}],["bloomberggpt",{"2":{"10":1,"182":1,"186":1}}],["blog",{"2":{"172":2,"185":1}}],["block",{"2":{"59":3,"178":2,"179":2,"194":2}}],["blocks",{"2":{"53":2,"60":2,"89":48,"184":1}}],["biderman等人",{"2":{"186":1}}],["bidirectional",{"2":{"182":1}}],["binary",{"2":{"152":1,"154":1}}],["bincount",{"2":{"80":1}}],["bing",{"2":{"9":1}}],["bias=true",{"2":{"155":3}}],["bias=qkv",{"2":{"112":3,"123":3,"129":3}}],["bias=cfg",{"2":{"59":1}}],["bias=false",{"2":{"53":1,"60":1,"112":1,"123":1,"127":1,"129":1}}],["bias",{"2":{"53":3,"59":1,"70":1,"89":13,"112":3,"123":3,"127":1,"129":3,"187":1}}],["ba",{"2":{"185":1}}],["bahdanau",{"2":{"96":3,"184":1}}],["bashcheckpoint",{"2":{"88":1}}],["bashpip",{"2":{"37":1,"88":1,"140":1,"141":1}}],["bar",{"2":{"80":4}}],["bard",{"2":{"9":1}}],["backend=",{"2":{"170":2}}],["backends",{"2":{"144":1,"166":1}}],["back",{"2":{"78":1,"190":2}}],["backward",{"2":{"58":3,"78":1,"154":3,"155":1,"159":1,"160":1,"165":1,"190":4}}],["batchsize",{"2":{"170":1}}],["batches=eval",{"2":{"78":2}}],["batches=none",{"2":{"77":1}}],["batches",{"2":{"77":7}}],["batch",{"2":{"44":8,"45":1,"48":1,"53":8,"54":3,"55":1,"60":5,"61":1,"63":1,"71":4,"77":17,"78":6,"122":1,"124":2,"128":2,"131":3,"157":8,"159":9,"160":1,"165":9,"170":1,"177":2,"187":2,"188":2,"189":2,"190":10}}],["bpe",{"0":{"39":1,"41":1},"2":{"36":1,"37":4,"38":3,"39":3,"40":1,"41":2,"42":2,"46":1,"48":1,"49":1,"53":1,"88":1,"183":2}}],["box",{"2":{"194":2}}],["bonus",{"2":{"186":1}}],["bool",{"2":{"121":1,"123":1,"129":3}}],["books2",{"2":{"12":1,"13":1}}],["books1",{"2":{"12":1,"13":1}}],["bos",{"2":{"36":1}}],["both",{"2":{"0":1}}],["b",{"0":{"181":1,"182":1,"183":1,"184":1,"185":1,"186":1},"1":{"182":1,"183":1,"184":1,"185":1,"186":1},"2":{"10":1,"11":1,"28":1,"33":1,"41":1,"43":1,"44":1,"53":1,"59":1,"61":1,"63":1,"70":1,"72":1,"77":2,"78":3,"82":1,"83":1,"89":61,"105":1,"121":1,"123":3,"129":6,"130":2,"149":1,"152":4,"154":6,"155":1,"157":2,"159":1,"161":1,"165":1,"170":2,"175":5,"188":1,"189":1,"190":1}}],["beam",{"2":{"186":1}}],["better",{"2":{"184":1,"185":1}}],["behavior",{"2":{"14":1}}],["bert",{"2":{"11":6,"12":1,"182":1}}],["beispiel",{"2":{"11":1}}],["be",{"2":{"0":1,"89":1,"129":1,"164":1}}],["byeswickattribute",{"2":{"63":1}}],["bytes",{"2":{"61":2}}],["by",{"2":{"0":1,"14":1,"78":1,"129":1,"184":1,"186":2,"190":2,"192":1,"193":1}}],["png",{"2":{"190":1}}],["peak",{"2":{"188":4,"189":2,"190":4}}],["penedo等人",{"2":{"186":1}}],["peng等人",{"2":{"182":1}}],["pt",{"2":{"162":1}}],["pth是pytorch文件的常用扩展名",{"2":{"86":1}}],["pth",{"2":{"86":5,"162":6,"180":1}}],["p",{"2":{"60":4,"155":3,"174":3,"179":4}}],["plot",{"2":{"57":1,"78":5,"188":1,"189":1}}],["plt",{"2":{"57":10,"78":3,"80":3,"188":5,"189":3}}],["platform",{"2":{"183":1}}],["placeholder",{"2":{"53":1}}],["plaintextcharacters",{"2":{"76":1}}],["plaintextflattened",{"2":{"73":1}}],["plaintextlogits",{"2":{"73":1}}],["plaintextlayers",{"2":{"58":2}}],["plaintexthello",{"2":{"63":1}}],["plaintextencoded",{"2":{"63":1}}],["plaintextnumber",{"2":{"60":1}}],["plaintextnormalized",{"2":{"54":1}}],["plaintextinput",{"2":{"59":1,"60":1}}],["plaintextinputs",{"2":{"45":1}}],["plaintextmean",{"2":{"54":2,"55":1}}],["plaintextoutput",{"2":{"53":1,"63":1,"70":1}}],["plaintexttraining",{"2":{"77":1}}],["plaintexttrain",{"2":{"77":1}}],["plaintexttext",{"2":{"71":1}}],["plaintexttensor",{"2":{"46":1,"47":1,"53":1,"54":1,"72":2,"73":1}}],["plaintexttargets",{"2":{"71":1}}],["plaintexttorch",{"2":{"48":3,"57":1,"71":1}}],["plaintexttoken",{"2":{"48":1,"60":1,"71":1}}],["plaintexttotal",{"2":{"21":1,"60":1,"61":1}}],["plaintextparameter",{"2":{"46":1}}],["plaintextand",{"2":{"42":1}}],["plaintextx",{"2":{"42":1}}],["plaintextkeyerror",{"2":{"30":1}}],["plaintext4649",{"2":{"25":1}}],["plaintext",{"2":{"23":3,"24":1,"25":1,"27":1,"29":2,"32":1,"34":2,"35":1,"37":2,"42":1,"44":2}}],["py",{"2":{"88":3,"183":1}}],["pyplot",{"2":{"57":1,"78":1,"188":1}}],["pythia",{"2":{"186":1}}],["pythonplt",{"2":{"189":1}}],["pythonpredictions",{"2":{"161":3}}],["pythonpreprocessed",{"2":{"25":1}}],["pythonprobas",{"2":{"80":1}}],["pythonprint",{"2":{"25":1,"29":1,"35":1,"46":1,"47":1,"58":1,"60":1,"71":1,"73":1,"77":1,"89":2,"102":1,"103":1,"130":1,"144":1,"151":6,"154":1,"155":3,"157":1,"161":2,"164":1,"177":2}}],["pythonoptimizer",{"2":{"188":1}}],["pythonout",{"2":{"54":1}}],["pythonoutput",{"2":{"48":1}}],["python包中的galoreadamw优化器替换pytorch的adamw优化器",{"2":{"186":1}}],["python主类实现重新组织成更小的子模块",{"2":{"185":1}}],["pythonhparams",{"2":{"180":2}}],["pythonblock",{"2":{"178":1,"179":1}}],["pythonbatch",{"2":{"122":1}}],["pythonn",{"2":{"188":1}}],["pythonnum",{"2":{"155":1}}],["pythonnew",{"2":{"82":1,"89":2}}],["pythonnext",{"2":{"80":1}}],["pythonneg",{"2":{"72":1}}],["pythonrow",{"2":{"119":1}}],["pythonres",{"2":{"99":1}}],["pythonresult",{"2":{"23":2}}],["pythonqueries",{"2":{"116":1}}],["pythonquery",{"2":{"99":2,"106":1}}],["pythonkeys",{"2":{"106":1,"107":1}}],["pythonx",{"2":{"105":1,"156":1}}],["pythona",{"2":{"130":1,"175":2}}],["pythonattn",{"2":{"99":2,"101":2,"102":1,"107":1,"121":1}}],["pythonavg",{"2":{"72":1}}],["pythonall",{"2":{"26":1,"32":1,"103":1}}],["pythonload",{"2":{"89":1}}],["pythonloss",{"2":{"73":1,"154":1}}],["pythonlogits",{"2":{"73":1}}],["pythonlog",{"2":{"72":1}}],["pythonlayer",{"2":{"58":1}}],["pythonln",{"2":{"55":1}}],["pythonmask",{"2":{"121":1}}],["pythonmasked",{"2":{"118":1}}],["pythonmax",{"2":{"48":1}}],["pythonmodel",{"2":{"63":1,"79":1,"86":1,"89":2,"155":1,"161":1,"162":1,"174":1}}],["pythonmean",{"2":{"54":1}}],["pythongpt",{"2":{"53":1,"89":1,"179":1}}],["pythonif",{"2":{"170":1}}],["pythoninputs",{"2":{"71":1}}],["pythoninput",{"2":{"46":1,"48":1}}],["pythonimport",{"2":{"23":1,"43":1,"53":1,"57":1,"70":1,"78":1,"88":1,"89":1,"99":1,"110":1,"141":1,"143":1,"149":1,"152":1,"154":1,"159":1,"170":1,"187":2,"188":1,"189":1}}],["pythond",{"2":{"108":1,"178":1}}],["pythondevice",{"2":{"77":1,"165":1,"166":2}}],["pythondecoded",{"2":{"63":1}}],["pythondef",{"2":{"44":1,"58":1,"77":2,"78":3,"80":2,"83":1,"89":1,"99":1,"161":1,"170":3,"190":1}}],["pythondataloader",{"2":{"45":1,"177":2}}],["pythonsa",{"2":{"178":1}}],["pythonstart",{"2":{"63":1}}],["pythonstrings",{"2":{"37":1}}],["pythonsecond",{"2":{"44":1}}],["pythoncheckpoint",{"2":{"86":1,"180":1}}],["pythoncontext",{"2":{"42":1,"48":1,"109":1,"117":1}}],["pythonclass",{"2":{"28":1,"33":1,"112":1,"123":1,"127":1,"129":1,"155":1}}],["pythonenc",{"2":{"42":1}}],["pythonfloatvec",{"2":{"150":2}}],["pythonfile",{"2":{"76":1}}],["pythonffn",{"2":{"57":1}}],["pythonfrom",{"2":{"37":1,"77":1,"88":1,"157":2,"187":1,"190":2}}],["pythonfor",{"2":{"32":1,"42":2,"157":2}}],["pythontensor2d",{"2":{"151":1}}],["pythontensor1d",{"2":{"150":1}}],["pythontensor",{"2":{"130":1,"164":2}}],["pythontemperatures",{"2":{"80":1}}],["pythontext1",{"2":{"34":1}}],["pythontext",{"2":{"24":1,"30":1,"37":1,"71":1}}],["pythontrain",{"2":{"77":1,"157":1,"180":1}}],["pythontargets",{"2":{"71":1}}],["pythontopk",{"2":{"82":1}}],["pythontop",{"2":{"82":1}}],["pythontotal",{"2":{"60":2,"61":1,"76":1}}],["pythontoken",{"2":{"48":1,"71":1}}],["pythontokenizer",{"2":{"29":1,"34":1,"37":1,"79":1}}],["pythontorch",{"2":{"46":1,"53":1,"54":2,"58":1,"59":1,"60":1,"78":1,"80":1,"83":1,"86":2,"89":1,"105":1,"110":1,"112":1,"121":1,"122":1,"124":1,"128":1,"131":1,"155":2,"161":2,"162":1,"165":1,"190":2}}],["pythonvocab",{"2":{"27":1,"46":1,"80":1}}],["pythonwith",{"2":{"21":1,"42":1,"44":1,"71":1,"155":2}}],["python",{"0":{"21":1,"140":1},"2":{"20":1,"23":1,"24":1,"26":2,"27":1,"37":2,"46":1,"47":1,"53":3,"54":1,"57":2,"58":1,"59":1,"60":1,"63":1,"65":1,"70":1,"88":5,"89":2,"109":2,"122":1,"135":1,"136":1,"139":1,"140":5,"142":3,"143":1,"148":1,"150":2,"157":1,"162":2}}],["pytorch计算损失梯度",{"2":{"190":1}}],["pytorch还实现了基于scaled",{"2":{"184":1}}],["pytorch实现了支持flashattention的自注意力和因果注意力功能",{"2":{"184":1}}],["pytorch将此功能保留为选项",{"2":{"172":1}}],["pytorch包含dataset和dataloader类",{"2":{"171":1}}],["pytorch中的深度学习工具提供了构建自定义深度神经网络的模块",{"2":{"171":1}}],["pytorch中的自动微分",{"2":{"171":1}}],["pytorch的multiprocessing子模块包含如multiprocessing",{"2":{"170":1}}],["pytorch的distributeddataparallel",{"2":{"169":1}}],["pytorch分布式训练的工具",{"2":{"170":1}}],["pytorch会在每个gpu上启动一个单独的进程",{"2":{"169":1}}],["pytorch张量可以在cpu上执行",{"2":{"171":1}}],["pytorch张量还支持gpu计算",{"2":{"147":1}}],["pytorch张量类似于",{"2":{"147":1}}],["pytorch",{"0":{"86":1,"111":1,"112":1,"134":1,"136":1,"137":1,"139":1,"142":1,"144":1,"148":1,"151":1,"164":1,"166":1},"1":{"87":1,"135":1,"136":1,"137":2,"138":2,"139":2,"140":3,"141":3,"142":3,"143":3,"144":3,"145":2,"146":2,"147":1,"148":1,"149":1,"150":1,"151":1,"152":1,"153":1,"154":2,"155":1,"156":1,"157":2,"158":2,"159":1,"160":2,"161":2,"162":1,"163":1,"164":1,"165":1,"166":1,"167":1,"168":1,"169":1,"170":1,"171":1,"172":1,"173":1,"174":1,"175":1},"2":{"42":4,"43":5,"46":1,"48":1,"49":1,"54":1,"57":2,"58":2,"63":1,"68":2,"72":1,"73":3,"88":1,"110":2,"111":1,"117":1,"121":1,"124":1,"135":9,"136":7,"137":7,"138":1,"139":2,"140":4,"141":12,"142":1,"143":2,"144":5,"145":1,"147":2,"148":2,"149":3,"150":4,"151":8,"152":4,"153":2,"154":6,"155":4,"156":3,"157":6,"159":2,"160":1,"161":2,"162":1,"163":1,"164":5,"171":2,"172":5,"184":2}}],["poli等人",{"2":{"182":1}}],["port",{"2":{"170":1}}],["powered",{"2":{"193":1}}],["pow",{"2":{"57":1}}],["positions",{"2":{"82":2}}],["post",{"2":{"59":1,"185":2}}],["pos",{"2":{"48":7,"53":4,"60":4,"78":1,"82":2,"89":2}}],["phi",{"2":{"57":2}}],["pin",{"2":{"170":2}}],["pizza",{"2":{"80":3,"81":2,"82":1,"180":3}}],["pi",{"2":{"57":1,"189":1,"190":1}}],["pip",{"2":{"37":1,"140":1}}],["pile",{"2":{"13":2,"182":2,"186":1}}],["pdf",{"2":{"14":1,"172":2,"185":1}}],["path",{"2":{"76":2,"187":5}}],["partitioning",{"2":{"184":1}}],["parallelism",{"2":{"184":1}}],["parallel",{"2":{"170":1,"172":2}}],["params",{"2":{"60":5,"61":1,"88":2,"89":27,"155":2,"174":2,"179":4,"180":2}}],["param",{"2":{"58":2,"188":4,"189":4,"190":10}}],["parameters",{"2":{"58":2,"60":6,"68":1,"78":1,"86":1,"88":1,"155":3,"159":1,"165":1,"170":1,"174":2,"179":8,"180":1,"188":1,"190":4}}],["parameter",{"2":{"54":2,"89":3,"105":3,"110":3,"111":1,"113":1,"178":3}}],["pardonable",{"2":{"29":2}}],["pad",{"2":{"36":2}}],["palace",{"2":{"34":2,"35":1}}],["painted",{"2":{"29":2}}],["paperswithcode",{"2":{"136":1}}],["papers",{"2":{"136":1,"184":1}}],["paper",{"2":{"14":1}}],["page",{"0":{"3":1,"4":1},"2":{"0":7,"3":1,"192":1}}],["practice",{"2":{"89":1}}],["pride",{"2":{"29":2}}],["printoptions",{"2":{"54":1,"161":1}}],["print",{"2":{"21":2,"23":3,"24":1,"25":1,"26":1,"27":1,"29":1,"32":3,"34":2,"37":3,"42":5,"44":2,"45":2,"46":2,"48":7,"53":3,"54":8,"55":2,"57":1,"58":5,"59":2,"60":6,"61":1,"63":5,"70":1,"71":5,"72":3,"73":4,"76":2,"77":5,"78":8,"79":2,"80":6,"82":4,"83":1,"89":3,"99":10,"101":2,"102":1,"103":1,"106":3,"107":2,"108":1,"109":1,"110":1,"112":1,"116":1,"117":1,"118":1,"119":1,"121":3,"122":2,"124":1,"128":2,"131":2,"150":3,"151":1,"154":3,"155":7,"157":2,"159":1,"161":4,"164":2,"165":1,"170":4,"174":1,"177":1,"179":2,"190":5}}],["prize",{"2":{"8":1}}],["pretraining",{"2":{"186":3}}],["pretrained",{"2":{"14":1,"52":1}}],["prevent",{"2":{"184":1}}],["previous",{"2":{"59":1,"103":1,"187":2,"190":2}}],["prepare",{"2":{"170":2}}],["preprocessed",{"2":{"25":4,"26":2,"28":4,"32":1,"33":6}}],["predictions",{"2":{"161":5}}],["pre",{"2":{"14":1,"59":1,"182":1,"185":1,"186":1}}],["pre>",{"2":{"0":6}}],["provides",{"2":{"193":1}}],["provided",{"2":{"0":1,"192":1}}],["progress",{"2":{"189":2,"190":2}}],["product函数的高效多头注意力类",{"2":{"184":1}}],["product",{"2":{"184":2}}],["processing",{"2":{"183":1}}],["process",{"2":{"170":9}}],["proj",{"2":{"89":10,"129":2,"130":1}}],["projection",{"2":{"186":1}}],["project",{"2":{"13":1,"75":1,"186":1}}],["probs",{"2":{"83":2}}],["probability",{"2":{"80":1}}],["probas",{"2":{"63":2,"71":9,"72":14,"80":7,"82":2,"161":3,"180":1}}]],"serializationVersion":2}';export{t as default};
