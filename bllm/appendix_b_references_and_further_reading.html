<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>附录 B. 参考文献及延伸阅读 | 大模型知识库</title>
    <meta name="description" content="A VitePress Site">
    <meta name="generator" content="VitePress v1.5.0">
    <link rel="preload stylesheet" href="/dllm/assets/style.D3kgSBoU.css" as="style">
    <link rel="preload stylesheet" href="/dllm/vp-icons.css" as="style">
    
    <script type="module" src="/dllm/assets/app.mXzA32WD.js"></script>
    <link rel="preload" href="/dllm/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/dllm/assets/chunks/theme.BygCzp_D.js">
    <link rel="modulepreload" href="/dllm/assets/chunks/framework.Bzjhwp8w.js">
    <link rel="modulepreload" href="/dllm/assets/bllm_appendix_b_references_and_further_reading.md.D476jJpX.lean.js">
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-04082acb><!--[--><!--]--><!--[--><span tabindex="-1" data-v-2abd97fb></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-2abd97fb> Skip to content </a><!--]--><!----><header class="VPNav" data-v-04082acb data-v-4d8a2a03><div class="VPNavBar" data-v-4d8a2a03 data-v-d662bdbc><div class="wrapper" data-v-d662bdbc><div class="container" data-v-d662bdbc><div class="title" data-v-d662bdbc><div class="VPNavBarTitle has-sidebar" data-v-d662bdbc data-v-851315c7><a class="title" href="/dllm/" data-v-851315c7><!--[--><!--]--><!----><span data-v-851315c7>大模型知识库</span><!--[--><!--]--></a></div></div><div class="content" data-v-d662bdbc><div class="content-body" data-v-d662bdbc><!--[--><!--]--><div class="VPNavBarSearch search" data-v-d662bdbc><!--[--><!----><div id="local-search"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><span class="vp-icon DocSearch-Search-Icon"></span><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-d662bdbc data-v-d3871c33><span id="main-nav-aria-label" class="visually-hidden" data-v-d3871c33> Main Navigation </span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/dllm/" tabindex="0" data-v-d3871c33 data-v-abb8c4e8><!--[--><span data-v-abb8c4e8>首页</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/dllm/markdown-examples.html" tabindex="0" data-v-d3871c33 data-v-abb8c4e8><!--[--><span data-v-abb8c4e8>示例</span><!--]--></a><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-d662bdbc data-v-cc58c11e><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-cc58c11e data-v-e81c969b data-v-5d4aee43><span class="check" data-v-5d4aee43><span class="icon" data-v-5d4aee43><!--[--><span class="vpi-sun sun" data-v-e81c969b></span><span class="vpi-moon moon" data-v-e81c969b></span><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-d662bdbc data-v-d3159530 data-v-c5f7a4ac><!--[--><a class="VPSocialLink no-icon" href="https://github.com/vuejs/vitepress" aria-label="github" target="_blank" rel="noopener" data-v-c5f7a4ac data-v-acb7c684><span class="vpi-social-github"></span></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-d662bdbc data-v-42c56eeb data-v-6c050606><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-6c050606><span class="vpi-more-horizontal icon" data-v-6c050606></span></button><div class="menu" data-v-6c050606><div class="VPMenu" data-v-6c050606 data-v-a0042935><!----><!--[--><!--[--><!----><div class="group" data-v-42c56eeb><div class="item appearance" data-v-42c56eeb><p class="label" data-v-42c56eeb>Appearance</p><div class="appearance-action" data-v-42c56eeb><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-42c56eeb data-v-e81c969b data-v-5d4aee43><span class="check" data-v-5d4aee43><span class="icon" data-v-5d4aee43><!--[--><span class="vpi-sun sun" data-v-e81c969b></span><span class="vpi-moon moon" data-v-e81c969b></span><!--]--></span></span></button></div></div></div><div class="group" data-v-42c56eeb><div class="item social-links" data-v-42c56eeb><div class="VPSocialLinks social-links-list" data-v-42c56eeb data-v-c5f7a4ac><!--[--><a class="VPSocialLink no-icon" href="https://github.com/vuejs/vitepress" aria-label="github" target="_blank" rel="noopener" data-v-c5f7a4ac data-v-acb7c684><span class="vpi-social-github"></span></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-d662bdbc data-v-716a6ba8><span class="container" data-v-716a6ba8><span class="top" data-v-716a6ba8></span><span class="middle" data-v-716a6ba8></span><span class="bottom" data-v-716a6ba8></span></span></button></div></div></div></div><div class="divider" data-v-d662bdbc><div class="divider-line" data-v-d662bdbc></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-04082acb data-v-3a93fdb1><div class="container" data-v-3a93fdb1><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-3a93fdb1><span class="vpi-align-left menu-icon" data-v-3a93fdb1></span><span class="menu-text" data-v-3a93fdb1>Menu</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-3a93fdb1 data-v-9fecb94d><button data-v-9fecb94d>Return to top</button><!----></div></div></div><aside class="VPSidebar" data-v-04082acb data-v-e9366181><div class="curtain" data-v-e9366181></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-e9366181><span class="visually-hidden" id="sidebar-aria-label" data-v-e9366181> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-999dfcb0><section class="VPSidebarItem level-0 has-active" data-v-999dfcb0 data-v-f88eca86><div class="item" role="button" tabindex="0" data-v-f88eca86><div class="indicator" data-v-f88eca86></div><h2 class="text" data-v-f88eca86>书籍</h2><!----></div><div class="items" data-v-f88eca86><!--[--><div class="VPSidebarItem level-1 is-link" data-v-f88eca86 data-v-f88eca86><div class="item" data-v-f88eca86><div class="indicator" data-v-f88eca86></div><a class="VPLink link link" href="/dllm/markdown-examples.html" data-v-f88eca86><!--[--><p class="text" data-v-f88eca86>Markdown Examples</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f88eca86 data-v-f88eca86><div class="item" data-v-f88eca86><div class="indicator" data-v-f88eca86></div><a class="VPLink link link" href="/dllm/api-examples.html" data-v-f88eca86><!--[--><p class="text" data-v-f88eca86>Runtime API Examples</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f88eca86 data-v-f88eca86><div class="item" data-v-f88eca86><div class="indicator" data-v-f88eca86></div><a class="VPLink link link" href="/dllm/bllm/" data-v-f88eca86><!--[--><p class="text" data-v-f88eca86>欢迎</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f88eca86 data-v-f88eca86><div class="item" data-v-f88eca86><div class="indicator" data-v-f88eca86></div><a class="VPLink link link" href="/dllm/bllm/1_understanding_large_language_models.html" data-v-f88eca86><!--[--><p class="text" data-v-f88eca86>理解大型语言模型</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f88eca86 data-v-f88eca86><div class="item" data-v-f88eca86><div class="indicator" data-v-f88eca86></div><a class="VPLink link link" href="/dllm/bllm/2_working_with_text_data.html" data-v-f88eca86><!--[--><p class="text" data-v-f88eca86>处理文本数据</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f88eca86 data-v-f88eca86><div class="item" data-v-f88eca86><div class="indicator" data-v-f88eca86></div><a class="VPLink link link" href="/dllm/bllm/3_coding_attention_mechanisms.html" data-v-f88eca86><!--[--><p class="text" data-v-f88eca86>编写注意力机制</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f88eca86 data-v-f88eca86><div class="item" data-v-f88eca86><div class="indicator" data-v-f88eca86></div><a class="VPLink link link" href="/dllm/bllm/4_implementing_a_gpt_model_from_scratch_to_generate_text.html" data-v-f88eca86><!--[--><p class="text" data-v-f88eca86>从零实现GPT模型生成文本</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f88eca86 data-v-f88eca86><div class="item" data-v-f88eca86><div class="indicator" data-v-f88eca86></div><a class="VPLink link link" href="/dllm/bllm/5_pretraining_on_unlabeled_data.html" data-v-f88eca86><!--[--><p class="text" data-v-f88eca86> 在无标签数据上预训练</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f88eca86 data-v-f88eca86><div class="item" data-v-f88eca86><div class="indicator" data-v-f88eca86></div><a class="VPLink link link" href="/dllm/bllm/appendix_a_introduction_to_pytorch.html" data-v-f88eca86><!--[--><p class="text" data-v-f88eca86>附录 A. PyTorch简介</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f88eca86 data-v-f88eca86><div class="item" data-v-f88eca86><div class="indicator" data-v-f88eca86></div><a class="VPLink link link" href="/dllm/bllm/appendix_b_references_and_further_reading.html" data-v-f88eca86><!--[--><p class="text" data-v-f88eca86>附录 B. 参考文献与进一步阅读</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f88eca86 data-v-f88eca86><div class="item" data-v-f88eca86><div class="indicator" data-v-f88eca86></div><a class="VPLink link link" href="/dllm/bllm/appendix_c_exercise_solutions.html" data-v-f88eca86><!--[--><p class="text" data-v-f88eca86>附录 C. 习题解答</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f88eca86 data-v-f88eca86><div class="item" data-v-f88eca86><div class="indicator" data-v-f88eca86></div><a class="VPLink link link" href="/dllm/bllm/appendix_d_adding_bells_and_whistles_to_the_training_loop.html" data-v-f88eca86><!--[--><p class="text" data-v-f88eca86>附录 D. 给训练循环添加附加功能</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-999dfcb0><section class="VPSidebarItem level-0" data-v-999dfcb0 data-v-f88eca86><div class="item" role="button" tabindex="0" data-v-f88eca86><div class="indicator" data-v-f88eca86></div><h2 class="text" data-v-f88eca86>课程</h2><!----></div><div class="items" data-v-f88eca86><!--[--><div class="VPSidebarItem level-1 is-link" data-v-f88eca86 data-v-f88eca86><div class="item" data-v-f88eca86><div class="indicator" data-v-f88eca86></div><a class="VPLink link link" href="/dllm/course/1.html" data-v-f88eca86><!--[--><p class="text" data-v-f88eca86>课程1</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f88eca86 data-v-f88eca86><div class="item" data-v-f88eca86><div class="indicator" data-v-f88eca86></div><a class="VPLink link link" href="/dllm/course/2.html" data-v-f88eca86><!--[--><p class="text" data-v-f88eca86>课程2</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-04082acb data-v-472f5592><div class="VPDoc has-sidebar has-aside" data-v-472f5592 data-v-56ee120b><!--[--><!--]--><div class="container" data-v-56ee120b><div class="aside" data-v-56ee120b><div class="aside-curtain" data-v-56ee120b></div><div class="aside-container" data-v-56ee120b><div class="aside-content" data-v-56ee120b><div class="VPDocAside" data-v-56ee120b data-v-3b40b7c4><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-3b40b7c4 data-v-3861d3c5><div class="content" data-v-3861d3c5><div class="outline-marker" data-v-3861d3c5></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-3861d3c5>On this page</div><ul class="VPDocOutlineItem root" data-v-3861d3c5 data-v-6358f6a3><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-3b40b7c4></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-56ee120b><div class="content-container" data-v-56ee120b><!--[--><!--]--><main class="main" data-v-56ee120b><div style="position:relative;" class="vp-doc _dllm_bllm_appendix_b_references_and_further_reading" data-v-56ee120b><div><h1 id="附录-b-参考文献及延伸阅读" tabindex="-1">附录 B. 参考文献及延伸阅读 <a class="header-anchor" href="#附录-b-参考文献及延伸阅读" aria-label="Permalink to &quot;附录 B. 参考文献及延伸阅读&quot;">​</a></h1><h2 id="b-1-第1章" tabindex="-1">B.1 第1章 <a class="header-anchor" href="#b-1-第1章" aria-label="Permalink to &quot;B.1 第1章&quot;">​</a></h2><p>自定义的LLM（大型语言模型）能够在特定领域的任务上优于通用LLM。Bloomberg团队通过在金融数据上从头预训练的GPT版本展示了这一点。这个定制的LLM在金融任务上超越了ChatGPT，同时在通用LLM基准测试中也保持了良好的表现：</p><ul><li><strong>BloombergGPT: A Large Language Model for Finance (2023)</strong> ，作者：Wu等人，<a href="https://arxiv.org/abs/2303.17564" target="_blank" rel="noreferrer">https://arxiv.org/abs/2303.17564</a></li></ul><p>现有的LLM也可以通过微调来适应特定领域，从而在特定任务上超越通用LLM。谷歌研究和DeepMind的团队在医疗领域展示了这一点：</p><ul><li><strong>Towards Expert-Level Medical Question Answering with Large Language Models (2023)</strong> ，作者：Singhal等人，<a href="https://arxiv.org/abs/2305.09617" target="_blank" rel="noreferrer">https://arxiv.org/abs/2305.09617</a></li></ul><p>提出最初Transformer架构的论文：</p><ul><li><strong>Attention Is All You Need (2017)</strong> ，作者：Vaswani等人，<a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noreferrer">https://arxiv.org/abs/1706.03762</a></li></ul><p>最早的编码器风格Transformer模型，称为BERT：</p><ul><li><strong>BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2018)</strong> ，作者：Devlin等人，<a href="https://arxiv.org/abs/1810.04805" target="_blank" rel="noreferrer">https://arxiv.org/abs/1810.04805</a></li></ul><p>描述解码器风格GPT-3模型的论文，该模型启发了现代LLM，并将作为本书中从头实现LLM的模板：</p><ul><li><strong>Language Models are Few-Shot Learners (2020)</strong> ，作者：Brown等人，<a href="https://arxiv.org/abs/2005.14165" target="_blank" rel="noreferrer">https://arxiv.org/abs/2005.14165</a></li></ul><p>用于图像分类的最早视觉Transformer，展示了Transformer架构不仅适用于文本输入：</p><ul><li><strong>An images/image is Worth 16x16 Words: Transformers for images/image Recognition at Scale (2020)</strong> ，作者：Dosovitskiy等人，<a href="https://arxiv.org/abs/2010.11929" target="_blank" rel="noreferrer">https://arxiv.org/abs/2010.11929</a></li></ul><p>两种实验性（但不太流行）的LLM架构示例，说明并非所有LLM都基于Transformer架构：</p><ul><li><strong>RWKV: Reinventing RNNs for the Transformer Era (2023)</strong> ，作者：Peng等人，<a href="https://arxiv.org/abs/2305.13048" target="_blank" rel="noreferrer">https://arxiv.org/abs/2305.13048</a></li><li><strong>Hyena Hierarchy: Towards Larger Convolutional Language Models (2023)</strong> ，作者：Poli等人，<a href="https://arxiv.org/abs/2302.10866" target="_blank" rel="noreferrer">https://arxiv.org/abs/2302.10866</a></li><li><strong>Mamba: Linear-Time Sequence Modeling with Selective State Spaces (2023)</strong> ，作者：Gu和Dao，<a href="https://arxiv.org/abs/2312.00752" target="_blank" rel="noreferrer">https://arxiv.org/abs/2312.00752</a></li></ul><p>Meta AI的模型是一种类似GPT的开源模型，与GPT-3和ChatGPT不同，它是公开可用的：</p><ul><li><strong>Llama 2: Open Foundation and Fine-Tuned Chat Models (2023)</strong> ，作者：Touvron等人，<a href="https://arxiv.org/abs/2307.092881" target="_blank" rel="noreferrer">https://arxiv.org/abs/2307.092881</a></li></ul><p>对于那些对1.5节中提到的数据集感兴趣的读者，以下论文介绍了Eleuther AI创建的公开可用的数据集The Pile：</p><ul><li><strong>The Pile: An 800GB Dataset of Diverse Text for Language Modeling (2020)</strong> ，作者：Gao等人，<a href="https://arxiv.org/abs/2101.00027" target="_blank" rel="noreferrer">https://arxiv.org/abs/2101.00027</a></li></ul><p>以下论文提供了InstructGPT的参考文献，InstructGPT用于GPT-3的微调，该模型在1.6节中提到，将在第7章中详细讨论：</p><ul><li><strong>Training Language Models to Follow Instructions with Human Feedback (2022)</strong> ，作者：Ouyang等人，<a href="https://arxiv.org/abs/2203.02155" target="_blank" rel="noreferrer">https://arxiv.org/abs/2203.02155</a></li></ul><h2 id="b-2-第2章" tabindex="-1">B.2 第2章 <a class="header-anchor" href="#b-2-第2章" aria-label="Permalink to &quot;B.2 第2章&quot;">​</a></h2><p>对嵌入空间、潜在空间及向量表示的概念感兴趣的读者，可以参考我书中《Machine Learning Q and AI》的第一章，了解更多信息：</p><ul><li><strong>Machine Learning Q and AI (2023)</strong> ，作者：Sebastian Raschka， <a href="https://leanpub.com/machine-learning-q-and-ai" target="_blank" rel="noreferrer">https://leanpub.com/machine-learning-q-and-ai</a></li></ul><p>以下论文深入探讨了如何使用字节对编码（BPE）作为一种分词方法：</p><ul><li><strong>Neural Machine Translation of Rare Words with Subword Units (2015)</strong> ，作者：Sennrich等人， <a href="https://arxiv.org/abs/1508.07909" target="_blank" rel="noreferrer">https://arxiv.org/abs/1508.07909</a></li></ul><p>OpenAI开源了用于训练GPT-2的字节对编码分词器代码：</p><ul><li><a href="https://github.com/openai/gpt-2/blob/master/src/encoder.py" target="_blank" rel="noreferrer">https://github.com/openai/gpt-2/blob/master/src/encoder.py</a></li></ul><p>OpenAI还提供了一个交互式网页界面，展示GPT模型中的字节对分词器的工作原理：</p><ul><li><a href="https://platform.openai.com/tokenizer" target="_blank" rel="noreferrer">https://platform.openai.com/tokenizer</a></li></ul><p>对从零开始编写和训练BPE分词器感兴趣的读者，可以参考Andrej Karpathy的GitHub仓库<code>minbpe</code>，提供了一个简洁易懂的实现：</p><ul><li><strong>A minimal implementation of a BPE tokenizer</strong>， <a href="https://github.com/karpathy/minbpe" target="_blank" rel="noreferrer">https://github.com/karpathy/minbpe</a></li></ul><p>有兴趣了解其他流行LLM所使用的分词方案的读者，可以参考以下论文：</p><ul><li><strong>SentencePiece: A Simple and Language Independent Subword Tokenizer and Detokenizer for Neural Text Processing (2018)</strong> ，作者：Kudo和Richardson， <a href="https://aclanthology.org/D18-2012/" target="_blank" rel="noreferrer">https://aclanthology.org/D18-2012/</a></li><li><strong>Fast WordPiece Tokenization (2020)</strong> ，作者：Song等人， <a href="https://arxiv.org/abs/2012.15524" target="_blank" rel="noreferrer">https://arxiv.org/abs/2012.15524</a></li></ul><h2 id="b-3-第3章" tabindex="-1">B.3 第3章 <a class="header-anchor" href="#b-3-第3章" aria-label="Permalink to &quot;B.3 第3章&quot;">​</a></h2><p>对Bahdanau注意力机制及语言翻译中的应用感兴趣的读者，可以在以下论文中找到详细的信息：</p><ul><li><strong>Neural Machine Translation by Jointly Learning to Align and Translate (2014)</strong> ，作者：Bahdanau, Cho和Bengio， <a href="https://arxiv.org/abs/1409.0473" target="_blank" rel="noreferrer">https://arxiv.org/abs/1409.0473</a></li></ul><p>自注意力机制作为缩放点积注意力的概念首次提出于最初的Transformer论文：</p><ul><li><strong>Attention Is All You Need (2017)</strong> ，作者：Vaswani等人， <a href="https://arxiv.org/abs/1706.03762" target="_blank" rel="noreferrer">https://arxiv.org/abs/1706.03762</a></li></ul><p>FlashAttention是一个高效的自注意力机制实现，通过优化内存访问模式加速计算过程。FlashAttention在数学上与标准的自注意力机制相同，但在计算效率上进行了优化：</p><ul><li><strong>FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness (2022)</strong> ，作者：Dao等人， <a href="https://arxiv.org/abs/2205.14135" target="_blank" rel="noreferrer">https://arxiv.org/abs/2205.14135</a></li><li><strong>FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning (2023)</strong> ，作者：Dao， <a href="https://arxiv.org/abs/2307.08691" target="_blank" rel="noreferrer">https://arxiv.org/abs/2307.08691</a></li></ul><p>PyTorch实现了支持FlashAttention的自注意力和因果注意力功能。这些函数处于测试阶段，可能会有所更改：</p><ul><li><strong>scaled_dot_product_attention documentation</strong>， <a href="https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html" target="_blank" rel="noreferrer">https://pytorch.org/docs/stable/generated/torch.nn.functional.scaled_dot_product_attention.html</a></li></ul><p>PyTorch还实现了基于<code>scaled_dot_product</code>函数的高效多头注意力类：</p><ul><li><strong>MultiHeadAttention documentation</strong>， <a href="https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html" target="_blank" rel="noreferrer">https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html</a></li></ul><p>Dropout是一种正则化技术，用于通过在训练过程中随机丢弃神经网络中的单元（及其连接）来防止过拟合：</p><ul><li><strong>Dropout: A Simple Way to Prevent Neural Networks from Overfitting (2014)</strong> ，作者：Srivastava等人， <a href="https://jmlr.org/papers/v15/srivastava14a.html" target="_blank" rel="noreferrer">https://jmlr.org/papers/v15/srivastava14a.html</a></li></ul><p>尽管基于缩放点积的多头注意力是自注意力中最常见的变体，作者发现去除value权重矩阵和投影层仍能实现较好的性能：</p><ul><li><strong>Simplifying Transformer Blocks (2023)</strong> ，作者：He和Hofmann， <a href="https://arxiv.org/abs/2311.01906" target="_blank" rel="noreferrer">https://arxiv.org/abs/2311.01906</a></li></ul><h2 id="b-4-第4章" tabindex="-1">B.4 第4章 <a class="header-anchor" href="#b-4-第4章" aria-label="Permalink to &quot;B.4 第4章&quot;">​</a></h2><p>标题为“Layer Normalization”的论文介绍了一种层归一化技术，它通过对隐藏层中的神经元输入求和并进行归一化来稳定网络的隐藏状态动态，与此前发布的方法相比显著缩短了训练时间：</p><ul><li><strong>Layer Normalization (2016)</strong> ，作者：Ba, Kiros和Hinton， <a href="https://arxiv.org/abs/1607.06450" target="_blank" rel="noreferrer">https://arxiv.org/abs/1607.06450</a></li></ul><p>后归一化（Post-LayerNorm）在原始Transformer模型中应用于自注意力和前馈网络之后。相比之下，前归一化（Pre-LayerNorm）在GPT-2及更先进的LLM中被应用于这些组件之前，这可以带来更稳定的训练动态，并在某些情况下提高性能，详细信息见以下论文：</p><ul><li><strong>On Layer Normalization in the Transformer Architecture (2020)</strong> ，作者：Xiong等人， <a href="https://arxiv.org/abs/2002.04745" target="_blank" rel="noreferrer">https://arxiv.org/abs/2002.04745</a></li><li><strong>ResiDual: Transformer with Dual Residual Connections (2023)</strong> ，作者：Tie等人， <a href="https://arxiv.org/abs/2304.14802" target="_blank" rel="noreferrer">https://arxiv.org/abs/2304.14802</a></li></ul><p>现代LLM中使用的一种流行的LayerNorm变体是RMSNorm，因为其计算效率更高。这种变体仅通过输入的均方根进行归一化，而不在平方之前减去均值，意味着数据在计算缩放之前不会居中。RMSNorm的详细介绍见以下论文：</p><ul><li><strong>Root Mean Square Layer Normalization (2019)</strong> ，作者：Zhang和Sennrich， <a href="https://arxiv.org/abs/1910.07467" target="_blank" rel="noreferrer">https://arxiv.org/abs/1910.07467</a></li></ul><p>GELU（Gaussian Error Linear Unit）激活函数结合了经典ReLU激活函数和正态分布累积分布函数的特性，能够对层输出进行建模，在深度学习模型中允许随机正则化和非线性，详见以下论文：</p><ul><li><strong>Gaussian Error Linear Units (GELUs) (2016)</strong> ，作者：Hendricks和Gimpel， <a href="https://arxiv.org/abs/1606.08415" target="_blank" rel="noreferrer">https://arxiv.org/abs/1606.08415</a></li></ul><p>GPT-2论文介绍了一系列不同规模的基于Transformer的LLM，参数量从124M、355M、774M到1.5B：</p><ul><li><strong>Language Models are Unsupervised Multitask Learners (2019)</strong> ，作者：Radford等人， <a href="./.html">https://d4mucfpksywv.cloudfront.net/better-language￾models/language_models_are_unsupervised_multitask_learners.pdf</a></li></ul><p>OpenAI的GPT-3在架构上与GPT-2基本相同，但最大的版本有1750亿参数，比GPT-2最大的模型大100倍，并且训练数据量更多。感兴趣的读者可以参考OpenAI的GPT-3官方论文以及Lambda Labs的技术概述，其计算出在单个RTX 8000消费级GPU上训练GPT-3需要665年：</p><ul><li><strong>Language Models are Few-Shot Learners (2023)</strong> ，作者：Brown等人， <a href="https://arxiv.org/abs/2005.14165" target="_blank" rel="noreferrer">https://arxiv.org/abs/2005.14165</a></li><li><strong>OpenAI&#39;s GPT-3 Language Model: A Technical Overview</strong>， <a href="https://lambdalabs.com/blog/demystifying-gpt-3" target="_blank" rel="noreferrer">https://lambdalabs.com/blog/demystifying-gpt-3</a></li></ul><p>NanoGPT是一个代码仓库，包含了GPT-2模型的简洁且高效的实现，与本书中实现的模型类似。尽管本书中的代码与nanoGPT有所不同，但该仓库启发了将大型GPT Python主类实现重新组织成更小的子模块：</p><ul><li><strong>NanoGPT, a repository for training medium-sized GPTs</strong>， <a href="https://github.com/karpathy/nanoGPT" target="_blank" rel="noreferrer">https://github.com/karpathy/nanoGPT</a></li></ul><p>一篇有趣的博客文章显示，当上下文大小小于32,000个标记时，LLM的大部分计算开销花费在前馈层，而不是注意力层：</p><ul><li><strong>In the long (context) run</strong>，作者：Harm de Vries， <a href="https://www.harmdevries.com/post/context-length/" target="_blank" rel="noreferrer">https://www.harmdevries.com/post/context-length/</a></li></ul><h2 id="b-5-第5章" tabindex="-1">B.5 第5章 <a class="header-anchor" href="#b-5-第5章" aria-label="Permalink to &quot;B.5 第5章&quot;">​</a></h2><p>作者的一段视频讲座，详细讲解了损失函数及其通过对数变换以便于数学优化的内容：</p><ul><li><strong>L8.2 Logistic Regression Loss Function</strong>， <a href="https://www.youtube.com/watch?v=GxJe0DZvydM" target="_blank" rel="noreferrer">https://www.youtube.com/watch?v=GxJe0DZvydM</a></li></ul><p>以下两篇论文详细介绍了预训练LLM所用的数据集、超参数及架构细节：</p><ul><li><strong>Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling (2023)</strong> ，作者：Biderman等人， <a href="https://arxiv.org/abs/2304.01373" target="_blank" rel="noreferrer">https://arxiv.org/abs/2304.01373</a></li><li><strong>OLMo: Accelerating the Science of Language Models (2024)</strong> ，作者：Groeneveld等人， <a href="https://arxiv.org/abs/2402.00838" target="_blank" rel="noreferrer">https://arxiv.org/abs/2402.00838</a></li></ul><p>本书的配套代码包含了用于准备Project Gutenberg的60,000本公共领域书籍以供LLM训练的说明：</p><ul><li><strong>Pretraining GPT on the Project Gutenberg Dataset</strong>， <a href="https://github.com/rasbt/LLMs-from-scratch/tree/main/ch05/03_bonus_pretraining_on_gutenberg" target="_blank" rel="noreferrer">https://github.com/rasbt/LLMs-from-scratch/tree/main/ch05/03_bonus_pretraining_on_gutenberg</a></li></ul><p>第5章讨论了LLM的预训练，附录D涵盖了更高级的训练功能，如线性预热和余弦退火。以下论文发现类似的技术可成功应用于已预训练的LLM继续预训练，并提供了额外的提示和见解：</p><ul><li><strong>Simple and Scalable Strategies to Continually Pre-train Large Language Models (2024)</strong> ，作者：Ibrahim等人， <a href="https://arxiv.org/abs/2403.08763" target="_blank" rel="noreferrer">https://arxiv.org/abs/2403.08763</a></li></ul><p>BloombergGPT是一个在金融领域专门设计的大型语言模型（LLM），通过在通用和特定领域文本语料库上进行训练：</p><ul><li><strong>BloombergGPT: A Large Language Model for Finance (2023)</strong> ，作者：Wu等人， <a href="https://arxiv.org/abs/2303.17564" target="_blank" rel="noreferrer">https://arxiv.org/abs/2303.17564</a></li></ul><p>GaLore是一个近期的研究项目，旨在使LLM预训练更加高效。代码更改仅需在训练函数中用<code>galore-torch</code> Python包中的GaLoreAdamW优化器替换PyTorch的AdamW优化器：</p><ul><li><strong>GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection (2024)</strong> ，作者：Zhao等人， <a href="https://arxiv.org/abs/2403.03507" target="_blank" rel="noreferrer">https://arxiv.org/abs/2403.03507</a></li><li><strong>GaLore code repository</strong>， <a href="https://github.com/jiaweizzhao/GaLore" target="_blank" rel="noreferrer">https://github.com/jiaweizzhao/GaLore</a></li></ul><p>以下论文和资源提供了公开的大规模预训练数据集，包含数百GB到TB的文本数据：</p><ul><li><strong>Dolma: an Open Corpus of Three Trillion Tokens for LLM Pretraining Research</strong>，作者：Soldaini等人，2024，<a href="https://arxiv.org/abs/2402.00159" target="_blank" rel="noreferrer">https://arxiv.org/abs/2402.00159</a></li><li><strong>The Pile: An 800GB Dataset of Diverse Text for Language Modeling</strong>，作者：Gao等人，2020，<a href="https://arxiv.org/abs/2101.00027" target="_blank" rel="noreferrer">https://arxiv.org/abs/2101.00027</a></li><li><strong>The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data, and Web Data Only</strong>，作者：Penedo等人，2023，<a href="https://arxiv.org/abs/2306.01116" target="_blank" rel="noreferrer">https://arxiv.org/abs/2306.01116</a></li><li><strong>RedPajama by Together AI</strong>， <a href="https://github.com/togethercomputer/RedPajama-Data" target="_blank" rel="noreferrer">https://github.com/togethercomputer/RedPajama-Data</a></li></ul><p>首次提出Top-k采样的论文：</p><ul><li><strong>Hierarchical Neural Story Generation (2018)</strong> ，作者：Fan等人， <a href="https://arxiv.org/abs/1805.04833" target="_blank" rel="noreferrer">https://arxiv.org/abs/1805.04833</a></li></ul><p>束搜索（未在第5章中涉及）是一种替代解码算法，通过在每一步仅保留得分最高的部分序列来生成输出序列，兼顾了效率和质量：</p><ul><li><strong>Diverse Beam Search: Decoding Diverse Solutions from Neural Sequence Models (2016)</strong> ，作者：Vijayakumar等人， <a href="https://arxiv.org/abs/1610.02424" target="_blank" rel="noreferrer">https://arxiv.org/abs/1610.02424</a></li></ul></div></div></main><footer class="VPDocFooter" data-v-56ee120b data-v-d7c1e045><!--[--><!--]--><!----><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-d7c1e045><span class="visually-hidden" id="doc-footer-aria-label" data-v-d7c1e045>Pager</span><div class="pager" data-v-d7c1e045><a class="VPLink link pager-link prev" href="/dllm/bllm/appendix_a_introduction_to_pytorch.html" data-v-d7c1e045><!--[--><span class="desc" data-v-d7c1e045>Previous page</span><span class="title" data-v-d7c1e045>附录 A. PyTorch简介</span><!--]--></a></div><div class="pager" data-v-d7c1e045><a class="VPLink link pager-link next" href="/dllm/bllm/appendix_c_exercise_solutions.html" data-v-d7c1e045><!--[--><span class="desc" data-v-d7c1e045>Next page</span><span class="title" data-v-d7c1e045>附录 C. 习题解答</span><!--]--></a></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><!----><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"api-examples.md\":\"DldUZpiA\",\"bllm_1_understanding_large_language_models.md\":\"fSRy1vIZ\",\"bllm_2_working_with_text_data.md\":\"Bx3OGh_M\",\"bllm_3_coding_attention_mechanisms.md\":\"BEkd3JCm\",\"bllm_4_implementing_a_gpt_model_from_scratch_to_generate_text.md\":\"7-_e9ERl\",\"bllm_5_pretraining_on_unlabeled_data.md\":\"BdP6TADs\",\"bllm_appendix_a_introduction_to_pytorch.md\":\"z3XsYjzi\",\"bllm_appendix_b_references_and_further_reading.md\":\"D476jJpX\",\"bllm_appendix_c_exercise_solutions.md\":\"jZkh75TN\",\"bllm_appendix_d_adding_bells_and_whistles_to_the_training_loop.md\":\"CdY_lDP7\",\"bllm_index.md\":\"DuAB0eyc\",\"index.md\":\"Du7zn73F\",\"markdown-examples.md\":\"NPSNuN8g\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"大模型知识库\",\"description\":\"A VitePress Site\",\"base\":\"/dllm/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"nav\":[{\"text\":\"首页\",\"link\":\"/\"},{\"text\":\"示例\",\"link\":\"/markdown-examples\"}],\"search\":{\"provider\":\"local\"},\"sidebar\":[{\"text\":\"书籍\",\"items\":[{\"text\":\"Markdown Examples\",\"link\":\"/markdown-examples\"},{\"text\":\"Runtime API Examples\",\"link\":\"/api-examples\"},{\"text\":\"欢迎\",\"link\":\"/bllm/\"},{\"text\":\"理解大型语言模型\",\"link\":\"/bllm/1_understanding_large_language_models\"},{\"text\":\"处理文本数据\",\"link\":\"/bllm/2_working_with_text_data\"},{\"text\":\"编写注意力机制\",\"link\":\"/bllm/3_coding_attention_mechanisms\"},{\"text\":\"从零实现GPT模型生成文本\",\"link\":\"/bllm/4_implementing_a_gpt_model_from_scratch_to_generate_text\"},{\"text\":\" 在无标签数据上预训练\",\"link\":\"/bllm/5_pretraining_on_unlabeled_data\"},{\"text\":\"附录 A. PyTorch简介\",\"link\":\"/bllm/appendix_a_introduction_to_pytorch\"},{\"text\":\"附录 B. 参考文献与进一步阅读\",\"link\":\"/bllm/appendix_b_references_and_further_reading\"},{\"text\":\"附录 C. 习题解答\",\"link\":\"/bllm/appendix_c_exercise_solutions\"},{\"text\":\"附录 D. 给训练循环添加附加功能\",\"link\":\"/bllm/appendix_d_adding_bells_and_whistles_to_the_training_loop\"}]},{\"text\":\"课程\",\"items\":[{\"text\":\"课程1\",\"link\":\"/course/1\"},{\"text\":\"课程2\",\"link\":\"/course/2\"}]}],\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/vuejs/vitepress\"}]},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":false}");</script>
    
  </body>
</html>