<!DOCTYPE html>
<html lang="en-US" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>编码注意力机制 | 大模型知识库</title>
    <meta name="description" content="A VitePress Site">
    <meta name="generator" content="VitePress v1.5.0">
    <link rel="preload stylesheet" href="/repo/assets/style.CRVa3LcY.css" as="style">
    <link rel="preload stylesheet" href="/repo/vp-icons.css" as="style">
    
    <script type="module" src="/repo/assets/app.PxpG6aUR.js"></script>
    <link rel="preload" href="/repo/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
    <link rel="modulepreload" href="/repo/assets/chunks/theme.CsUvNU42.js">
    <link rel="modulepreload" href="/repo/assets/chunks/framework.CpXGQlwB.js">
    <link rel="modulepreload" href="/repo/assets/bllm_3_coding_attention_mechanisms.md.3P--8seq.lean.js">
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
  </head>
  <body>
    <div id="app"><div class="Layout" data-v-04082acb><!--[--><!--]--><!--[--><span tabindex="-1" data-v-2abd97fb></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-2abd97fb> Skip to content </a><!--]--><!----><header class="VPNav" data-v-04082acb data-v-4d8a2a03><div class="VPNavBar" data-v-4d8a2a03 data-v-d662bdbc><div class="wrapper" data-v-d662bdbc><div class="container" data-v-d662bdbc><div class="title" data-v-d662bdbc><div class="VPNavBarTitle has-sidebar" data-v-d662bdbc data-v-851315c7><a class="title" href="/repo/" data-v-851315c7><!--[--><!--]--><!----><span data-v-851315c7>大模型知识库</span><!--[--><!--]--></a></div></div><div class="content" data-v-d662bdbc><div class="content-body" data-v-d662bdbc><!--[--><!--]--><div class="VPNavBarSearch search" data-v-d662bdbc><!--[--><!----><div id="local-search"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><span class="vp-icon DocSearch-Search-Icon"></span><span class="DocSearch-Button-Placeholder">Search</span></span><span class="DocSearch-Button-Keys"><kbd class="DocSearch-Button-Key"></kbd><kbd class="DocSearch-Button-Key">K</kbd></span></button></div><!--]--></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-d662bdbc data-v-d3871c33><span id="main-nav-aria-label" class="visually-hidden" data-v-d3871c33> Main Navigation </span><!--[--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/repo/" tabindex="0" data-v-d3871c33 data-v-abb8c4e8><!--[--><span data-v-abb8c4e8>首页</span><!--]--></a><!--]--><!--[--><a class="VPLink link VPNavBarMenuLink" href="/repo/markdown-examples.html" tabindex="0" data-v-d3871c33 data-v-abb8c4e8><!--[--><span data-v-abb8c4e8>示例</span><!--]--></a><!--]--><!--]--></nav><!----><div class="VPNavBarAppearance appearance" data-v-d662bdbc data-v-cc58c11e><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-cc58c11e data-v-e81c969b data-v-5d4aee43><span class="check" data-v-5d4aee43><span class="icon" data-v-5d4aee43><!--[--><span class="vpi-sun sun" data-v-e81c969b></span><span class="vpi-moon moon" data-v-e81c969b></span><!--]--></span></span></button></div><div class="VPSocialLinks VPNavBarSocialLinks social-links" data-v-d662bdbc data-v-d3159530 data-v-c5f7a4ac><!--[--><a class="VPSocialLink no-icon" href="https://github.com/vuejs/vitepress" aria-label="github" target="_blank" rel="noopener" data-v-c5f7a4ac data-v-acb7c684><span class="vpi-social-github"></span></a><!--]--></div><div class="VPFlyout VPNavBarExtra extra" data-v-d662bdbc data-v-42c56eeb data-v-6c050606><button type="button" class="button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation" data-v-6c050606><span class="vpi-more-horizontal icon" data-v-6c050606></span></button><div class="menu" data-v-6c050606><div class="VPMenu" data-v-6c050606 data-v-a0042935><!----><!--[--><!--[--><!----><div class="group" data-v-42c56eeb><div class="item appearance" data-v-42c56eeb><p class="label" data-v-42c56eeb>Appearance</p><div class="appearance-action" data-v-42c56eeb><button class="VPSwitch VPSwitchAppearance" type="button" role="switch" title aria-checked="false" data-v-42c56eeb data-v-e81c969b data-v-5d4aee43><span class="check" data-v-5d4aee43><span class="icon" data-v-5d4aee43><!--[--><span class="vpi-sun sun" data-v-e81c969b></span><span class="vpi-moon moon" data-v-e81c969b></span><!--]--></span></span></button></div></div></div><div class="group" data-v-42c56eeb><div class="item social-links" data-v-42c56eeb><div class="VPSocialLinks social-links-list" data-v-42c56eeb data-v-c5f7a4ac><!--[--><a class="VPSocialLink no-icon" href="https://github.com/vuejs/vitepress" aria-label="github" target="_blank" rel="noopener" data-v-c5f7a4ac data-v-acb7c684><span class="vpi-social-github"></span></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><!--[--><!--]--><button type="button" class="VPNavBarHamburger hamburger" aria-label="mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-d662bdbc data-v-716a6ba8><span class="container" data-v-716a6ba8><span class="top" data-v-716a6ba8></span><span class="middle" data-v-716a6ba8></span><span class="bottom" data-v-716a6ba8></span></span></button></div></div></div></div><div class="divider" data-v-d662bdbc><div class="divider-line" data-v-d662bdbc></div></div></div><!----></header><div class="VPLocalNav has-sidebar empty" data-v-04082acb data-v-3a93fdb1><div class="container" data-v-3a93fdb1><button class="menu" aria-expanded="false" aria-controls="VPSidebarNav" data-v-3a93fdb1><span class="vpi-align-left menu-icon" data-v-3a93fdb1></span><span class="menu-text" data-v-3a93fdb1>Menu</span></button><div class="VPLocalNavOutlineDropdown" style="--vp-vh:0px;" data-v-3a93fdb1 data-v-9fecb94d><button data-v-9fecb94d>Return to top</button><!----></div></div></div><aside class="VPSidebar" data-v-04082acb data-v-e9366181><div class="curtain" data-v-e9366181></div><nav class="nav" id="VPSidebarNav" aria-labelledby="sidebar-aria-label" tabindex="-1" data-v-e9366181><span class="visually-hidden" id="sidebar-aria-label" data-v-e9366181> Sidebar Navigation </span><!--[--><!--]--><!--[--><div class="no-transition group" data-v-999dfcb0><section class="VPSidebarItem level-0 has-active" data-v-999dfcb0 data-v-f88eca86><div class="item" role="button" tabindex="0" data-v-f88eca86><div class="indicator" data-v-f88eca86></div><h2 class="text" data-v-f88eca86>书籍</h2><!----></div><div class="items" data-v-f88eca86><!--[--><div class="VPSidebarItem level-1 is-link" data-v-f88eca86 data-v-f88eca86><div class="item" data-v-f88eca86><div class="indicator" data-v-f88eca86></div><a class="VPLink link link" href="/repo/markdown-examples.html" data-v-f88eca86><!--[--><p class="text" data-v-f88eca86>Markdown Examples</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f88eca86 data-v-f88eca86><div class="item" data-v-f88eca86><div class="indicator" data-v-f88eca86></div><a class="VPLink link link" href="/repo/api-examples.html" data-v-f88eca86><!--[--><p class="text" data-v-f88eca86>Runtime API Examples</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f88eca86 data-v-f88eca86><div class="item" data-v-f88eca86><div class="indicator" data-v-f88eca86></div><a class="VPLink link link" href="/repo/bllm/" data-v-f88eca86><!--[--><p class="text" data-v-f88eca86>欢迎</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f88eca86 data-v-f88eca86><div class="item" data-v-f88eca86><div class="indicator" data-v-f88eca86></div><a class="VPLink link link" href="/repo/bllm/1_understanding_large_language_models.html" data-v-f88eca86><!--[--><p class="text" data-v-f88eca86>理解大型语言模型</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f88eca86 data-v-f88eca86><div class="item" data-v-f88eca86><div class="indicator" data-v-f88eca86></div><a class="VPLink link link" href="/repo/bllm/2_working_with_text_data.html" data-v-f88eca86><!--[--><p class="text" data-v-f88eca86>处理文本数据</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f88eca86 data-v-f88eca86><div class="item" data-v-f88eca86><div class="indicator" data-v-f88eca86></div><a class="VPLink link link" href="/repo/bllm/3_coding_attention_mechanisms.html" data-v-f88eca86><!--[--><p class="text" data-v-f88eca86>编写注意力机制</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f88eca86 data-v-f88eca86><div class="item" data-v-f88eca86><div class="indicator" data-v-f88eca86></div><a class="VPLink link link" href="/repo/bllm/4_implementing_a_gpt_model_from_scratch_to_generate_text.html" data-v-f88eca86><!--[--><p class="text" data-v-f88eca86>从零实现GPT模型生成文本</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f88eca86 data-v-f88eca86><div class="item" data-v-f88eca86><div class="indicator" data-v-f88eca86></div><a class="VPLink link link" href="/repo/bllm/5_pretraining_on_unlabeled_data.html" data-v-f88eca86><!--[--><p class="text" data-v-f88eca86> 在无标签数据上预训练</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f88eca86 data-v-f88eca86><div class="item" data-v-f88eca86><div class="indicator" data-v-f88eca86></div><a class="VPLink link link" href="/repo/bllm/appendix_a_introduction_to_pytorch.html" data-v-f88eca86><!--[--><p class="text" data-v-f88eca86>附录 A. PyTorch简介</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f88eca86 data-v-f88eca86><div class="item" data-v-f88eca86><div class="indicator" data-v-f88eca86></div><a class="VPLink link link" href="/repo/bllm/appendix_b_references_and_further_reading.html" data-v-f88eca86><!--[--><p class="text" data-v-f88eca86>附录 B. 参考文献与进一步阅读</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f88eca86 data-v-f88eca86><div class="item" data-v-f88eca86><div class="indicator" data-v-f88eca86></div><a class="VPLink link link" href="/repo/bllm/appendix_c_exercise_solutions.html" data-v-f88eca86><!--[--><p class="text" data-v-f88eca86>附录 C. 习题解答</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f88eca86 data-v-f88eca86><div class="item" data-v-f88eca86><div class="indicator" data-v-f88eca86></div><a class="VPLink link link" href="/repo/bllm/appendix_d_adding_bells_and_whistles_to_the_training_loop.html" data-v-f88eca86><!--[--><p class="text" data-v-f88eca86>附录 D. 给训练循环添加附加功能</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><div class="no-transition group" data-v-999dfcb0><section class="VPSidebarItem level-0" data-v-999dfcb0 data-v-f88eca86><div class="item" role="button" tabindex="0" data-v-f88eca86><div class="indicator" data-v-f88eca86></div><h2 class="text" data-v-f88eca86>课程</h2><!----></div><div class="items" data-v-f88eca86><!--[--><div class="VPSidebarItem level-1 is-link" data-v-f88eca86 data-v-f88eca86><div class="item" data-v-f88eca86><div class="indicator" data-v-f88eca86></div><a class="VPLink link link" href="/repo/course/1.html" data-v-f88eca86><!--[--><p class="text" data-v-f88eca86>课程1</p><!--]--></a><!----></div><!----></div><div class="VPSidebarItem level-1 is-link" data-v-f88eca86 data-v-f88eca86><div class="item" data-v-f88eca86><div class="indicator" data-v-f88eca86></div><a class="VPLink link link" href="/repo/course/2.html" data-v-f88eca86><!--[--><p class="text" data-v-f88eca86>课程2</p><!--]--></a><!----></div><!----></div><!--]--></div></section></div><!--]--><!--[--><!--]--></nav></aside><div class="VPContent has-sidebar" id="VPContent" data-v-04082acb data-v-472f5592><div class="VPDoc has-sidebar has-aside" data-v-472f5592 data-v-56ee120b><!--[--><!--]--><div class="container" data-v-56ee120b><div class="aside" data-v-56ee120b><div class="aside-curtain" data-v-56ee120b></div><div class="aside-container" data-v-56ee120b><div class="aside-content" data-v-56ee120b><div class="VPDocAside" data-v-56ee120b data-v-3b40b7c4><!--[--><!--]--><!--[--><!--]--><nav aria-labelledby="doc-outline-aria-label" class="VPDocAsideOutline" data-v-3b40b7c4 data-v-3861d3c5><div class="content" data-v-3861d3c5><div class="outline-marker" data-v-3861d3c5></div><div aria-level="2" class="outline-title" id="doc-outline-aria-label" role="heading" data-v-3861d3c5>On this page</div><ul class="VPDocOutlineItem root" data-v-3861d3c5 data-v-6358f6a3><!--[--><!--]--></ul></div></nav><!--[--><!--]--><div class="spacer" data-v-3b40b7c4></div><!--[--><!--]--><!----><!--[--><!--]--><!--[--><!--]--></div></div></div></div><div class="content" data-v-56ee120b><div class="content-container" data-v-56ee120b><!--[--><!--]--><main class="main" data-v-56ee120b><div style="position:relative;" class="vp-doc _repo_bllm_3_coding_attention_mechanisms" data-v-56ee120b><div><h1 id="编码注意力机制" tabindex="-1">编码注意力机制 <a class="header-anchor" href="#编码注意力机制" aria-label="Permalink to &quot;编码注意力机制&quot;">​</a></h1><h2 id="本章内容" tabindex="-1">本章内容 <a class="header-anchor" href="#本章内容" aria-label="Permalink to &quot;本章内容&quot;">​</a></h2><ul><li>探讨在神经网络中使用注意力机制的原因</li><li>介绍一个基本的自注意力框架，并进一步扩展为增强型自注意力机制</li><li>实现一个因果注意力模块，使得大型语言模型（LLMs）能够逐词生成</li><li>通过使用 dropout 随机屏蔽部分注意力权重以减少过拟合</li><li>将多个因果注意力模块堆叠成多头注意力模块</li></ul><p>在上一章中，你学习了如何为训练 LLM 准备输入文本。这涉及将文本分割为单词或子词，并将其编码为向量表示（即嵌入）供 LLM 使用。本章将深入探讨 LLM 架构中不可或缺的部分：注意力机制，如图 3.1 所示。</p><p>图 3.1 展示了编码一个 LLM 的三个主要阶段的思维模型，即对通用文本数据集的预训练，以及对标注数据集的微调。本章聚焦于注意力机制，它是 LLM 架构中的核心组成部分。 <img src="/repo/assets/image-28.nDkwvKl8.png" alt="alt text"> 注意力机制是一个复杂的话题，因此我们将用整整一章来讨论。在本章中，我们将主要孤立地研究这些注意力机制，聚焦其在机制层面的运作。在下一章，我们将编写围绕自注意力机制的剩余 LLM 代码，以实现文本生成模型。</p><p>在本章中，我们将实现四种不同的注意力机制变体，如图 3.2 所示。</p><p>图 3.2 展示了我们在本章中将要编码的不同注意力机制，从一个简化的自注意力版本开始，再逐步添加可训练权重。因果注意力机制为自注意力添加了一个掩码，使得 LLM 可以逐词生成。最后，多头注意力将注意力机制分成多个头，使模型能够并行捕捉输入数据的不同方面。 <img src="/repo/assets/image-29.AHKHtLze.png" alt="alt text"> 这些不同的注意力变体如图 3.2 所示，它们逐步构建，最终目标是实现一个紧凑且高效的多头注意力模块，我们可以在下一章将其插入 LLM 架构中。</p><h2 id="_3-1-处理长序列的问题" tabindex="-1">3.1 处理长序列的问题 <a class="header-anchor" href="#_3-1-处理长序列的问题" aria-label="Permalink to &quot;3.1 处理长序列的问题&quot;">​</a></h2><p>在深入自注意力机制之前，让我们先看看在 LLM 出现之前缺少注意力机制的架构存在的问题。假设我们要开发一个将文本从一种语言翻译成另一种语言的模型。如图 3.3 所示，我们不能逐词翻译，因为源语言和目标语言的语法结构不同。</p><p>图 3.3 展示了在从一种语言（例如德语）翻译成另一种语言（如英语）时，不可能简单地逐词翻译，而是需要考虑上下文理解和语法匹配。 <img src="/repo/assets/image-30.BqWolf19.png" alt="alt text"> 为了解决无法逐词翻译的问题，通常使用一个包含编码器和解码器两个子模块的深度神经网络。编码器的任务是读取并处理整个文本，而解码器则生成翻译后的文本。</p><p>我们在第 1 章（1.4 节 使用 LLM 进行不同任务）简要介绍过编码器-解码器网络。在 transformer 出现之前，循环神经网络（RNNs）是最受欢迎的编码器-解码器架构之一，用于语言翻译。</p><p>RNN 是一种神经网络，其前一步的输出作为当前步的输入，适合处理像文本这样的序列数据。即使你不熟悉 RNN，也不必担心，因为本讨论的重点在于编码器-解码器的总体概念。</p><p>在编码器-解码器 RNN 中，输入文本依次输入编码器，编码器在每一步更新其隐藏状态（即隐藏层的内部值），试图在最终的隐藏状态中捕捉整个句子的含义，如图 3.4 所示。解码器然后使用这个隐藏状态开始逐词生成翻译句子，并在每一步更新其隐藏状态，用于下一个词的预测。</p><p>图 3.4 在 transformer 模型出现之前，编码器-解码器 RNN 是机器翻译的流行选择。编码器将源语言的词序列作为输入，其隐藏状态（即中间层）编码了整个输入序列的压缩表示，然后解码器使用当前隐藏状态逐词生成翻译。 <img src="/repo/assets/image-31.40UHU2vF.png" alt="alt text"> 尽管我们不需要深入了解编码器-解码器 RNN 的内部运作，但其关键在于编码器部分将整个输入文本转换为隐藏状态（记忆单元），解码器使用该隐藏状态生成输出。可以将该隐藏状态视为一个嵌入向量，这是我们在第 2 章讨论过的概念。</p><p>编码器-解码器 RNN 的主要问题在于，在解码阶段 RNN 无法直接访问编码器的早期隐藏状态。它只能依赖当前的隐藏状态来包含所有相关信息，这可能导致上下文丢失，特别是当句子复杂且依赖关系跨度较长时。</p><p>对于不熟悉 RNN 的读者，无需深入了解这种架构，因为我们不会在本书中使用它。这里的关键是，编码器-解码器 RNN 的局限性促使了注意力机制的设计。</p><h2 id="_3-2-使用注意力机制捕获数据依赖关系" tabindex="-1">3.2 使用注意力机制捕获数据依赖关系 <a class="header-anchor" href="#_3-2-使用注意力机制捕获数据依赖关系" aria-label="Permalink to &quot;3.2 使用注意力机制捕获数据依赖关系&quot;">​</a></h2><p>在 transformer LLM 出现之前，RNN 常被用于语言建模任务，例如语言翻译。正如前文所述，RNN 对于翻译短句效果良好，但在处理长文本时表现较差，因为它无法直接访问输入中的前面部分。</p><p>这种方法的主要缺陷在于，RNN 必须将整个编码后的输入信息保存在一个单一的隐藏状态中，然后再传递给解码器，如上一节图 3.4 所示。</p><p>因此，研究人员在 2014 年开发了所谓的 Bahdanau 注意力机制（以论文的第一作者命名），该机制对 RNN 编码器-解码器进行了修改，使得解码器可以在每个解码步骤中选择性地访问输入序列的不同部分，如图 3.5 所示。</p><p>图 3.5 展示了通过使用注意力机制，网络中的文本生成解码器部分可以选择性地访问所有输入标记。这意味着，对于生成特定的输出标记，某些输入标记比其他标记更为重要。重要性由所谓的注意力权重决定，我们将在后面计算这些权重。需要注意的是，这张图展示了注意力机制的概念，并未精确展示 Bahdanau 机制的实现方式，后者是一种超出本书范围的 RNN 方法。 <img src="/repo/assets/image-32.Cg9kzsOg.png" alt="alt text"> 有趣的是，仅仅三年后，研究人员发现构建自然语言处理的深度神经网络并不需要 RNN 架构，并提出了最初的 transformer 架构（在第 1 章中讨论），该架构的自注意力机制受到了 Bahdanau 注意力机制的启发。</p><p>自注意力是一种机制，允许输入序列中的每个位置在计算序列表示时关注该序列中的所有位置。自注意力是基于 transformer 架构的现代 LLM（例如 GPT 系列）的关键组成部分。</p><p>本章将着重于编写并理解 GPT 类模型中使用的自注意力机制，如图 3.6 所示。在下一章中，我们将编写 LLM 的其余部分代码。</p><p>图 3.6 展示了自注意力在 transformer 中的应用，它通过允许序列中每个位置与序列中所有其他位置交互并衡量其重要性，来计算更高效的输入表示。在本章中，我们将从零开始编写这个自注意力机制代码，然后在下一章中编写 GPT 类 LLM 的剩余部分。 <img src="/repo/assets/image-33.CjqU8wLJ.png" alt="alt text"></p><h2 id="_3-3-使用自注意力关注输入的不同部分" tabindex="-1">3.3 使用自注意力关注输入的不同部分 <a class="header-anchor" href="#_3-3-使用自注意力关注输入的不同部分" aria-label="Permalink to &quot;3.3 使用自注意力关注输入的不同部分&quot;">​</a></h2><p>接下来，我们将深入探讨自注意力机制的内部工作原理，并学习如何从头开始编写它。自注意力是基于 transformer 架构的每个 LLM 的基石。值得注意的是，这一主题可能需要相当多的专注，但一旦掌握其基本原理，你将征服本书中最具挑战性的一部分，同时也是实现 LLM 的关键环节之一。</p><h4 id="自注意力中的-自我" tabindex="-1">自注意力中的“自我” <a class="header-anchor" href="#自注意力中的-自我" aria-label="Permalink to &quot;自注意力中的“自我”&quot;">​</a></h4><p>在自注意力中，“自我”指的是该机制通过关联单个输入序列中的不同位置来计算注意力权重的能力。它评估并学习输入内部各个部分（如句子中的单词或图像中的像素）之间的关系和依赖性。这与传统的注意力机制不同，后者关注的是两个不同序列的元素之间的关系，比如在序列到序列模型中，注意力可能集中在输入序列和输出序列之间的关系上（如图 3.5 所示的例子）。</p><p>由于自注意力可能显得较为复杂，特别是如果你是第一次接触它，我们将首先在下一小节介绍一个简化版的自注意力。然后，在 3.4 节中，我们将实现带有可训练权重的自注意力机制，即 LLM 中使用的自注意力。</p><h3 id="_3-3-1-一个简单的无可训练权重的自注意力机制" tabindex="-1">3.3.1 一个简单的无可训练权重的自注意力机制 <a class="header-anchor" href="#_3-3-1-一个简单的无可训练权重的自注意力机制" aria-label="Permalink to &quot;3.3.1 一个简单的无可训练权重的自注意力机制&quot;">​</a></h3><p>本节中，我们将实现一个简化的自注意力机制版本，不包含任何可训练权重，内容如图 3.7 所示。本节的目标是展示自注意力中的一些关键概念，以便为后续 3.4 节的带可训练权重的自注意力机制打下基础。</p><p><strong>图 3.7</strong> 中显示了自注意力的目标，即为每个输入元素计算一个上下文向量，结合所有其他输入元素的信息。图中展示了如何计算上下文向量 z(2)z^{(2)}z(2)，即输入序列中第 2 个元素的上下文向量。对于每个输入元素，计算 z(2)z^{(2)}z(2) 的重要性或贡献由注意力权重 α21\alpha_{21}α21​ 到 α2T\alpha_{2T}α2T​ 决定。具体计算将在稍后讨论。</p><p>图 3.7 展示了一个包含 T 个元素的输入序列 xxx，元素分别表示为 x(1)x^{(1)}x(1) 到 x(T)x^{(T)}x(T)。该序列通常是文本，例如一个已经转化为词嵌入（token embeddings）的句子，如第 2 章所述。 <img src="/repo/assets/image-34.BXsxEI0B.png" alt="alt text"> 例如，输入文本为 “Your journey starts with one step.”，其中每个元素（如 x(1)x^{(1)}x(1)）对应一个 d 维的嵌入向量，表示一个特定的词（如 &quot;Your&quot;）。在图 3.7 中，这些输入向量被表示为 3 维嵌入。</p><p>在自注意力中，我们的目标是计算每个输入元素 x(i)x^{(i)}x(i) 的上下文向量 z(i)z^{(i)}z(i)。上下文向量可以被视为一个富含信息的嵌入向量。</p><p>为了说明这一概念，我们专注于第二个输入元素 x(2)x^{(2)}x(2) 的嵌入向量（对应词 &quot;journey&quot;）以及相应的上下文向量 z(2)z^{(2)}z(2)，如图 3.7 底部所示。该增强的上下文向量 z(2)z^{(2)}z(2) 是一个嵌入，包含了关于 x(2)x^{(2)}x(2) 及其他所有输入元素 x(1)x^{(1)}x(1) 到 x(T)x^{(T)}x(T) 的信息。</p><p>上下文向量在自注意力中扮演着重要角色。它们的作用是通过包含序列中其他元素的信息，创建每个元素的富信息表示（如句子中的词），正如图 3.7 所示。这对于需要理解句子中词语之间关系的 LLM 至关重要。稍后我们将添加可训练权重，帮助 LLM 学习构建这些上下文向量，使其对 LLM 生成下一个词更为重要。</p><p>本节中，我们实现一个简化的自注意力机制，逐步计算这些权重和生成的上下文向量。</p><p>假设以下输入句子已嵌入为 3 维向量（为了便于展示，选择小的嵌入维度）：</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">inputs </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.tensor(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    [[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.43</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.15</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.89</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">], </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Your (x^1)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">     [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.55</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.87</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.66</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">], </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># journey (x^2)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">     [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.57</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.85</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.64</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">], </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># starts (x^3)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">     [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.22</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.58</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.33</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">], </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># with (x^4)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">     [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.77</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.25</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.10</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">], </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># one (x^5)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">     [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.05</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.80</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.55</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]] </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># step (x^6)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br></div></div><p><strong>步骤 1：计算中间值 ω\omegaω</strong></p><p>如图 3.8 所示，自注意力实现的第一步是计算注意力得分（attention scores），通过查询 x(2)x^{(2)}x(2) 与其他输入元素的点积来实现： <img src="/repo/assets/image-35.DHgP-CZ0.png" alt="alt text"></p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">query </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> inputs[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># A</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">attn_scores_2 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.empty(inputs.shape[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">])</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> i, x_i </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> enumerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(inputs):</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    attn_scores_2[i] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.dot(x_i, query)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(attn_scores_2)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>输出的注意力得分为：</p><div class="language-scss vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">scss</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([0</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">.9544</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, 1</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">.4950</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, 1</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">.4754</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, 0</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">.8434</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, 0</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">.7070</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, 1.0865])</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p><strong>理解点积</strong></p><p>点积是一种将两个向量按元素相乘并求和的简便方式。通过以下代码示例，可以展示点积的计算过程：</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">res </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> idx, element </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> enumerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(inputs[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]):</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    res </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> inputs[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">][idx] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> query[idx]</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(res)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(torch.dot(inputs[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">], query))</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>结果确认了点积的计算方式：</p><div class="language-scss vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">scss</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor(0</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">.9544</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor(0</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">.9544</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>在自注意力机制中，点积衡量了两个向量的相似性，即确定序列中元素之间相互关注的程度。</p><p><strong>步骤 2：归一化注意力得分</strong></p><p>下一步（如图 3.9 所示）是对注意力得分进行归一化，以获得注意力权重 α\alphaα，使其和为 1。代码如下： <img src="/repo/assets/image-36.-9ourlwa.png" alt="alt text"></p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">attn_weights_2_tmp </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> attn_scores_2 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> attn_scores_2.sum()</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Attention weights:&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, attn_weights_2_tmp)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Sum:&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, attn_weights_2_tmp.sum())</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>输出显示注意力权重的和为 1：</p><div class="language-css vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">css</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Attention weights: tensor([0.1455, 0.2278, 0.2249, 0.1285, 0.1077, 0.1656])</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">Sum: tensor(1</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">.0000</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>更常见的是使用 softmax 函数来归一化：</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> softmax_naive</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(x):</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.exp(x) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.exp(x).sum(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dim</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">attn_weights_2_naive </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> softmax_naive(attn_scores_2)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Attention weights:&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, attn_weights_2_naive)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Sum:&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, attn_weights_2_naive.sum())</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>PyTorch 自带的 softmax 实现：</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">attn_weights_2 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.softmax(attn_scores_2, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dim</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Attention weights:&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, attn_weights_2)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Sum:&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, attn_weights_2.sum())</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p><strong>步骤 3：计算上下文向量 z(2)z^{(2)}z(2)</strong></p><p>最后一步（如图 3.10 所示）是计算上下文向量 z(2)z^{(2)}z(2)，方法是将输入向量 x(i)x^{(i)}x(i) 与对应的注意力权重相乘并求和： <img src="/repo/assets/image-37.qcEQGdSZ.png" alt="alt text"></p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">query </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> inputs[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 第 2 个输入作为查询</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">context_vec_2 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.zeros(query.shape)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> i, x_i </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> enumerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(inputs):</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    context_vec_2 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> attn_weights_2[i] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> x_i</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(context_vec_2)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>结果为：</p><div class="language-scss vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">scss</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([0</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">.4419</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, 0</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">.6515</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, 0.5683])</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>接下来，我们将泛化此过程，以同时计算所有上下文向量。</p><h3 id="_3-3-2-为所有输入标记计算注意力权重" tabindex="-1">3.3.2 为所有输入标记计算注意力权重 <a class="header-anchor" href="#_3-3-2-为所有输入标记计算注意力权重" aria-label="Permalink to &quot;3.3.2 为所有输入标记计算注意力权重&quot;">​</a></h3><p>在上一节中，我们为第 2 个输入元素计算了注意力权重和上下文向量，如图 3.11 高亮显示的行所示。本节将扩展该计算，计算所有输入元素的注意力权重和上下文向量。</p><p>图 3.11 中展示了第 2 个输入元素作为查询的注意力权重。此节将通用化此计算，以获得所有其他的注意力权重。与之前一样，我们将按照图 3.12 所示的三个步骤操作，只是会对代码做一些修改，以计算所有上下文向量，而不仅仅是第 2 个上下文向量 z(2)z^{(2)}z(2)。 <img src="/repo/assets/image-38.CP3-tSaO.png" alt="alt text"></p><h4 id="步骤-1-计算所有输入对之间的点积" tabindex="-1">步骤 1：计算所有输入对之间的点积 <a class="header-anchor" href="#步骤-1-计算所有输入对之间的点积" aria-label="Permalink to &quot;步骤 1：计算所有输入对之间的点积&quot;">​</a></h4><p>我们添加一个额外的 <code>for</code> 循环来计算所有输入对之间的点积：</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">attn_scores </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.empty(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">6</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">6</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> i, x_i </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> enumerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(inputs):</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> j, x_j </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> enumerate</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(inputs):</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        attn_scores[i, j] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.dot(x_i, x_j)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(attn_scores)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>生成的注意力得分如下：</p><div class="language-css vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">css</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p><img src="/repo/assets/image-39.XevGaHFE.png" alt="alt text"> 每个元素表示输入对之间的注意力得分，如图 3.11 所示。计算中使用了 <code>for</code> 循环，然而，<code>for</code> 循环的执行速度通常较慢，可以使用矩阵乘法实现相同效果：</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">attn_scores </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> inputs </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">@</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> inputs.T</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(attn_scores)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>结果与之前一致：</p><div class="language-css vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">css</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[0.9995, 0.9544, 0.9422, 0.4753, 0.4576, 0.6310],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.9544, 1.4950, 1.4754, 0.8434, 0.7070, 1.0865],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.9422, 1.4754, 1.4570, 0.8296, 0.7154, 1.0605],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.4753, 0.8434, 0.8296, 0.4937, 0.3474, 0.6565],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.4576, 0.7070, 0.7154, 0.3474, 0.6654, 0.2935],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.6310, 1.0865, 1.0605, 0.6565, 0.2935, 0.9450]])</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><h4 id="步骤-2-归一化每行使得总和为-1" tabindex="-1">步骤 2：归一化每行使得总和为 1 <a class="header-anchor" href="#步骤-2-归一化每行使得总和为-1" aria-label="Permalink to &quot;步骤 2：归一化每行使得总和为 1&quot;">​</a></h4><p>使用 softmax 归一化每行，使得各行的值总和为 1：</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">attn_weights </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.softmax(attn_scores, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dim</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(attn_weights)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>归一化后的注意力权重如下：</p><div class="language-css vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">css</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[0.2098, 0.2006, 0.1981, 0.1242, 0.1220, 0.1452],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.1385, 0.2379, 0.2333, 0.1240, 0.1082, 0.1581],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.1390, 0.2369, 0.2326, 0.1242, 0.1108, 0.1565],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.1435, 0.2074, 0.2046, 0.1462, 0.1263, 0.1720],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.1526, 0.1958, 0.1975, 0.1367, 0.1879, 0.1295],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.1385, 0.2184, 0.2128, 0.1420, 0.0988, 0.1896]])</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>验证所有行的总和确实为 1：</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;All row sums:&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, attn_weights.sum(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dim</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>输出结果：</p><div class="language-css vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">css</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">All row sums: tensor([1.0000, 1.0000, 1.0000, 1.0000, 1.0000, 1.0000])</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><h4 id="步骤-3-计算所有上下文向量" tabindex="-1">步骤 3：计算所有上下文向量 <a class="header-anchor" href="#步骤-3-计算所有上下文向量" aria-label="Permalink to &quot;步骤 3：计算所有上下文向量&quot;">​</a></h4><p>使用这些注意力权重，通过矩阵乘法计算所有上下文向量：</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">all_context_vecs </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> attn_weights </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">@</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> inputs</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(all_context_vecs)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>输出的每一行包含一个 3 维上下文向量：</p><div class="language-css vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">css</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[0.4421, 0.5931, 0.5790],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.4419, 0.6515, 0.5683],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.4431, 0.6496, 0.5671],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.4304, 0.6298, 0.5510],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.4671, 0.5910, 0.5266],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.4177, 0.6503, 0.5645]])</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>验证第 2 行与之前计算的上下文向量 z(2)z^{(2)}z(2) 是否一致：</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;Previous 2nd context vector:&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, context_vec_2)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>结果显示与之前计算的 <code>context_vec_2</code> 完全一致：</p><div class="language-arduino vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">arduino</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>Previous 2nd context vector: tensor([0.4419, 0.6515, 0.5683])</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>至此，我们完成了一个简单自注意力机制的代码演练。下一节中，我们将添加可训练权重，使得 LLM 能够从数据中学习并在特定任务上提升表现。</p><h2 id="_3-4-实现带可训练权重的自注意力机制" tabindex="-1">3.4 实现带可训练权重的自注意力机制 <a class="header-anchor" href="#_3-4-实现带可训练权重的自注意力机制" aria-label="Permalink to &quot;3.4 实现带可训练权重的自注意力机制&quot;">​</a></h2><p>本节将实现原始 Transformer 架构、GPT 模型和大多数流行大型语言模型（LLM）中使用的自注意力机制。这种自注意力机制也被称为<strong>缩放点积注意力</strong>。<strong>图 3.13</strong> 提供了一个心智模型，展示了这种自注意力机制在实现 LLM 时的整体架构中的位置。</p><hr><blockquote><p><strong>图 3.13</strong> 描绘了本节实现的自注意力机制如何融入本书和本章的上下文。在上一节中，我们编写了一个简化的注意力机制代码，以理解注意力机制的基本原理。在本节中，我们将为该注意力机制添加<strong>可训练的权重</strong>。随后，在接下来的部分中，我们将通过添加<strong>因果遮罩</strong>和<strong>多头机制</strong>进一步扩展这个自注意力机制。</p></blockquote><hr><p><img src="/repo/assets/image-40.CPcS5KTY.png" alt="alt text"></p><p>如<strong>图 3.13</strong> 所示，带可训练权重的自注意力机制是基于前面所介绍的概念构建的：我们的目标是<strong>针对特定的输入元素，计算出输入向量的加权和作为上下文向量</strong>。正如你将看到的，与我们在<strong>3.3 节</strong>中编写的基本自注意力机制相比，这里仅存在细微的差别。</p><ul><li>最显著的差别是<strong>引入了在模型训练过程中更新的权重矩阵</strong>。这些可训练的权重矩阵至关重要，使模型（特别是模型中的注意力模块）能够学习生成“优质”的上下文向量。（需要注意的是，我们将在<strong>第 5 章</strong>对 LLM 进行训练。）</li></ul><hr><p>我们将通过两个小节来实现此自注意力机制。首先，我们将像之前一样逐步编写代码。接着，我们会将代码组织成一个紧凑的<strong>Python 类</strong>，以便可以在<strong>第 4 章</strong>编写的 LLM 架构中导入使用。</p><p><strong>3.4.1 逐步计算注意力权重</strong></p><p>我们将通过引入三个可训练的权重矩阵 WqW_qWq​、WkW_kWk​ 和 WvW_vWv​ 来逐步实现自注意力机制。这三个矩阵用于将嵌入的输入标记 x(i)x^{(i)}x(i) 映射到查询、键和值向量，见图 3.14。</p><hr><blockquote><p><strong>图 3.14</strong>：在具有可训练权重矩阵的自注意力机制的第一步中，我们为输入元素 xxx 计算查询（qqq）、键（k \））和值（( v \））向量。与之前的章节类似，我们将第二个输入 ( x^{(2)} 作为查询输入。查询向量 q(2)q^{(2)}q(2) 通过输入 x(2)x^{(2)}x(2) 与权重矩阵 WqW_qWq​ 的矩阵乘法获得。类似地，通过矩阵乘法，我们利用权重矩阵 WkW_kWk​ 和 WvW_vWv​ 得到键和值向量。</p></blockquote><hr><p><img src="/repo/assets/image-41.BA4vXqjP.png" alt="alt text"></p><p>在 <strong>3.3.1 节</strong>中，当我们计算简化的注意力权重以获得上下文向量 z(2)z^{(2)}z(2) 时，定义了第二个输入元素 x(2)x^{(2)}x(2) 作为查询。随后在 <strong>3.3.2 节</strong> 中，我们将其泛化以计算六词输入句 &quot;Your journey starts with one step.&quot; 的所有上下文向量 z(1)z^{(1)}z(1) 到 z(T)z^{(T)}z(T)。</p><p>类似地，我们将从计算一个上下文向量 z(2)z^{(2)}z(2) 开始用于演示，下一节中我们会修改代码以计算所有上下文向量。</p><h3 id="变量定义" tabindex="-1">变量定义 <a class="header-anchor" href="#变量定义" aria-label="Permalink to &quot;变量定义&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">x_2 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> inputs[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">] </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">#A</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">d_in </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> inputs.shape[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">] </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">#B</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">d_out </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 2</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"> #C</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>在类似 GPT 的模型中，输入和输出的维度通常相同，但为便于理解计算，我们这里选择不同的输入（<code>d_in=3</code>）和输出（<code>d_out=2</code>）维度。</p><p>接下来，我们初始化权重矩阵 WqW_qWq​、WkW_kWk​、WvW_vWv​，如图 3.14 所示：</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">torch.manual_seed(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">123</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">W_query </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.nn.Parameter(torch.rand(d_in, d_out), </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">False</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">W_key </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.nn.Parameter(torch.rand(d_in, d_out), </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">False</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">W_value </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.nn.Parameter(torch.rand(d_in, d_out), </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">requires_grad</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">False</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>注意，这里设置 <code>requires_grad=False</code> 以减少输出干扰，但在模型训练中应设置 <code>requires_grad=True</code>，以便更新这些矩阵。</p><hr><h3 id="计算查询、键和值向量" tabindex="-1">计算查询、键和值向量 <a class="header-anchor" href="#计算查询、键和值向量" aria-label="Permalink to &quot;计算查询、键和值向量&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">query_2 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> x_2 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">@</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> W_query</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">key_2 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> x_2 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">@</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> W_key</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">value_2 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> x_2 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">@</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> W_value</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(query_2)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>输出查询结果为 2 维向量，因为我们将权重矩阵的列数 <code>d_out</code> 设置为 2：</p><div class="language-scss vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">scss</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([0</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">.4306</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, 1.4551])</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><hr><p><strong>权重参数与注意力权重的区别</strong></p><p>权重矩阵 WWW 中的“权重”是指模型训练中优化的神经网络参数，不要与注意力权重混淆。注意力权重决定了上下文向量在多大程度上依赖输入的不同部分。</p><p>简而言之，权重参数是定义网络连接的学习系数，而注意力权重是动态的、特定于上下文的值。</p><p>尽管当前目标是计算一个上下文向量 z(2)z^{(2)}z(2)，我们仍需所有输入元素的键和值向量以便与查询 q(2)q^{(2)}q(2) 计算注意力权重。</p><p>通过矩阵乘法，我们可以获得所有的键和值：</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">keys </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> inputs </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">@</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> W_key</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">values </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> inputs </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">@</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> W_value</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;keys.shape:&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, keys.shape)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;values.shape:&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, values.shape)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>输出显示，我们成功地将 6 个输入标记从 3D 投影到 2D 嵌入空间：</p><div class="language-css vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">css</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">keys</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">.shape</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: torch.Size([6, 2])</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">values</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">.shape</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: torch.Size([6, 2])</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><hr><h3 id="第二步-计算注意力得分" tabindex="-1">第二步：计算注意力得分 <a class="header-anchor" href="#第二步-计算注意力得分" aria-label="Permalink to &quot;第二步：计算注意力得分&quot;">​</a></h3><blockquote><p><strong>图 3.15</strong>：注意力得分计算是类似于简化自注意力机制中的点积计算，不同之处在于现在我们使用了通过权重矩阵变换后的查询和键。 <img src="/repo/assets/image-42.BSM_hcYL.png" alt="alt text"> 首先，计算注意力得分 ω22\omega_{22}ω22​：</p></blockquote><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">keys_2 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> keys[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">] </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">#A</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">attn_score_22 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> query_2.dot(keys_2)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(attn_score_22)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>输出如下：</p><div class="language-scss vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">scss</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor(1</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">.8524</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>接下来，通过矩阵乘法计算所有的注意力得分：</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">attn_scores_2 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> query_2 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">@</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> keys.T</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(attn_scores_2)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>快速检查结果，输出的第二个元素与我们之前计算的 <code>attn_score_22</code> 一致：</p><div class="language-scss vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">scss</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([1</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">.2705</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, 1</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">.8524</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, 1</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">.8111</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, 1</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">.0795</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, 0</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">.5577</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, 1.5440])</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><hr><h3 id="第三步-从注意力得分到注意力权重" tabindex="-1">第三步：从注意力得分到注意力权重 <a class="header-anchor" href="#第三步-从注意力得分到注意力权重" aria-label="Permalink to &quot;第三步：从注意力得分到注意力权重&quot;">​</a></h3><blockquote><p><strong>图 3.16</strong>：在计算注意力得分 ω\omegaω 后，下一步是通过 softmax 函数来归一化这些得分，以获得注意力权重 α\alphaα。 <img src="/repo/assets/image-43.CdsjVt1S.png" alt="alt text"> 如图 3.16 所示，我们通过缩放注意力得分并使用 softmax 函数计算注意力权重。不同之处在于，现在我们通过将注意力得分除以键嵌入维度的平方根进行缩放：</p></blockquote><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">d_k </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> keys.shape[</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">attn_weights_2 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.softmax(attn_scores_2 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> d_k</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">**</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dim</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(attn_weights_2)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>结果的注意力权重如下：</p><div class="language-scss vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">scss</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([0</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">.1500</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, 0</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">.2264</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, 0</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">.2199</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, 0</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">.1311</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, 0</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">.0906</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, 0.1820])</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p><strong>缩放点积注意力的原理</strong></p><p>通过嵌入维度大小进行归一化，可以避免小梯度，从而改善训练效果。嵌入维度增加时，较大的点积会使 softmax 函数表现为阶跃函数，导致梯度接近零。通过嵌入维度平方根的缩放可以缓解这种情况，因此这种机制称为<strong>缩放点积注意力</strong>。</p><hr><h3 id="最后一步-计算上下文向量" tabindex="-1">最后一步：计算上下文向量 <a class="header-anchor" href="#最后一步-计算上下文向量" aria-label="Permalink to &quot;最后一步：计算上下文向量&quot;">​</a></h3><blockquote><p><strong>图 3.17</strong>：在自注意力计算的最后一步，通过注意力权重组合所有的值向量来计算上下文向量。 <img src="/repo/assets/image-44.B-kXJUyG.png" alt="alt text"> 如同 <strong>3.3 节</strong> 中我们通过输入向量的加权和计算上下文向量一样，现在通过值向量的加权和计算上下文向量。注意力权重作为加权因子来衡量每个值向量的重要性。可以通过矩阵乘法一步获得输出：</p></blockquote><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">context_vec_2 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> attn_weights_2 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">@</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> values</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(context_vec_2)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>结果向量内容如下：</p><div class="language-scss vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">scss</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([0</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">.3061</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, 0.8210])</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>到目前为止，我们仅计算了单个上下文向量 z(2)z^{(2)}z(2)。下一节将泛化代码以计算输入序列中的所有上下文向量 z(1)z^{(1)}z(1) 到 z(T)z^{(T)}z(T)。</p><hr><p><strong>为何使用查询、键和值？</strong></p><p>在注意力机制中，“键”、“查询”和“值”概念源于信息检索和数据库领域，其中使用相似的概念来存储、搜索和检索信息。</p><ul><li><strong>查询（Query）</strong> ：类似于数据库中的搜索查询，代表当前关注的项目（如句子中的词或标记），用于探测输入序列的其他部分。</li><li><strong>键（Key）</strong> ：类似于数据库中的键，用于索引和搜索。每个输入元素（如句中的每个词）都有一个键用于与查询匹配。</li><li><strong>值（Value）</strong> ：类似于键值对中的值，表示输入项的实际内容或表示。模型确定哪些键与查询最匹配后，检索对应的值。</li></ul><p><strong>3.4.2 实现紧凑的自注意力 Python 类</strong></p><p>在前面的章节中，我们逐步计算了自注意力的输出，主要是为了便于逐步讲解。在实际操作中，考虑到下一章的 LLM 实现，我们可以将这些代码组织到一个 Python 类中，如下所示：</p><hr><h3 id="代码清单-3-1-一个紧凑的自注意力类" tabindex="-1">代码清单 3.1：一个紧凑的自注意力类 <a class="header-anchor" href="#代码清单-3-1-一个紧凑的自注意力类" aria-label="Permalink to &quot;代码清单 3.1：一个紧凑的自注意力类&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.nn </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">as</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">class</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> SelfAttention_v1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">nn</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">Module</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">):</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    def</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> __init__</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self, d_in, d_out):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        super</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">().</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">__init__</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.d_out </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> d_out</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.W_query </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Parameter(torch.rand(d_in, d_out))</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.W_key </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Parameter(torch.rand(d_in, d_out))</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.W_value </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Parameter(torch.rand(d_in, d_out))</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> forward</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self, x):</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        keys </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">@</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.W_key</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        queries </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">@</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.W_query</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        values </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> x </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">@</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.W_value</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        attn_scores </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> queries </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">@</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> keys.T  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># omega</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        attn_weights </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.softmax(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            attn_scores </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> keys.shape[</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">**</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dim</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        )</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        context_vec </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> attn_weights </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">@</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> values</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> context_vec</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br></div></div><p>在这段 PyTorch 代码中，<code>SelfAttention_v1</code> 是一个继承自 <code>nn.Module</code> 的类，这是 PyTorch 模型的基本构建块，提供了创建和管理模型层的必要功能。</p><ul><li><code>__init__</code> 方法初始化了可训练的权重矩阵（<code>W_query</code>、<code>W_key</code> 和 <code>W_value</code>），用于将查询、键和值的输入维度 dind_{in}din​ 映射到输出维度 doutd_{out}dout​。</li><li>在前向传播中，<code>forward</code> 方法通过查询和键相乘计算注意力得分（<code>attn_scores</code>），并使用 softmax 归一化这些得分。最后，通过用这些归一化后的注意力得分对值进行加权生成上下文向量。</li></ul><p>可以像这样使用该类：</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">torch.manual_seed(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">123</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">sa_v1 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> SelfAttention_v1(d_in, d_out)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(sa_v1(inputs))</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>由于 <code>inputs</code> 包含六个嵌入向量，这会生成一个存储六个上下文向量的矩阵：</p><div class="language-css vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">css</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[0.2996, 0.8053],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.3061, 0.8210],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.3058, 0.8203],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.2948, 0.7939],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.2927, 0.7891],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.2990, 0.8040]], grad_fn=&lt;MmBackward0</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>作为快速检查，可以看到第二行 <code>[0.3061, 0.8210]</code> 与上一节的 <code>context_vec_2</code> 内容一致。</p><hr><blockquote><p><strong>图 3.18</strong>：在自注意力中，我们用三个权重矩阵 WqW_qWq​、WkW_kWk​ 和 WvW_vWv​ 变换输入矩阵 XXX 中的输入向量，然后根据生成的查询（QQQ）和键（KKK）计算注意力权重矩阵。使用注意力权重和值（VVV），我们随后计算上下文向量（ZZZ）。为简洁起见，我们在图中只展示了单个输入文本的 nnn 个标记，而不是多个输入的批量，以便更清晰地展示和理解流程。 <img src="/repo/assets/image-45.DvOehRo-.png" alt="alt text"> 如图 3.18 所示，自注意力涉及可训练的权重矩阵 WqW_qWq​、WkW_kWk​ 和 WvW_vWv​。这些矩阵将输入数据转换为查询、键和值，构成注意力机制的核心部分。随着模型接触更多数据，这些可训练权重会得到调整。</p></blockquote><hr><h3 id="使用-pytorch-的-linear-层改进-selfattention-v1-实现" tabindex="-1">使用 PyTorch 的 Linear 层改进 SelfAttention_v1 实现 <a class="header-anchor" href="#使用-pytorch-的-linear-层改进-selfattention-v1-实现" aria-label="Permalink to &quot;使用 PyTorch 的 Linear 层改进 SelfAttention_v1 实现&quot;">​</a></h3><p>利用 PyTorch 的 <code>nn.Linear</code> 层可以进一步优化 <code>SelfAttention_v1</code> 的实现，该层在禁用偏置项时可以高效执行矩阵乘法。此外，使用 <code>nn.Linear</code> 而不是手动实现 <code>nn.Parameter(torch.rand(...))</code> 的优势在于 <code>nn.Linear</code> 拥有优化的权重初始化方案，有助于更稳定和有效的模型训练。</p><hr><h3 id="代码清单-3-2-使用-pytorch-linear-层的自注意力类" tabindex="-1">代码清单 3.2：使用 PyTorch Linear 层的自注意力类 <a class="header-anchor" href="#代码清单-3-2-使用-pytorch-linear-层的自注意力类" aria-label="Permalink to &quot;代码清单 3.2：使用 PyTorch Linear 层的自注意力类&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">class</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> SelfAttention_v2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">nn</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">Module</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">):</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    def</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> __init__</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self, d_in, d_out, qkv_bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">False</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        super</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">().</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">__init__</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.d_out </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> d_out</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.W_query </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(d_in, d_out, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">qkv_bias)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.W_key </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(d_in, d_out, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">qkv_bias)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.W_value </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(d_in, d_out, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">qkv_bias)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> forward</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self, x):</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        keys </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.W_key(x)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        queries </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.W_query(x)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        values </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.W_value(x)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        attn_scores </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> queries </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">@</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> keys.T</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        attn_weights </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.softmax(attn_scores </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> keys.shape[</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">**</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dim</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        context_vec </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> attn_weights </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">@</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> values</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> context_vec</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br></div></div><p>可以像 <code>SelfAttention_v1</code> 一样使用 <code>SelfAttention_v2</code>：</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">torch.manual_seed(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">789</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">sa_v2 </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> SelfAttention_v2(d_in, d_out)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(sa_v2(inputs))</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>输出如下：</p><div class="language-css vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">css</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[-0.0739, 0.0713],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [-0.0748, 0.0703],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [-0.0749, 0.0702],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [-0.0760, 0.0685],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [-0.0763, 0.0679],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [-0.0754, 0.0693]], grad_fn=&lt;MmBackward0</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>注意，由于 <code>nn.Linear</code> 使用了更复杂的权重初始化方案，<code>SelfAttention_v1</code> 和 <code>SelfAttention_v2</code> 产生了不同的输出。</p><hr><h3 id="练习-3-1-比较-selfattention-v1-和-selfattention-v2" tabindex="-1">练习 3.1 比较 SelfAttention_v1 和 SelfAttention_v2 <a class="header-anchor" href="#练习-3-1-比较-selfattention-v1-和-selfattention-v2" aria-label="Permalink to &quot;练习 3.1 比较 SelfAttention_v1 和 SelfAttention_v2&quot;">​</a></h3><p>注意 <code>nn.Linear</code> 在 <code>SelfAttention_v2</code> 中使用的权重初始化方案与 <code>SelfAttention_v1</code> 中的 <code>nn.Parameter(torch.rand(d_in, d_out))</code> 不同，这会导致两种机制生成不同的结果。为检查 <code>SelfAttention_v1</code> 和 <code>SelfAttention_v2</code> 结构上的相似性，我们可以将 <code>SelfAttention_v2</code> 的权重矩阵转移到 <code>SelfAttention_v1</code> 上，使得它们生成相同的结果。</p><p>你的任务是将 <code>SelfAttention_v2</code> 实例的权重正确赋值给 <code>SelfAttention_v1</code> 实例。请理解两者的权重关系（提示：<code>nn.Linear</code> 存储的权重矩阵是转置形式）。完成赋值后，应该可以观察到两个实例生成相同的输出。</p><hr><h3 id="下一节预告" tabindex="-1">下一节预告 <a class="header-anchor" href="#下一节预告" aria-label="Permalink to &quot;下一节预告&quot;">​</a></h3><p>在下一节中，我们将进一步增强自注意力机制，重点是引入<strong>因果性</strong>和<strong>多头</strong>元素。</p><ul><li><strong>因果性</strong>：为了防止模型在序列中访问未来信息，因果性修正注意力机制，使得语言建模任务中，每个词的预测仅依赖于前面的词。</li><li><strong>多头</strong>：多头注意力将注意力机制分成多个“头”，每个头学习数据的不同方面，使模型能够同时关注不同位置的信息。这在复杂任务中显著提升了模型性能。</li></ul><h2 id="_3-5-使用因果注意力隐藏未来词" tabindex="-1">3.5 使用因果注意力隐藏未来词 <a class="header-anchor" href="#_3-5-使用因果注意力隐藏未来词" aria-label="Permalink to &quot;3.5 使用因果注意力隐藏未来词&quot;">​</a></h2><p>本节中，我们将对标准自注意力机制进行修改，创建因果注意力机制，这是在后续章节中开发大型语言模型（LLM）时的关键步骤。</p><p>因果注意力，也称为<strong>遮罩注意力</strong>，是一种特殊的自注意力形式。它限制模型在处理任何给定的标记时，仅考虑序列中之前和当前的输入。这与标准自注意力机制不同，后者允许同时访问整个输入序列。</p><p>因此，在计算注意力得分时，因果注意力机制确保模型仅考虑当前标记及之前出现的标记，而不会关注当前标记之后的未来标记。</p><p>在类似 GPT 的 LLM 中，为实现这一点，我们在处理每个标记时会对未来标记进行遮罩，忽略当前标记之后的标记，如<strong>图 3.19</strong> 所示。</p><hr><blockquote><p><strong>图 3.19</strong>：在因果注意力中，我们遮罩了对角线以上的注意力权重，以便在计算上下文向量时，LLM 不能访问未来标记。例如，对于第二行的“journey”一词，仅保留该词之前（“Your”）和当前词（“journey”）的位置的注意力权重。 <img src="/repo/assets/image-46.CqHDOskO.png" alt="alt text"></p></blockquote><hr><p>如<strong>图 3.19</strong> 所示，我们遮罩了对角线以上的注意力权重，并对未遮罩的注意力权重进行归一化，使每行的注意力权重之和为 1。在接下来的章节中，我们将用代码实现该遮罩和归一化过程。</p><p><strong>3.5.1 应用因果注意力遮罩</strong></p><p>本节将实现因果注意力遮罩的代码。我们将按照<strong>图 3.20</strong>中总结的步骤进行。</p><hr><blockquote><p><strong>图 3.20</strong>：获得因果注意力中的遮罩注意力权重矩阵的一种方法是，将 softmax 应用于注意力得分，零化对角线以上的元素，并对结果矩阵进行归一化。</p></blockquote><hr><p><img src="/repo/assets/image-47.BasutS3D.png" alt="alt text"></p><p>为了实现因果注意力遮罩的步骤，首先我们对上节的注意力得分和权重进行操作，编码因果注意力机制。</p><h3 id="第一步-计算注意力权重" tabindex="-1">第一步：计算注意力权重 <a class="header-anchor" href="#第一步-计算注意力权重" aria-label="Permalink to &quot;第一步：计算注意力权重&quot;">​</a></h3><p>我们首先使用 softmax 函数来计算注意力权重：</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">queries </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sa_v2.W_query(inputs)  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># A</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">keys </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> sa_v2.W_key(inputs)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">attn_scores </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> queries </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">@</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> keys.T</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">attn_weights </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.softmax(attn_scores </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> keys.shape[</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">**</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dim</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(attn_weights)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>结果的注意力权重如下：</p><div class="language-css vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">css</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[0.1921, 0.1646, 0.1652, 0.1550, 0.1721, 0.1510],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.2041, 0.1659, 0.1662, 0.1496, 0.1665, 0.1477],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.2036, 0.1659, 0.1662, 0.1498, 0.1664, 0.1480],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.1869, 0.1667, 0.1668, 0.1571, 0.1661, 0.1564],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.1585],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">       grad_fn=&lt;SoftmaxBackward0</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><hr><h3 id="第二步-生成遮罩矩阵" tabindex="-1">第二步：生成遮罩矩阵 <a class="header-anchor" href="#第二步-生成遮罩矩阵" aria-label="Permalink to &quot;第二步：生成遮罩矩阵&quot;">​</a></h3><p>利用 PyTorch 的 <code>tril</code> 函数创建一个遮罩矩阵，将对角线以上的值置零：</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">context_length </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> attn_scores.shape[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">mask_simple </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.tril(torch.ones(context_length, context_length))</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(mask_simple)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>得到的遮罩矩阵如下：</p><div class="language-css vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">css</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[1., 0., 0., 0., 0., 0.],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [1., 1., 0., 0., 0., 0.],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [1., 1., 1., 0., 0., 0.],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [1., 1., 1., 1., 0., 0.],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [1., 1., 1., 1., 1., 0.],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [1., 1., 1., 1., 1., 1.]])</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><h3 id="第三步-应用遮罩" tabindex="-1">第三步：应用遮罩 <a class="header-anchor" href="#第三步-应用遮罩" aria-label="Permalink to &quot;第三步：应用遮罩&quot;">​</a></h3><p>将生成的遮罩矩阵与注意力权重相乘，将对角线以上的元素置零：</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">masked_simple </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> attn_weights </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">*</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> mask_simple</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(masked_simple)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>输出结果显示，对角线上方的元素已成功置零：</p><div class="language-css vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">css</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[0.1921, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.2041, 0.1659, 0.0000, 0.0000, 0.0000, 0.0000],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.2036, 0.1659, 0.1662, 0.0000, 0.0000, 0.0000],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.1869, 0.1667, 0.1668, 0.1571, 0.0000, 0.0000],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.1830, 0.1669, 0.1670, 0.1588, 0.1658, 0.0000],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">       grad_fn=&lt;MulBackward0</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><h3 id="第四步-重新归一化遮罩的注意力权重" tabindex="-1">第四步：重新归一化遮罩的注意力权重 <a class="header-anchor" href="#第四步-重新归一化遮罩的注意力权重" aria-label="Permalink to &quot;第四步：重新归一化遮罩的注意力权重&quot;">​</a></h3><p>接下来，将每行元素除以该行的和，使其重新归一化为 1：</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">row_sums </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> masked_simple.sum(</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dim</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">keepdim</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">True</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">masked_simple_norm </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> masked_simple </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> row_sums</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(masked_simple_norm)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>得到的归一化注意力权重矩阵中，每行的和为 1，且对角线以上的值为零：</p><div class="language-css vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">css</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">       grad_fn=&lt;DivBackward0</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><hr><h3 id="避免信息泄露的说明" tabindex="-1">避免信息泄露的说明 <a class="header-anchor" href="#避免信息泄露的说明" aria-label="Permalink to &quot;避免信息泄露的说明&quot;">​</a></h3><p>在应用遮罩后重新归一化注意力权重，可能会让人觉得未来标记（我们试图遮罩的）会影响当前标记。然而，重新归一化后的注意力权重分布实际上相当于仅在未遮罩的部分上进行 softmax 计算，因此不会发生信息泄露。</p><hr><h3 id="更高效的遮罩方法" tabindex="-1">更高效的遮罩方法 <a class="header-anchor" href="#更高效的遮罩方法" aria-label="Permalink to &quot;更高效的遮罩方法&quot;">​</a></h3><p>为实现更高效的遮罩，可以在应用 softmax 函数前，将遮罩位置的值设置为负无穷大（-∞），如<strong>图 3.21</strong> 所示。</p><blockquote><p><strong>图 3.21</strong>：更高效的因果注意力遮罩方法是，在应用 softmax 函数前，将遮罩位置设置为负无穷大（-∞）。softmax 会将这些位置视为零概率。 <img src="/repo/assets/image-48.D_ZKMXUU.png" alt="alt text"> 实现该方法的代码如下：</p></blockquote><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">mask </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.triu(torch.ones(context_length, context_length), </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">diagonal</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">masked </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> attn_scores.masked_fill(mask.bool(), </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">torch.inf)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(masked)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br></div></div><p>生成的遮罩结果如下：</p><div class="language-css vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">css</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[0.2899, -inf, -inf, -inf, -inf, </span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">-inf</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.4656, 0.1723, -inf, -inf, -inf, </span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">-inf</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.4594, 0.1703, 0.1731, -inf, -inf, </span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">-inf</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.2642, 0.1024, 0.1036, 0.0186, -inf, </span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">-inf</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.2183, 0.0874, 0.0882, 0.0177, 0.0786, </span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">-inf</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.3408, 0.1270, 0.1290, 0.0198, 0.1290, 0.0078]],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">       grad_fn=&lt;MaskedFillBackward0</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>接下来，应用 softmax 函数完成操作：</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">attn_weights </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.softmax(masked </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> keys.shape[</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">**</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dim</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(attn_weights)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>输出结果显示每行的和为 1，无需进一步归一化：</p><div class="language-css vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">css</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.5517, 0.4483, 0.0000, 0.0000, 0.0000, 0.0000],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.3800, 0.3097, 0.3103, 0.0000, 0.0000, 0.0000],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.2758, 0.2460, 0.2462, 0.2319, 0.0000, 0.0000],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.2175, 0.1983, 0.1984, 0.1888, 0.1971, 0.0000],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.1935, 0.1663, 0.1666, 0.1542, 0.1666, 0.1529]],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">       grad_fn=&lt;SoftmaxBackward0</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>至此，我们可以使用修改后的注意力权重来计算上下文向量，通过 <code>context_vec = attn_weights @ values</code> 实现，如 <strong>3.4 节</strong> 中所示。不过在下一节中，我们将介绍对因果注意力机制的另一项小改动，用于在训练 LLM 时减少过拟合。</p><p><strong>3.5.2 使用 Dropout 进一步遮罩注意力权重</strong></p><p>在深度学习中，<strong>Dropout</strong> 是一种技术，用于在训练期间随机忽略部分隐藏层单元，有效地“丢弃”它们。这种方法可以防止模型过拟合，确保模型不会过于依赖特定的隐藏层单元。注意，Dropout 仅在训练期间使用，推理阶段则禁用。</p><p>在 Transformer 架构（包括 GPT 等模型）中，Dropout 通常应用于两个特定位置：<strong>计算完注意力得分后</strong>或<strong>将注意力权重应用到值向量后</strong>。在这里，我们将把 Dropout 遮罩应用在计算完注意力权重之后，如<strong>图 3.22</strong>所示，因为这种变体在实践中更为常见。</p><hr><blockquote><p><strong>图 3.22</strong>：使用因果注意力遮罩（左上）后，我们应用额外的 Dropout 遮罩（右上），以在训练期间将更多的注意力权重置零，从而减少过拟合。</p></blockquote><hr><p><img src="/repo/assets/image-49.CXesDWbO.png" alt="alt text"></p><p>在下面的代码示例中，我们使用 50% 的 Dropout，即遮罩掉一半的注意力权重（在训练 GPT 模型时，我们会使用更低的 Dropout 比率，例如 0.1 或 0.2）。为便于演示，我们首先对一个 6×6 的全为 1 的张量应用 PyTorch 的 Dropout 实现：</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">torch.manual_seed(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">123</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">dropout </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.nn.Dropout(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># A</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">example </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.ones(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">6</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">6</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># B</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(dropout(example))</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br></div></div><p>输出显示约一半的值被置零：</p><div class="language-css vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">css</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[2., 2., 0., 2., 2., 0.],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0., 0., 0., 2., 0., 2.],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [2., 2., 2., 2., 0., 2.],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0., 2., 2., 0., 0., 2.],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0., 2., 0., 2., 0., 2.],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0., 2., 2., 2., 2., 0.]])</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>应用 50% Dropout 后，矩阵中的一半元素随机设为零。为补偿活跃元素的减少，剩余元素的值会按 1/0.5=21 / 0.5 = 21/0.5=2 的因子进行放大。这种放大至关重要，确保注意力机制的平均影响在训练和推理阶段保持一致。</p><h3 id="将-dropout-应用于注意力权重矩阵" tabindex="-1">将 Dropout 应用于注意力权重矩阵 <a class="header-anchor" href="#将-dropout-应用于注意力权重矩阵" aria-label="Permalink to &quot;将 Dropout 应用于注意力权重矩阵&quot;">​</a></h3><p>我们现在将 Dropout 应用于实际的注意力权重矩阵：</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">torch.manual_seed(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">123</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(dropout(attn_weights))</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>结果中的注意力权重矩阵有额外的元素被置零，且剩余的元素进行了重新缩放：</p><div class="language-css vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">css</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[2.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.7599, 0.6194, 0.6206, 0.0000, 0.0000, 0.0000],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.0000, 0.4921, 0.4925, 0.0000, 0.0000, 0.0000],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.0000, 0.3966, 0.0000, 0.3775, 0.0000, 0.0000],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [0.0000, 0.3327, 0.3331, 0.3084, 0.3331, 0.0000]],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">       grad_fn=&lt;MulBackward0</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><hr><p><strong>3.5.3 实现紧凑的因果注意力类</strong></p><p>接下来，我们将因果注意力和 Dropout 修改合并到 <strong>SelfAttention</strong> Python 类中。这一类将作为实现多头注意力的模板，这是我们将在下一节中实现的最终注意力模块。</p><p>为简化起见，我们确保代码能够处理由多个输入组成的批次，以便 <strong>CausalAttention</strong> 类支持第 2 章中数据加载器生成的批次输出。</p><p>首先，为模拟批次输入，我们将输入文本示例重复一次：</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">batch </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.stack((inputs, inputs), </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dim</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(batch.shape)  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># A</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>这将生成一个 3D 张量，其中包含两个输入文本，每个有 6 个标记，每个标记为 3 维嵌入向量：</p><div class="language-css vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">css</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">torch.Size([2, 6, 3])</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><hr><h3 id="代码清单-3-3-一个紧凑的因果注意力类" tabindex="-1">代码清单 3.3：一个紧凑的因果注意力类 <a class="header-anchor" href="#代码清单-3-3-一个紧凑的因果注意力类" aria-label="Permalink to &quot;代码清单 3.3：一个紧凑的因果注意力类&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">class</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> CausalAttention</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">nn</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">Module</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">):</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    def</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> __init__</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self, d_in, d_out, context_length, dropout, qkv_bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">False</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        super</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">().</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">__init__</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.d_out </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> d_out</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.W_query </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(d_in, d_out, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">qkv_bias)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.W_key </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(d_in, d_out, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">qkv_bias)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.W_value </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(d_in, d_out, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">qkv_bias)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.dropout </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Dropout(dropout)  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># A</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.register_buffer(</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">            &#39;mask&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            torch.triu(torch.ones(context_length, context_length), </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">diagonal</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        )  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># B</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> forward</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self, x):</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        b, num_tokens, d_in </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> x.shape  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 新增批次维度 b</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        keys </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.W_key(x)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        queries </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.W_query(x)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        values </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.W_value(x)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        attn_scores </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> queries </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">@</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> keys.transpose(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># C</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        attn_scores.masked_fill_(</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">            self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.mask.bool()[:num_tokens, :num_tokens], </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">torch.inf</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        )  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># D</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        attn_weights </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.softmax(attn_scores </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> keys.shape[</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">**</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dim</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        attn_weights </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.dropout(attn_weights)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        context_vec </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> attn_weights </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">@</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> values</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> context_vec</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br></div></div><hr><h3 id="代码解释" tabindex="-1">代码解释 <a class="header-anchor" href="#代码解释" aria-label="Permalink to &quot;代码解释&quot;">​</a></h3><p>在 <code>__init__</code> 方法中，我们新增了 <code>self.register_buffer()</code>。在 PyTorch 中使用 <code>register_buffer</code> 虽非所有场景都必须，但在此处有几个优点。例如，在 LLM 中使用 <strong>CausalAttention</strong> 类时，缓冲区会自动随模型移动到相应的设备（CPU 或 GPU），这在训练 LLM 时尤为重要。这意味着不需要手动确保这些张量与模型参数位于同一设备，避免了设备不匹配错误。</p><p>可以像以前一样使用 <strong>CausalAttention</strong> 类：</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">torch.manual_seed(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">123</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">context_length </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> batch.shape[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">ca </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> CausalAttention(d_in, d_out, context_length, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">context_vecs </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> ca(batch)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;context_vecs.shape:&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, context_vecs.shape)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br></div></div><p>输出的上下文向量为一个 3D 张量，每个标记现由一个 2D 嵌入表示：</p><div class="language-css vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">css</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">context_vecs</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">.shape</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: torch.Size([2, 6, 2])</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><hr><blockquote><p><strong>图 3.23</strong>：图中总结了我们到目前为止实现的四种不同的注意力模块。从简化的注意力机制开始，我们依次添加了可训练权重和因果注意力遮罩。在本章的剩余部分，我们将扩展因果注意力机制并实现多头注意力，这是下一章 LLM 实现中使用的最终模块。</p></blockquote><hr><p><img src="/repo/assets/image-50.CZq_9qTW.png" alt="alt text"></p><p>在这一节中，我们专注于神经网络中的因果注意力概念和实现。下一节中，我们将扩展此概念，实现一个<strong>多头注意力模块</strong>，该模块能够并行地执行多个因果注意力机制。</p><h2 id="_3-6-从单头注意力扩展到多头注意力" tabindex="-1">3.6 从单头注意力扩展到多头注意力 <a class="header-anchor" href="#_3-6-从单头注意力扩展到多头注意力" aria-label="Permalink to &quot;3.6 从单头注意力扩展到多头注意力&quot;">​</a></h2><p>在本章的最后部分，我们将先前实现的因果注意力类扩展为多头注意力（multi-head attention）。<strong>多头</strong>表示将注意力机制分成多个独立的“头”，每个头独立操作。单个因果注意力模块可以看作是单头注意力，它只有一组注意力权重顺序处理输入。</p><p>在接下来的小节中，我们将从因果注意力扩展到多头注意力。第一个小节将通过堆叠多个 <strong>CausalAttention</strong> 模块来直观地构建多头注意力模块。第二个小节将以更复杂但计算更高效的方式实现同样的多头注意力模块。</p><h3 id="_3-6-1-堆叠多个单头注意力层" tabindex="-1">3.6.1 堆叠多个单头注意力层 <a class="header-anchor" href="#_3-6-1-堆叠多个单头注意力层" aria-label="Permalink to &quot;3.6.1 堆叠多个单头注意力层&quot;">​</a></h3><p>实际实现多头注意力时，需要创建多个自注意力机制（在<strong>3.4.1 节</strong>中的 <strong>图 3.18</strong> 进行了描述），每个自注意力机制都有自己的权重，然后将它们的输出组合起来。使用多个自注意力机制虽然计算密集，但对于 transformer 架构的 LLM 等复杂模式识别非常重要。</p><blockquote><p><strong>图 3.24</strong> 展示了多头注意力模块的结构，由多个单头注意力模块堆叠而成。</p></blockquote><p>在代码中，我们可以通过实现一个简单的 <strong>MultiHeadAttentionWrapper</strong> 类来堆叠多个先前实现的 <strong>CausalAttention</strong> 模块：</p><hr><p><img src="/repo/assets/image-51.bvADHACF.png" alt="alt text"></p><h3 id="代码清单-3-4-实现多头注意力的包装类" tabindex="-1">代码清单 3.4：实现多头注意力的包装类 <a class="header-anchor" href="#代码清单-3-4-实现多头注意力的包装类" aria-label="Permalink to &quot;代码清单 3.4：实现多头注意力的包装类&quot;">​</a></h3><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">class</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> MultiHeadAttentionWrapper</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">nn</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">Module</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">):</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    def</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> __init__</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">False</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        super</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">().</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">__init__</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.heads </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.ModuleList(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            [CausalAttention(d_in, d_out, context_length, dropout, qkv_bias)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">             for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> _ </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> range</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(num_heads)]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        )</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> forward</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self, x):</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.cat([head(x) </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> head </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.heads], </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dim</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br></div></div><p>例如，若我们使用 <strong>MultiHeadAttentionWrapper</strong> 类，指定两个注意力头（<code>num_heads=2</code>）和 <strong>CausalAttention</strong> 输出维度 <code>d_out=2</code>，则结果是 4 维的上下文向量（<code>d_out*num_heads=4</code>），如<strong>图 3.25</strong> 所示。</p><blockquote><p><strong>图 3.25</strong>：在 <strong>MultiHeadAttentionWrapper</strong> 中，我们指定了注意力头的数量（<code>num_heads</code>）。若设置 <code>num_heads=2</code>，得到包含两组上下文向量矩阵的张量，每个上下文向量矩阵的行表示标记的上下文向量，列表示嵌入维度（由 <code>d_out=4</code> 指定）。沿列维度拼接这些上下文向量矩阵。由于有 2 个注意力头和 2 维嵌入维度，最终嵌入维度为 2×2=42 \times 2 = 42×2=4。</p></blockquote><hr><p><img src="/repo/assets/image-52.C70_pDez.png" alt="alt text"></p><h3 id="具体代码示例" tabindex="-1">具体代码示例 <a class="header-anchor" href="#具体代码示例" aria-label="Permalink to &quot;具体代码示例&quot;">​</a></h3><p>可以像以前一样使用 <strong>MultiHeadAttentionWrapper</strong> 类：</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">torch.manual_seed(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">123</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">context_length </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> batch.shape[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 标记数</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">d_in, d_out </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">mha </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> MultiHeadAttentionWrapper(d_in, d_out, context_length, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">num_heads</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">context_vecs </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> mha(batch)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(context_vecs)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;context_vecs.shape:&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, context_vecs.shape)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>输出的张量表示上下文向量：</p><div class="language-css vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">css</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[[-0.4519, 0.2216, 0.4772, 0.1063],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">         [-0.5874, 0.0058, 0.5891, 0.3257],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">         [-0.6300, -0.0632, 0.6202, 0.3860],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">         [-0.5675, -0.0843, 0.5478, 0.3589],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">         [-0.5526, -0.0981, 0.5321, 0.3428],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">         [-0.5299, -0.1081, 0.5077, 0.3493]],</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        [[-0.4519, 0.2216, 0.4772, 0.1063],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">         [-0.5874, 0.0058, 0.5891, 0.3257],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">         [-0.6300, -0.0632, 0.6202, 0.3860],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">         [-0.5675, -0.0843, 0.5478, 0.3589],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">         [-0.5526, -0.0981, 0.5321, 0.3428],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">         [-0.5299, -0.1081, 0.5077, 0.3493]]], grad_fn=&lt;CatBackward0</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">&gt;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br></div></div><p>输出张量 <code>context_vecs.shape</code> 为：</p><div class="language-css vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">css</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">context_vecs</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">.shape</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: torch.Size([2, 6, 4])</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>结果中 <code>context_vecs</code> 张量的第一个维度为 2，因为有两个输入文本（输入文本重复，因此这两个上下文向量完全相同）。第二维度对应每个输入的 6 个标记。第三维度则是每个标记的 4 维嵌入。</p><hr><p><strong>练习 3.2</strong>：返回 2 维的嵌入向量</p><p>更改 <strong>MultiHeadAttentionWrapper(..., num_heads=2)</strong> 的输入参数，使输出上下文向量为 2 维而不是 4 维，同时保持 <code>num_heads=2</code>。提示：无需修改类实现，只需更改其他输入参数。</p><hr><p>在本节中，我们实现了一个 <strong>MultiHeadAttentionWrapper</strong> 类，用于组合多个单头注意力模块。然而请注意，在 <code>forward</code> 方法中，多个头是通过 <code>[head(x) for head in self.heads]</code> 顺序处理的。可以通过并行处理所有头来改进这一实现。例如，通过矩阵乘法同时计算所有注意力头的输出，这将在下一节中探索。</p><h3 id="_3-6-2-实现带有权重分割的多头注意力机制" tabindex="-1">3.6.2 实现带有权重分割的多头注意力机制 <a class="header-anchor" href="#_3-6-2-实现带有权重分割的多头注意力机制" aria-label="Permalink to &quot;3.6.2 实现带有权重分割的多头注意力机制&quot;">​</a></h3><p>在上一节中，我们通过堆叠多个单头注意力模块创建了一个 <code>MultiHeadAttentionWrapper</code> 来实现多头注意力机制。这是通过实例化并组合多个 <code>CausalAttention</code> 对象实现的。</p><p>与其维护两个单独的类 <code>MultiHeadAttentionWrapper</code> 和 <code>CausalAttention</code>，我们可以将这两个概念结合到一个 <code>MultiHeadAttention</code> 类中。此外，除了将 <code>MultiHeadAttentionWrapper</code> 与 <code>CausalAttention</code> 代码合并，我们还将做一些其他修改，以更高效地实现多头注意力机制。</p><p>在 <code>MultiHeadAttentionWrapper</code> 中，多个头是通过创建一个 <code>CausalAttention</code> 对象列表 (<code>self.heads</code>) 实现的，每个对象表示一个独立的注意力头。<code>CausalAttention</code> 类独立执行注意力机制，然后将每个头的结果拼接起来。而新的 <code>MultiHeadAttention</code> 类在一个类中集成了多头功能，通过对查询、键和值张量进行投影并重塑（<code>reshaping</code>），将输入分成多个头，在计算注意力后再组合这些头的结果。</p><p>下面是 <code>MultiHeadAttention</code> 类的代码：</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">class</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> MultiHeadAttention</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">nn</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">Module</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">):</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    def</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> __init__</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">False</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        super</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">().</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">__init__</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">()</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        assert</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> d_out </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">%</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> num_heads </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">==</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;d_out must be divisible by num_heads&quot;</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.d_out </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> d_out</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.num_heads </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> num_heads</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.head_dim </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> d_out </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">//</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> num_heads  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># A</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.W_query </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(d_in, d_out, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">qkv_bias)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.W_key </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(d_in, d_out, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">qkv_bias)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.W_value </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(d_in, d_out, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">bias</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">qkv_bias)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.out_proj </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Linear(d_out, d_out)  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># B</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.dropout </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> nn.Dropout(dropout)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.register_buffer(</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">            &#39;mask&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            torch.triu(torch.ones(context_length, context_length), </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">diagonal</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        )</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> forward</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self, x):</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        b, num_tokens, d_in </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> x.shape</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        keys </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.W_key(x)  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># C</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        queries </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.W_query(x)  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># C</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        values </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.W_value(x)  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># C</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        keys </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> keys.view(b, num_tokens, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.num_heads, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.head_dim)  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># D</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        values </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> values.view(b, num_tokens, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.num_heads, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.head_dim)  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># D</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        queries </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> queries.view(b, num_tokens, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.num_heads, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.head_dim)  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># D</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        keys </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> keys.transpose(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># E</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        queries </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> queries.transpose(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># E</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        values </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> values.transpose(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># E</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        attn_scores </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> queries </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">@</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> keys.transpose(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># F</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        mask_bool </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.mask.bool()[:num_tokens, :num_tokens]  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># G</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        attn_scores.masked_fill_(mask_bool, </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">torch.inf)  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># H</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        attn_weights </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.softmax(</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            attn_scores </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">/</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> keys.shape[</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">**</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 0.5</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">dim</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=-</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        )</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        attn_weights </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.dropout(attn_weights)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        context_vec </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> (attn_weights </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">@</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> values).transpose(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># I</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        context_vec </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> context_vec.contiguous().view(b, num_tokens, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.d_out)  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># J</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        context_vec </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.out_proj(context_vec)  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># K</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> context_vec</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br><span class="line-number">8</span><br><span class="line-number">9</span><br><span class="line-number">10</span><br><span class="line-number">11</span><br><span class="line-number">12</span><br><span class="line-number">13</span><br><span class="line-number">14</span><br><span class="line-number">15</span><br><span class="line-number">16</span><br><span class="line-number">17</span><br><span class="line-number">18</span><br><span class="line-number">19</span><br><span class="line-number">20</span><br><span class="line-number">21</span><br><span class="line-number">22</span><br><span class="line-number">23</span><br><span class="line-number">24</span><br><span class="line-number">25</span><br><span class="line-number">26</span><br><span class="line-number">27</span><br><span class="line-number">28</span><br><span class="line-number">29</span><br><span class="line-number">30</span><br><span class="line-number">31</span><br><span class="line-number">32</span><br><span class="line-number">33</span><br><span class="line-number">34</span><br><span class="line-number">35</span><br><span class="line-number">36</span><br><span class="line-number">37</span><br><span class="line-number">38</span><br><span class="line-number">39</span><br></div></div><p>虽然 <code>MultiHeadAttention</code> 类中的重塑（<code>.view</code>）和转置（<code>.transpose</code>）操作看起来复杂，但它在数学上实现的原理与之前的 <code>MultiHeadAttentionWrapper</code> 相同。</p><p>在 <code>MultiHeadAttentionWrapper</code> 中，我们堆叠多个单头注意力层，将它们组合成多头注意力层。而 <code>MultiHeadAttention</code> 类采取了整合的方法，它开始于一个多头层，然后在内部将该层分割成单独的注意力头，如图 3.26 所示。 <img src="/repo/assets/image-53.Bt5VHDzs.png" alt="alt text"></p><h4 id="示例-多头矩阵乘法的实现" tabindex="-1">示例：多头矩阵乘法的实现 <a class="header-anchor" href="#示例-多头矩阵乘法的实现" aria-label="Permalink to &quot;示例：多头矩阵乘法的实现&quot;">​</a></h4><p>假设我们有如下张量：</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">a </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> torch.tensor([[[[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2745</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.6584</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2775</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.8573</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">], [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.8993</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.0390</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.9268</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.7388</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">], [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.7179</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.7058</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.9156</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.4340</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">                  [[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.0772</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.3565</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.1479</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5331</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">], [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.4066</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.2318</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.4545</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.9737</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">], [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.4606</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5159</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.4220</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5786</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]]]])</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><p>现在我们可以执行矩阵乘法：</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(a </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">@</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> a.transpose(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">3</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">))</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br></div></div><p>结果如下：</p><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">tensor([[[[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1.3208</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1.1631</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1.2879</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">          [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1.1631</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2.2150</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1.8424</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">          [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1.2879</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1.8424</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2.0402</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">         [[</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.4391</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.7003</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5903</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">          [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.7003</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1.3737</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1.0620</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">],</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">          [</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.5903</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">1.0620</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.9912</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]]]])</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br></div></div><p>在 <code>MultiHeadAttention</code> 中，计算注意力权重和上下文向量后，来自所有头的上下文向量会被转置回形状 <code>(b, num_tokens, num_heads, head_dim)</code>。这些向量会被重塑（展平）成形状 <code>(b, num_tokens, d_out)</code>，从而有效地将所有头的输出组合起来。</p><p>此外，我们在 <code>MultiHeadAttention</code> 中添加了一个输出投影层 (<code>self.out_proj</code>)，这在 <code>CausalAttention</code> 类中没有。这个输出投影层不是严格必须的，但它在许多大模型架构中常见，因此我们在此添加它以便更全面。</p><h4 id="使用-multiheadattention-类" tabindex="-1">使用 <code>MultiHeadAttention</code> 类 <a class="header-anchor" href="#使用-multiheadattention-类" aria-label="Permalink to &quot;使用 `MultiHeadAttention` 类&quot;">​</a></h4><div class="language-python vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">torch.manual_seed(</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">123</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">batch_size, context_length, d_in </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> batch.shape</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">d_out </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> 2</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">mha </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> MultiHeadAttention(d_in, d_out, context_length, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">0.0</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">num_heads</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">context_vecs </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> mha(batch)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(context_vecs)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&quot;context_vecs.shape:&quot;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, context_vecs.shape)</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br><span class="line-number">3</span><br><span class="line-number">4</span><br><span class="line-number">5</span><br><span class="line-number">6</span><br><span class="line-number">7</span><br></div></div><p>输出的维度由 <code>d_out</code> 参数控制：</p><div class="language-lua vp-adaptive-theme line-numbers-mode"><button title="Copy Code" class="copy"></button><span class="lang">lua</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">tensor</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">[[[0.3190, 0.4858], [0.2943, 0.3897], ...]]</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">context_vecs.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">shape</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">: torch.</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">Size</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">([</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">6</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">, </span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">2</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">])</span></span></code></pre><div class="line-numbers-wrapper" aria-hidden="true"><span class="line-number">1</span><br><span class="line-number">2</span><br></div></div><h4 id="练习-3-3-初始化与-gpt-2-相同大小的注意力模块" tabindex="-1">练习 3.3 初始化与 GPT-2 相同大小的注意力模块 <a class="header-anchor" href="#练习-3-3-初始化与-gpt-2-相同大小的注意力模块" aria-label="Permalink to &quot;练习 3.3 初始化与 GPT-2 相同大小的注意力模块&quot;">​</a></h4><p>使用 <code>MultiHeadAttention</code> 类，初始化一个具有 12 个注意力头的多头注意力模块，并确保输入和输出的嵌入维度与 GPT-2 相同（768 维）。</p><h2 id="_3-7-总结" tabindex="-1">3.7 总结 <a class="header-anchor" href="#_3-7-总结" aria-label="Permalink to &quot;3.7 总结&quot;">​</a></h2><ul><li>注意力机制将输入元素转换为包含所有输入信息的增强上下文向量表示。</li><li>自注意力机制将上下文向量表示计算为输入的加权和。</li><li>在简化的注意力机制中，注意力权重是通过点积计算的。</li><li>点积是一种简洁的方式，将两个向量逐元素相乘并求和。</li><li>虽然矩阵乘法并非绝对必要，但它使我们可以通过替代嵌套的 <code>for</code> 循环来更高效和紧凑地实现计算。</li><li>用于大型语言模型（LLM）的自注意力机制称为缩放点积注意力，其中包含可训练的权重矩阵，用于计算输入的中间变换：查询（queries）、值（values）和键（keys）。</li><li>在从左到右读取和生成文本的 LLM 中，我们添加了因果注意力掩码，以防止 LLM 访问未来的标记（tokens）。</li><li>除了因果注意力掩码用于将未来标记的权重归零外，我们还可以添加一个 dropout 掩码，以减少 LLM 的过拟合。</li><li>基于 Transformer 的 LLM 中的注意力模块包含多个因果注意力实例，这称为多头注意力。</li><li>可以通过堆叠多个因果注意力模块实例来创建多头注意力模块。</li><li>一种更高效的创建多头注意力模块的方法是使用批量矩阵乘法。</li></ul></div></div></main><footer class="VPDocFooter" data-v-56ee120b data-v-d7c1e045><!--[--><!--]--><!----><nav class="prev-next" aria-labelledby="doc-footer-aria-label" data-v-d7c1e045><span class="visually-hidden" id="doc-footer-aria-label" data-v-d7c1e045>Pager</span><div class="pager" data-v-d7c1e045><a class="VPLink link pager-link prev" href="/repo/bllm/2_working_with_text_data.html" data-v-d7c1e045><!--[--><span class="desc" data-v-d7c1e045>Previous page</span><span class="title" data-v-d7c1e045>处理文本数据</span><!--]--></a></div><div class="pager" data-v-d7c1e045><a class="VPLink link pager-link next" href="/repo/bllm/4_implementing_a_gpt_model_from_scratch_to_generate_text.html" data-v-d7c1e045><!--[--><span class="desc" data-v-d7c1e045>Next page</span><span class="title" data-v-d7c1e045>从零实现GPT模型生成文本</span><!--]--></a></div></nav></footer><!--[--><!--]--></div></div></div><!--[--><!--]--></div></div><!----><!--[--><!--]--></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"api-examples.md\":\"oHVhhJSP\",\"bllm_1_understanding_large_language_models.md\":\"Dx1nExBx\",\"bllm_2_working_with_text_data.md\":\"Beus_XV7\",\"bllm_3_coding_attention_mechanisms.md\":\"3P--8seq\",\"bllm_4_implementing_a_gpt_model_from_scratch_to_generate_text.md\":\"B7S2U3Fn\",\"bllm_5_pretraining_on_unlabeled_data.md\":\"JFJoofxK\",\"bllm_appendix_a_introduction_to_pytorch.md\":\"D26eWALm\",\"bllm_appendix_b_references_and_further_reading.md\":\"ChWmz7Bh\",\"bllm_appendix_c_exercise_solutions.md\":\"8lHZTB1W\",\"bllm_appendix_d_adding_bells_and_whistles_to_the_training_loop.md\":\"K_WFgPqk\",\"bllm_index.md\":\"C5t-ezHk\",\"index.md\":\"CphcdwYM\",\"markdown-examples.md\":\"Bs9TG2dU\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"en-US\",\"dir\":\"ltr\",\"title\":\"大模型知识库\",\"description\":\"A VitePress Site\",\"base\":\"/repo/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"nav\":[{\"text\":\"首页\",\"link\":\"/\"},{\"text\":\"示例\",\"link\":\"/markdown-examples\"}],\"search\":{\"provider\":\"local\"},\"sidebar\":[{\"text\":\"书籍\",\"items\":[{\"text\":\"Markdown Examples\",\"link\":\"/markdown-examples\"},{\"text\":\"Runtime API Examples\",\"link\":\"/api-examples\"},{\"text\":\"欢迎\",\"link\":\"/bllm/\"},{\"text\":\"理解大型语言模型\",\"link\":\"/bllm/1_understanding_large_language_models\"},{\"text\":\"处理文本数据\",\"link\":\"/bllm/2_working_with_text_data\"},{\"text\":\"编写注意力机制\",\"link\":\"/bllm/3_coding_attention_mechanisms\"},{\"text\":\"从零实现GPT模型生成文本\",\"link\":\"/bllm/4_implementing_a_gpt_model_from_scratch_to_generate_text\"},{\"text\":\" 在无标签数据上预训练\",\"link\":\"/bllm/5_pretraining_on_unlabeled_data\"},{\"text\":\"附录 A. PyTorch简介\",\"link\":\"/bllm/appendix_a_introduction_to_pytorch\"},{\"text\":\"附录 B. 参考文献与进一步阅读\",\"link\":\"/bllm/appendix_b_references_and_further_reading\"},{\"text\":\"附录 C. 习题解答\",\"link\":\"/bllm/appendix_c_exercise_solutions\"},{\"text\":\"附录 D. 给训练循环添加附加功能\",\"link\":\"/bllm/appendix_d_adding_bells_and_whistles_to_the_training_loop\"}]},{\"text\":\"课程\",\"items\":[{\"text\":\"课程1\",\"link\":\"/course/1\"},{\"text\":\"课程2\",\"link\":\"/course/2\"}]}],\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/vuejs/vitepress\"}]},\"locales\":{},\"scrollOffset\":134,\"cleanUrls\":false}");</script>
    
  </body>
</html>